{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Catégorisez automatiquement des questions**\n",
    "\n",
    "### partie 4/8 : Prédiction de tags, approche supervisée + tracking mlflow\n",
    "\n",
    "#### <br> Notebook d’exploration et de pré-traitement des questions, comprenant une analyse univariée et multivariée, un nettoyage des questions, un feature engineering de type bag of words avec réduction de dimension (du vocabulaire et des tags) \n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Librairies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python version 3.11.5 (main, Sep 11 2023, 13:23:44) [GCC 11.2.0]\n",
      "pyLDAvis version 3.4.0\n",
      "\n",
      "Number of CPU cores: 8\n",
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os, sys, random\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "from pandarallel import pandarallel\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "# NLP\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# gensim\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import corpus2dense\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim import similarities\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.metrics import make_scorer, PredictionErrorDisplay, r2_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#tf\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# mlflow\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.models.signature import infer_signature, ModelSignature #, Schema, ParamSchema\n",
    "from mlflow.types import Schema, ParamSchema, ParamSpec, ColSpec\n",
    "\n",
    "# ! REQUIRES CONSOLE COMMAND : mlflow ui\n",
    "# depuis dossier notebooks\n",
    "os.environ['MLFLOW_TRACKING_URI'] = './'\n",
    "\n",
    "# Utilisable seulement en local...\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = MlflowClient(tracking_uri=\"http://127.0.0.1:5000\")\n",
    "\n",
    "# verif, reglages\n",
    "print('\\nPython version ' + sys.version)\n",
    "print('pyLDAvis version ' + pyLDAvis.__version__)\n",
    "\n",
    "# Modify if necessary\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"\\nNumber of CPU cores: {num_cores}\")\n",
    "pandarallel.initialize(progress_bar=False, nb_workers=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe manipulation, NLP\n",
    "\n",
    "def get_missing_values(df):\n",
    "    \"\"\"Generates a DataFrame containing the count and proportion of missing values for each feature.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with columns for the feature name, count of missing values,\n",
    "        count of non-missing values, proportion of missing values, and data type for each feature.\n",
    "    \"\"\"\n",
    "    # Count the missing values for each column\n",
    "    missing = df.isna().sum()\n",
    "\n",
    "    # Calculate the percentage of missing values\n",
    "    percent_missing = df.isna().mean() * 100\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    missings_df = pd.DataFrame({\n",
    "        'column_name': df.columns,\n",
    "        'missing': missing,\n",
    "        'present': df.shape[0] - missing,  # Count of non-missing values\n",
    "        'percent_missing': percent_missing.round(2),  # Rounded to 2 decimal places\n",
    "        'type': df.dtypes\n",
    "    })\n",
    "\n",
    "    # Sort the DataFrame by the count of missing values\n",
    "    missings_df.sort_values('missing', inplace=True)\n",
    "\n",
    "    return missings_df\n",
    "\n",
    "# with pd.option_context('display.max_rows', 1000):\n",
    "#   display(get_missing_values(df))\n",
    "\n",
    "\n",
    "def quick_look(df, miss=True):\n",
    "    \"\"\"\n",
    "    Display a quick overview of a DataFrame, including shape, head, tail, unique values, and duplicates.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame to inspect.\n",
    "        check_missing (bool, optional): Whether to check and display missing values (default is True).\n",
    "\n",
    "    The function provides a summary of the DataFrame, including its shape, the first and last rows, the count of unique values per column, and the number of duplicates.\n",
    "    If `check_missing` is set to True, it also displays missing value information.\n",
    "    \"\"\"\n",
    "    print(f'shape : {df.shape}')\n",
    "\n",
    "    display(df.head())\n",
    "    display(df.tail())\n",
    "\n",
    "    print('uniques :')\n",
    "    display(df.nunique())\n",
    "\n",
    "    print('Doublons ? ', df.duplicated(keep='first').sum(), '\\n')\n",
    "\n",
    "    if miss:\n",
    "        display(get_missing_values(df))\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    #Cleaning\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # Tokenization\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(\" \".join(tokens))  # Apply RegexpTokenizer to the entire list\n",
    "\n",
    "        # Remove punctuation (make sure, RegexpTokenizer should have done it already)\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in tokenization: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Get part of speech for each token\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    for token, pos_tag in pos_tags:\n",
    "        # ! Uncommenting next line may crash the cell\n",
    "        # print(f\"Token: {token}, POS Tag: {pos_tag}\")\n",
    "        if pos_tag.startswith('V'):\n",
    "            # On garde\n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(token, pos='v'))\n",
    "            # Returns the input word unchanged if it cannot be found in WordNet.\n",
    "        elif pos_tag.startswith('N'):\n",
    "            # On garde\n",
    "            try:\n",
    "                lemmatized_tokens.append(lemmatizer.lemmatize(token, pos='n'))\n",
    "            except Exception as e:\n",
    "                print(f\"Error lemmatizing verb {token}: {e}\")\n",
    "        # Sinon on supprime\n",
    "\n",
    "    # Read forbidden words (stopwords, too frequent, too rare) from the file\n",
    "    with open('./forbidden_words.txt', 'r') as file:\n",
    "        forbidden = [line.strip() for line in file]\n",
    "\n",
    "    filtered_list = [token for token in lemmatized_tokens if token not in forbidden]\n",
    "\n",
    "    # keep uniques\n",
    "    seen_tokens = set()\n",
    "    unique_tokens = []\n",
    "\n",
    "    for token in filtered_list:\n",
    "        if token not in seen_tokens:\n",
    "            seen_tokens.add(token)\n",
    "            if len(token) > 2:\n",
    "                unique_tokens.append(token)\n",
    "\n",
    "    return unique_tokens\n",
    "\n",
    "\n",
    "def turn_str_back_into_list(df):\n",
    "    \"\"\"Correct the type change due to .csv export\"\"\"\n",
    "\n",
    "    df['title_nltk'] = df['title_nltk'].apply(ast.literal_eval)\n",
    "    df['body_nltk'] = df['body_nltk'].apply(ast.literal_eval)\n",
    "    df['title_spacy'] = df['title_spacy'].apply(ast.literal_eval)\n",
    "    df['body_spacy'] = df['body_spacy'].apply(ast.literal_eval)\n",
    "    df['all_tags'] = df['all_tags'].apply(ast.literal_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow\n",
    "\n",
    "def create_mlflow_experiment(\n",
    "    experiment_name: str, artifact_location: str, tags: dict[str, str]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a new mlflow experiment with the given name and artifact location.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    experiment_name: str\n",
    "        The name of the experiment to create.\n",
    "    artifact_location: str\n",
    "        The artifact location of the experiment to create.\n",
    "    tags: dict[str,Any]\n",
    "        The tags of the experiment to create.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    experiment_id: str\n",
    "        The id of the created experiment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        experiment_id = mlflow.create_experiment(\n",
    "            name=experiment_name, artifact_location=artifact_location, tags=tags\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Experiment {experiment_name} already exists.\")\n",
    "        experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "    mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "    return experiment_id\n",
    "\n",
    "\n",
    "def get_mlflow_experiment(\n",
    "    experiment_id: str = None, experiment_name: str = None\n",
    ") -> mlflow.entities.Experiment:\n",
    "    \"\"\"\n",
    "    Retrieve the mlflow experiment with the given id or name.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    experiment_id: str\n",
    "        The id of the experiment to retrieve.\n",
    "    experiment_name: str\n",
    "        The name of the experiment to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    experiment: mlflow.entities.Experiment\n",
    "        The mlflow experiment with the given id or name.\n",
    "    \"\"\"\n",
    "    if experiment_id is not None:\n",
    "        experiment = mlflow.get_experiment(experiment_id)\n",
    "    elif experiment_name is not None:\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    else:\n",
    "        raise ValueError(\"Either experiment_id or experiment_name must be provided.\")\n",
    "\n",
    "    return experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! This needs mlfow ui console command first -> unusable on remote server\n",
    "# all_experiments = client.search_experiments()\n",
    "# pprint(all_experiments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>all_tags</th>\n",
       "      <th>title_nltk</th>\n",
       "      <th>body_nltk</th>\n",
       "      <th>title_spacy</th>\n",
       "      <th>body_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47660</th>\n",
       "      <td>2011-05-23 22:22:56</td>\n",
       "      <td>How can I send a file document to the printer ...</td>\n",
       "      <td>Here's the basic premise:\\nMy user clicks some...</td>\n",
       "      <td>[c#, winforms, pdf, .net-4.0, printing]</td>\n",
       "      <td>[send, file, document, printer, print]</td>\n",
       "      <td>[send, file, document, printer, print, premise...</td>\n",
       "      <td>[send, file, document, printer, print]</td>\n",
       "      <td>[premise, user, click, file, spit, desktop, wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47661</th>\n",
       "      <td>2011-05-23 21:15:51</td>\n",
       "      <td>CA1014 Mark 'some.dll' with CLSCompliant(true)...</td>\n",
       "      <td>When I run StyleCop, I got this error message ...</td>\n",
       "      <td>[visual-studio, visual-studio-2010, dll, style...</td>\n",
       "      <td>[mark, dll, error, message, vs2010]</td>\n",
       "      <td>[mark, dll, error, message, vs2010, run, get, ...</td>\n",
       "      <td>[error, message]</td>\n",
       "      <td>[run, error, message, need, mark, dll, set, dl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47662</th>\n",
       "      <td>2011-05-23 21:05:59</td>\n",
       "      <td>How to change a text file's name in C++?</td>\n",
       "      <td>I would like to change a txt file's name, but ...</td>\n",
       "      <td>[c++, algorithm, file, directory, file-rename]</td>\n",
       "      <td>[change, text, file, name, c]</td>\n",
       "      <td>[change, text, file, name, c, like, change, tx...</td>\n",
       "      <td>[change, text, file]</td>\n",
       "      <td>[like, change, txt, file, find, example, want,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47663</th>\n",
       "      <td>2011-05-23 20:06:35</td>\n",
       "      <td>php implode (101) with quotes</td>\n",
       "      <td>Imploding  a simple array \\nwould look like th...</td>\n",
       "      <td>[php, arrays, string, csv, implode]</td>\n",
       "      <td>[php, quote]</td>\n",
       "      <td>[php, quote, array, look, array, array, lastna...</td>\n",
       "      <td>[quote]</td>\n",
       "      <td>[implode, array, look, array, email, phone, ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47664</th>\n",
       "      <td>2011-05-23 20:00:57</td>\n",
       "      <td>What characters are allowed in a iOS file name?</td>\n",
       "      <td>I'm looking for a way to make sure a string ca...</td>\n",
       "      <td>[ios, file, filenames, character-encoding, nsf...</td>\n",
       "      <td>[character, allow, file, name]</td>\n",
       "      <td>[character, allow, file, name, look, way, make...</td>\n",
       "      <td>[character, allow, file]</td>\n",
       "      <td>[look, way, string, file, section, code, delet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              CreationDate                                              title  \\\n",
       "47660  2011-05-23 22:22:56  How can I send a file document to the printer ...   \n",
       "47661  2011-05-23 21:15:51  CA1014 Mark 'some.dll' with CLSCompliant(true)...   \n",
       "47662  2011-05-23 21:05:59           How to change a text file's name in C++?   \n",
       "47663  2011-05-23 20:06:35                      php implode (101) with quotes   \n",
       "47664  2011-05-23 20:00:57    What characters are allowed in a iOS file name?   \n",
       "\n",
       "                                                    body  \\\n",
       "47660  Here's the basic premise:\\nMy user clicks some...   \n",
       "47661  When I run StyleCop, I got this error message ...   \n",
       "47662  I would like to change a txt file's name, but ...   \n",
       "47663  Imploding  a simple array \\nwould look like th...   \n",
       "47664  I'm looking for a way to make sure a string ca...   \n",
       "\n",
       "                                                all_tags  \\\n",
       "47660            [c#, winforms, pdf, .net-4.0, printing]   \n",
       "47661  [visual-studio, visual-studio-2010, dll, style...   \n",
       "47662     [c++, algorithm, file, directory, file-rename]   \n",
       "47663                [php, arrays, string, csv, implode]   \n",
       "47664  [ios, file, filenames, character-encoding, nsf...   \n",
       "\n",
       "                                   title_nltk  \\\n",
       "47660  [send, file, document, printer, print]   \n",
       "47661     [mark, dll, error, message, vs2010]   \n",
       "47662           [change, text, file, name, c]   \n",
       "47663                            [php, quote]   \n",
       "47664          [character, allow, file, name]   \n",
       "\n",
       "                                               body_nltk  \\\n",
       "47660  [send, file, document, printer, print, premise...   \n",
       "47661  [mark, dll, error, message, vs2010, run, get, ...   \n",
       "47662  [change, text, file, name, c, like, change, tx...   \n",
       "47663  [php, quote, array, look, array, array, lastna...   \n",
       "47664  [character, allow, file, name, look, way, make...   \n",
       "\n",
       "                                  title_spacy  \\\n",
       "47660  [send, file, document, printer, print]   \n",
       "47661                        [error, message]   \n",
       "47662                    [change, text, file]   \n",
       "47663                                 [quote]   \n",
       "47664                [character, allow, file]   \n",
       "\n",
       "                                              body_spacy  \n",
       "47660  [premise, user, click, file, spit, desktop, wa...  \n",
       "47661  [run, error, message, need, mark, dll, set, dl...  \n",
       "47662  [like, change, txt, file, find, example, want,...  \n",
       "47663  [implode, array, look, array, email, phone, ar...  \n",
       "47664  [look, way, string, file, section, code, delet...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47665, 8)\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv('./../0_data/cleaned_data/bow_classic.csv', sep=',')\n",
    "\n",
    "turn_str_back_into_list(raw_data)\n",
    "\n",
    "display(raw_data.tail())\n",
    "\n",
    "print(raw_data.shape)\n",
    "\n",
    "feature = 'title_nltk'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Checkpoint : sample + ttsplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42898, 8)\n",
      "(4767, 8)\n"
     ]
    }
   ],
   "source": [
    "# pour parcourir rapidement la premiere partie du ntbk\n",
    "quick_df = raw_data[::1]\n",
    "\n",
    "# ! éviter > 200 (max),\n",
    "# sinon le testing set devient trop petit pour les prédictions\n",
    "\n",
    "train_df, test_df = train_test_split(quick_df, test_size=0.1, random_state=42)\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Often gives good results if enough data\n",
    "# relatively fast to train\n",
    "# Accepts basically any input, as long as it is numerical\n",
    "\n",
    "# => Perfect for testing different embeddings !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dummy knn : il copie sur le + proche voisin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['your', 'text', 'document', 'javascript'] \n",
      "\n",
      "42898\n",
      "(42898, 6128) \n",
      "\n",
      "['c#', '.net', 'ms-word', 'openxml', 'openxml-sdk']\n",
      "['c#', '.net', 'ms-word', 'openxml', 'openxml-sdk'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Notre baseline\n",
    "# Pour ce premier modèle je voulais quelque chose de simple, pour pouvoir tester les scores.\n",
    "# Donc on n'utilise pas la méthode .predict() du knn, seulement la méthode .kneighbors(),\n",
    "# disponible après l'entrainement (=le fit). Enuite on va regarder et return les n tags les + frequents\n",
    "# parmi les tags des voisins (ici, 1 seul voisin, le + proche).\n",
    "\n",
    "\n",
    "def predict_tags_using_dummy_knn(df, feature=feature, target='all_tags', k=1, exemple=None):\n",
    "    documents = df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "    print(len(corpus))\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    dense_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "    print(dense_matrix.shape, '\\n')\n",
    "\n",
    "    # Ici on ne va pas demander au knn de faire de prediction,\n",
    "    # On veut juste qu'il trouve les voisins.\n",
    "    # Mais la fonction fit a besoin de targets en param\n",
    "    target_values = df[target].values\n",
    "\n",
    "    # Initialize kNN model\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=k, metric='cosine', algorithm='brute')\n",
    "    # print(knn_model.n_neighbors)\n",
    "\n",
    "    knn_model.fit(dense_matrix, target_values)\n",
    "\n",
    "    # Example query\n",
    "    query_document = exemple\n",
    "    query_bow = gensim_dictionary.doc2bow(query_document)\n",
    "    query_vector = corpus2dense([query_bow], num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Find nearest neighbors\n",
    "    _, indices = knn_model.kneighbors(query_vector)\n",
    "\n",
    "    # Aggregate tags from neighbors\n",
    "    neighbor_tags = [tag for i in indices.flatten() for tag in df.iloc[i][target]]\n",
    "\n",
    "    print(neighbor_tags)\n",
    "\n",
    "    # Predict tags based on most common tags among neighbors\n",
    "    predicted_tags = [tag for tag, _ in Counter(neighbor_tags).most_common(n=10)]\n",
    "    # 5 tags/question en moyenne mais on peut suggérer +\n",
    "\n",
    "    return predicted_tags, knn_model\n",
    "\n",
    "\n",
    "exemple = [\"your\", 'text', 'document', 'javascript']\n",
    "print(exemple, '\\n')\n",
    "\n",
    "predicted_tags, knn_test = predict_tags_using_dummy_knn(df=train_df, exemple=exemple)\n",
    "print(predicted_tags, '\\n')\n",
    "\n",
    "# javascript ok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 knn basic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mm principe mais on peut choisir le nombre de voisins (-> paramètre à optimiser)\n",
    "# add score\n",
    "\n",
    "\n",
    "def predict_tags_using_knn(df, feature=feature, target='all_tags', k=50, exemple=None):\n",
    "    documents = df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    dense_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Ici on ne va pas demander au knn de faire de prediction,\n",
    "    # On veut juste qu'il trouve les voisins.\n",
    "    # Mais la fonction fit a besoin de targets en param\n",
    "    target_values = df[target].values\n",
    "\n",
    "    # Initialize kNN model\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=k, metric='cosine', algorithm='brute')\n",
    "    knn_model.fit(dense_matrix, target_values)\n",
    "\n",
    "    # Example query\n",
    "    query_document = exemple\n",
    "    query_bow = gensim_dictionary.doc2bow(query_document)\n",
    "    query_vector = corpus2dense([query_bow], num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Find nearest neighbors\n",
    "    _, indices = knn_model.kneighbors(query_vector)\n",
    "\n",
    "    # Aggregate tags from neighbors\n",
    "    neighbor_tags = [tag for i in indices.flatten() for tag in df.iloc[i][target]]\n",
    "\n",
    "    print(neighbor_tags)\n",
    "\n",
    "    # Predict tags based on most common tags among neighbors\n",
    "    predicted_tags = [tag for tag, _ in Counter(neighbor_tags).most_common(n=10)]\n",
    "    # 5 tags/question en moyenne mais on peut suggérer +\n",
    "\n",
    "    return query_vector, predicted_tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 predictions (input = list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['your', 'text', 'document', 'javascript'] \n",
      "\n",
      "['c#', '.net', 'ms-word', 'openxml', 'openxml-sdk', 'java', 'javascript', 'ajax', 'selenium', 'htmlunit-driver', 'javascript', 'internet-explorer', 'class', 'internet-explorer-8', 'classname', 'javascript', 'jquery', 'jquery-plugins', 'text-to-speech', 'html5-audio', 'javascript', 'html', 'css', 'text', 'truncate', 'javascript', 'html', 'string', 'text', 'extract', 'javascript', 'jquery', 'css', 'dom', 'document', 'javascript', 'dom', 'substring', 'indexof', 'getselection', 'python', 'text', 'replace', 'ms-word', 'python-docx', 'javascript', 'php', 'jquery', 'curl', 'http-headers', 'javascript', 'html', 'function', 'text', 'onclick', 'python', 'module', 'preprocessor', 'nlp', 'stemming', 'android', 'material-design', 'android-support-library', 'android-textinputlayout', 'android-support-design', 'c#', '.net', 'html', 'pdf', 'extract', 'html', 'url', 'browser', 'full-text-search', 'highlight', 'javascript', 'jquery', 'ruby-on-rails', 'tdd', 'jasmine', 'c#', 'javascript', 'html', 'http', 'dom', 'javascript', 'jquery', 'css', 'copy', 'cut', 'javascript', 'firebase', 'google-cloud-platform', 'google-cloud-firestore', 'typeerror', 'javascript', 'arrays', 'iterator', 'ecmascript-6', 'ecmascript-5', 'linux', 'assembly', 'x86-64', 'calling-convention', 'abi', 'python', 'numpy', 'machine-learning', 'scipy', 'scikit-learn', 'javascript', 'reactjs', 'functional-programming', 'immutability', 'immutable.js', 'javascript', 'object', 'recursion', 'comparison', 'equality', 'c#', 'wpf', 'winforms', 'textbox', 'textchanged', 'javascript', 'css', 'encapsulation', 'styling', 'web-component', 'javascript', 'jquery', 'ajax', 'asp.net-mvc-3', 'url', 'javascript', 'jquery', 'textarea', 'hyperlink', 'addition', 'iphone', 'objective-c', 'ios', 'xcode', 'uitextview', 'c#', '.net', 'xml', 'linq', 'xelement', 'ios', 'swift', 'height', 'uilabel', 'frame', 'html', 'meta-tags', 'semantics', 'semantic-web', 'semantic-markup', 'javascript', 'html', 'speech-recognition', 'getusermedia', 'opus', 'java', 'javascript', 'json', 'spring', 'integer', 'ios', 'swift', 'cocoa-touch', 'uilabel', 'strikethrough', 'javascript', 'performance', 'variables', 'ecmascript-harmony', 'ecmascript-4', 'javascript', 'jquery', 'html', 'css', 'bootstrap-4', 'javascript', 'arrays', 'object', 'arguments', 'slice', 'html', 'sublimetext2', 'sublimetext', 'indentation', 'reformat', 'ios', 'ocr', 'xcode4.5', 'tesseract', 'leptonica', 'javascript', 'jquery', 'html', 'dom', 'document-ready', 'html', 'css', 'text', 'autocomplete', 'sublimetext', 'c#', 'javascript', 'jquery', 'asp.net-mvc', 'razor', 'mongodb', 'mongodb-query', 'aggregation-framework', 'spring-data-mongodb', 'full-text-indexing', 'javascript', 'oop', 'functional-programming', 'polymorphism', 'parametric-polymorphism', 'javascript', 'types', 'numbers', 'integer', 'biginteger', 'java', 'swing', 'desktop', 'jtextfield', 'jprogressbar', 'python', 'django', 'orm', 'mongodb', 'mongoengine', 'spring-mvc', 'spring-boot', 'append', 'thymeleaf', 'dynamic-text', 'qt', 'layout', 'css', 'qlabel', 'text-styling']\n",
      "['javascript', 'html', 'jquery', 'css', 'c#', 'text', 'dom', 'python', 'ios', '.net'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemple1 = [\"your\", 'text', 'document', 'javascript']\n",
    "print(exemple, '\\n')\n",
    "\n",
    "_, predicted_tags1 = predict_tags_using_knn(df=train_df, exemple=exemple1)\n",
    "print(predicted_tags1, '\\n') # most frequent, sorted\n",
    "\n",
    "# javascript ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['your', 'text', 'document', 'python'] \n",
      "\n",
      "['python', 'module', 'preprocessor', 'nlp', 'stemming', 'c#', '.net', 'ms-word', 'openxml', 'openxml-sdk', 'python-2.7', 'ubuntu', 'python-3.x', 'spatial-index', 'r-tree', 'python-3.x', 'pdf', 'text', 'extract', 'pdfminer', 'python', 'text', 'stemming', 'plural', 'singular', 'python', 'html', 'web-scraping', 'text', 'beautifulsoup', 'python', 'selenium', 'selenium-webdriver', 'xpath', 'webdriverwait', 'python', 'plot', 'tree', 'data-visualization', 'visualization', 'python', 'pdf', 'python-3.7', 'pypdf', 'pdf-extraction', 'python', 'python-3.x', 'annotations', 'lint', 'type-hinting', 'python', 'python-2.7', 'reflection', 'delegation', 'message-passing', 'python', 'documentation', 'python-3.7', 'docstring', 'python-dataclasses', 'python', 'python-3.x', 'algorithm', 'sorting', 'mergesort', 'python', 'parsing', 'text', 'file-io', 'python-2.7', 'python', 'image', 'opencv', 'image-processing', 'computer-vision', 'python', 'utf-8', 'python-unicode', 'windows-1252', 'cp1252', 'python', 'macos', 'python-3.x', 'sublimetext2', 'sublimetext', 'python', 'python-3.x', 'pickle', 'python-2.4', 'python-2to3', 'nlp', 'cluster-analysis', 'data-mining', 'k-means', 'text-mining', 'python', 'python-3.x', 'python-2.7', 'text-extraction', 'pdfminer', 'python', 'windows', 'user-interface', 'text', 'screen', 'python-2.7', 'exception', 'python-3.x', 'traceback', 'raise', 'python', 'shell', 'encoding', 'utf-8', 'python-2.x', 'python', 'image', 'text', 'python-imaging-library', 'bold', 'python', 'python-2.7', 'tkinter', 'callback', 'tkinter.text', 'python', 'python-2.7', 'metaclass', 'python-3.5', 'abc', 'python', 'python-2.7', 'ubuntu', 'beautifulsoup', 'python-3.3', 'python', 'pandas', 'amazon-web-services', 'aws-lambda', 'aws-glue', 'python', 'django', 'autocomplete', 'sublimetext2', 'sublimetext', 'c#', '.net', 'html', 'pdf', 'extract', 'android', 'material-design', 'android-support-library', 'android-textinputlayout', 'android-support-design', 'python', 'python-3.x', 'python-2.7', 'virtualenv', 'virtualenvwrapper', 'python', 'regex', 'performance', 'perl', 'text-processing', 'python', 'file', 'encryption', 'aes', 'pycrypto', 'python', 'plugins', 'sublimetext2', 'distutils', 'python-requests', 'python', 'selenium', 'xpath', 'selenium-webdriver', 'webdriver', 'python', 'utf-8', 'character-encoding', 'locale', 'default', 'python', 'python-3.x', 'file', 'text', 'count', 'python', 'text', 'replace', 'ms-word', 'python-docx', 'python', 'parsing', 'data-structures', 'dictionary', 'nested', 'python', 'text', 'replace', 'docx', 'zip', 'python', 'pip', 'distutils', 'easy-install', 'maya', 'html', 'url', 'browser', 'full-text-search', 'highlight', 'python', 'numpy', 'ubuntu', 'pip', 'python-3.4', 'python', 'django', 'redis', 'django-cache', 'redis-py', 'python', 'r', 'tensorflow', 'keras', 'reticulate', 'python', 'python-3.x', 'linux', 'windows', 'sleep-mode', 'python', 'python-2.7', 'iterable-unpacking', 'argument-unpacking', 'pep448', 'python', 'list', 'file', 'ascii', 'newline', 'python', 'ubuntu', 'pip', 'distutils', 'ubuntu-20.04']\n",
      "['python', 'python-3.x', 'python-2.7', 'text', 'ubuntu', 'pdf', 'html', 'utf-8', 'sublimetext2', 'file'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemple2 = [\"your\", 'text', 'document', 'python']\n",
    "print(exemple2, '\\n')\n",
    "\n",
    "_, predicted_tags2 = predict_tags_using_knn(df=train_df, exemple=exemple2)\n",
    "print(predicted_tags2, '\\n')\n",
    "\n",
    "# python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['find', 'class', 'com', 'google', 'firebase', 'provider'] \n",
      "\n",
      "['java', 'spring', 'rest', 'gradle', 'spring-boot', 'android', 'android-studio', 'firebase', 'android-gradle-plugin', 'google-play-services', 'java', 'android', 'firebase', 'gradle', 'android-gradle-plugin', 'php', 'sql', 'laravel', 'laravel-5', 'laravel-artisan', 'php', 'class', 'laravel', 'alias', 'autoloader', 'android', 'android-intent', 'arraylist', 'unmarshalling', 'parcelable', 'java', 'spring', 'spring-boot', 'spring-security', 'spring-security-oauth2', 'android', 'google-maps', 'dictionary', 'android-mapview', 'inflate', 'android', 'firebase', 'android-studio', 'android-gradle-plugin', 'jcenter', 'ios', 'objective-c', 'xcode', 'storyboard', 'xib', 'javascript', 'node.js', 'typescript', 'nestjs', 'class-validator', 'java', 'jar', 'maven-2', 'manifest', 'program-entry-point', 'javascript', 'node.js', 'angular', 'typescript', 'angular7', 'angular', 'firebase', 'npm', 'angularfire', 'angularfire2', 'c++', 'c++11', 'bit-fields', 'bitmask', 'enum-class', 'java', 'maven', 'build', 'interop', 'kotlin', 'json', 'angular', 'typescript', 'jwt', 'guard', 'c++', 'gcc', 'vtable', 'virtual-inheritance', 'vtt', 'c++', 'ubuntu', 'sdl', 'sdl-2', 'sdl-image', 'iphone', 'objective-c', 'ios', 'xcode4.3', 'cocoapods', 'firebase', 'flutter', 'dart', 'firebase-authentication', 'google-authentication', 'c#', '.net', 'web-config', 'class-library', 'configurationmanager', 'javascript', 'android', 'reactjs', 'react-native', 'gradle', 'javascript', 'firebase', 'google-drive-api', 'firebase-authentication', 'adobe-indesign', 'android', 'firebase', 'gradle', 'android-gradle-plugin', 'crashlytics', 'php', 'laravel', 'https', 'laravel-valet', 'valet', 'javascript', 'node.js', 'mongodb', 'mongoose', 'mongodb-query', 'java', 'spring', 'junit', 'spring-boot', 'spring-data', 'java', 'android', 'design-patterns', 'inheritance', 'parcelable', 'c#', '.net', 'visual-studio', 'json.net', 'nuget', 'c#', 'android', 'firebase', 'unity-game-engine', 'firebase-authentication', 'php', 'laravel', 'amazon-web-services', 'laravel-5', 'laravelcollective', 'database', 'security', 'firebase', 'firebase-authentication', 'firebase-security', 'google-maps', 'angular', 'typescript', 'angular-cli', 'angular2-google-maps', 'java', 'android', 'android-studio', 'gradle', 'build', 'java', 'json', 'rest', 'jersey', 'jax-rs', 'python', 'class', 'inheritance', 'python-3.x', 'language-design', 'android', 'unity-game-engine', 'abi', 'arcore', 'unsatisfiedlinkerror', 'spring', 'maven', 'spring-mvc', 'spring-boot', 'spring-profiles', 'c#', 'asp.net', 'google-api', 'google-oauth', 'google-api-dotnet-client', 'android', 'android-studio', 'permissions', 'contacts', 'android-contacts', 'java', 'maven', 'maven-2', 'maven-3', 'protocol-buffers', 'android', 'android-studio', 'android-gradle-plugin', 'build.gradle', 'offline-mode', 'node.js', 'firebase', 'firebase-realtime-database', 'firebase-authentication', 'firebase-hosting', 'c#', 'asp.net', '.net', 'sql-server', 'entity-framework-6', 'android', 'firebase', 'flutter', 'google-play-services', 'flutter-dependencies', 'android', 'android-studio', 'flutter', 'sdk', 'android-sdk-manager', 'php', 'ubuntu', 'sqlite', 'ubuntu-12.04', 'lamp', 'php', 'amazon-web-services', 'heroku', 'laravel-5', 'amazon-s3', 'c++', 'templates', 'compilation', 'cppunit', 'googlemock']\n",
      "['android', 'firebase', 'java', 'android-studio', 'php', 'gradle', 'android-gradle-plugin', 'javascript', 'firebase-authentication', 'c#'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemple = test_df[feature][0]\n",
    "print(exemple, '\\n')\n",
    "\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "_, predicted_tags = predict_tags_using_knn(df=train_df, exemple=exemple)\n",
    "print(predicted_tags, '\\n')\n",
    "\n",
    "# firebase peut etre predit, (ou java selon taille datast test)\n",
    "# grand succes !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Predictions (input = text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find class \"com.google.firebase.provider.FirebaseInitProvider\"\n",
      "real tags: ['java', 'android', 'android-studio', 'error-handling', 'compiler-errors'] \n",
      "\n",
      "['java', 'spring', 'rest', 'gradle', 'spring-boot', 'android', 'android-studio', 'firebase', 'android-gradle-plugin', 'google-play-services', 'java', 'android', 'firebase', 'gradle', 'android-gradle-plugin', 'php', 'sql', 'laravel', 'laravel-5', 'laravel-artisan', 'php', 'class', 'laravel', 'alias', 'autoloader', 'android', 'android-intent', 'arraylist', 'unmarshalling', 'parcelable', 'java', 'spring', 'spring-boot', 'spring-security', 'spring-security-oauth2', 'android', 'google-maps', 'dictionary', 'android-mapview', 'inflate', 'android', 'firebase', 'android-studio', 'android-gradle-plugin', 'jcenter', 'ios', 'objective-c', 'xcode', 'storyboard', 'xib', 'javascript', 'node.js', 'typescript', 'nestjs', 'class-validator', 'java', 'jar', 'maven-2', 'manifest', 'program-entry-point', 'javascript', 'node.js', 'angular', 'typescript', 'angular7', 'angular', 'firebase', 'npm', 'angularfire', 'angularfire2', 'c++', 'c++11', 'bit-fields', 'bitmask', 'enum-class', 'java', 'maven', 'build', 'interop', 'kotlin', 'json', 'angular', 'typescript', 'jwt', 'guard', 'c++', 'gcc', 'vtable', 'virtual-inheritance', 'vtt', 'c++', 'ubuntu', 'sdl', 'sdl-2', 'sdl-image', 'iphone', 'objective-c', 'ios', 'xcode4.3', 'cocoapods', 'firebase', 'flutter', 'dart', 'firebase-authentication', 'google-authentication', 'c#', '.net', 'web-config', 'class-library', 'configurationmanager', 'javascript', 'android', 'reactjs', 'react-native', 'gradle', 'javascript', 'firebase', 'google-drive-api', 'firebase-authentication', 'adobe-indesign', 'android', 'firebase', 'gradle', 'android-gradle-plugin', 'crashlytics', 'php', 'laravel', 'https', 'laravel-valet', 'valet', 'javascript', 'node.js', 'mongodb', 'mongoose', 'mongodb-query', 'java', 'spring', 'junit', 'spring-boot', 'spring-data', 'java', 'android', 'design-patterns', 'inheritance', 'parcelable', 'c#', '.net', 'visual-studio', 'json.net', 'nuget', 'c#', 'android', 'firebase', 'unity-game-engine', 'firebase-authentication', 'php', 'laravel', 'amazon-web-services', 'laravel-5', 'laravelcollective', 'database', 'security', 'firebase', 'firebase-authentication', 'firebase-security', 'google-maps', 'angular', 'typescript', 'angular-cli', 'angular2-google-maps', 'java', 'android', 'android-studio', 'gradle', 'build', 'java', 'json', 'rest', 'jersey', 'jax-rs', 'python', 'class', 'inheritance', 'python-3.x', 'language-design', 'android', 'unity-game-engine', 'abi', 'arcore', 'unsatisfiedlinkerror', 'spring', 'maven', 'spring-mvc', 'spring-boot', 'spring-profiles', 'c#', 'asp.net', 'google-api', 'google-oauth', 'google-api-dotnet-client', 'android', 'android-studio', 'permissions', 'contacts', 'android-contacts', 'java', 'maven', 'maven-2', 'maven-3', 'protocol-buffers', 'android', 'android-studio', 'android-gradle-plugin', 'build.gradle', 'offline-mode', 'node.js', 'firebase', 'firebase-realtime-database', 'firebase-authentication', 'firebase-hosting', 'c#', 'asp.net', '.net', 'sql-server', 'entity-framework-6', 'android', 'firebase', 'flutter', 'google-play-services', 'flutter-dependencies', 'android', 'android-studio', 'flutter', 'sdk', 'android-sdk-manager', 'php', 'ubuntu', 'sqlite', 'ubuntu-12.04', 'lamp', 'php', 'amazon-web-services', 'heroku', 'laravel-5', 'amazon-s3', 'c++', 'templates', 'compilation', 'cppunit', 'googlemock']\n",
      "['android', 'firebase', 'java', 'android-studio', 'php', 'gradle', 'android-gradle-plugin', 'javascript', 'firebase-authentication', 'c#'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemple = test_df['title'][0]\n",
    "print(exemple)\n",
    "print(f\"real tags: {test_df['all_tags'][0]}\", '\\n')\n",
    "\n",
    "exemple_text = preprocess_text(exemple)\n",
    "\n",
    "_, predicted_tags = predict_tags_using_knn(df=train_df, exemple=exemple_text)\n",
    "print(predicted_tags, '\\n')\n",
    "\n",
    "# ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do I get `java.lang.NoClassDefFoundError: scala/Function1` when I run my code in ScalaIDE?\n",
      "real tags: ['java', 'scala', 'maven', 'noclassdeffounderror', 'scala-ide'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3540/2487938646.py:64: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, 'html.parser').get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c#', '.net', 'wpf', 'code-behind', 'itemspanel', 'reactjs', 'authentication', 'google-authentication', 'google-api-js-client', 'googleauthr', 'java', 'gradle', 'spring-boot', 'jar', 'build.gradle', 'iphone', 'ios', 'xamarin.ios', 'http-response-codes', 'nsurlconnectiondelegate', 'c++', 'python', 'ctypes', 'cython', 'boost-python', 'php', 'mysql', 'laravel', 'ubuntu', 'server', 'c', 'gcc', 'types', 'openmp', 'typeof', 'scala', 'maven', 'apache-spark', 'noclassdeffounderror', 'spark-streaming', 'c', 'assembly', 'x86', 'x86-64', 'shellcode', 'python', 'selenium', 'http', 'selenium-webdriver', 'ui-automation', 'stack-trace', 'glibc', 'sigabrt', 'segmentation-fault', 'backtrace', 'c++', 'c', 'cuda', 'parallel-processing', 'gpu', 'macos', 'shell', 'scala', 'terminal', 'installation', 'c#', 'sql-server', 'asp.net-mvc', 'entity-framework', 'asp.net-mvc-4', 'java', 'selenium', 'testing', 'selenium-webdriver', 'selenium-chromedriver', 'javascript', 'html', 'ajax', 'url', 'xmlhttprequest', 'scala', 'random', 'collections', 'set', 'scala-collections', 'c', 'assembly', 'x86', 'reverse-engineering', 'decompiling', 'javascript', 'firebase', 'react-native', 'mocking', 'jestjs', 'asp.net-core', 'oauth', 'identityserver4', 'openid-connect', 'asp.net-core-3.0', 'java', 'python', 'apache-spark', 'directed-acyclic-graphs', 'airflow', 'python', 'python-2.7', 'stdout', 'stderr', 'os.system', 'java', 'rest', 'exception', 'jersey', 'provider', 'javascript', 'python', 'google-chrome', 'sandbox', 'brython', 'c++', 'c', 'types', 'int64', 'long-long', 'c#', 'multithreading', 'winforms', 'backgroundworker', 'infinite-loop', 'c#', 'c++', 'wpf', 'visual-studio-2013', 'visual-studio-2015', 'java', 'spring', 'aop', 'aspectj', 'spring-aop', 'ubuntu-18.04', 'windows-subsystem-for-linux', 'ubuntu-20.04', 'wsl-2', 'vscode-remote', 'java', 'file', 'jar', 'io', 'extract', 'android', 'retrofit', 'rx-java', 'android-networking', 'rx-android', 'java', 'maven', 'junit', 'maven-surefire-plugin', 'junit5', 'javascript', 'node.js', 'timer', 'delay', 'settimeout', 'android', 'google-maps', 'google-maps-android-api-2', 'latitude-longitude', 'google-places-api', 'python', 'pandas', 'visual-studio-code', 'jupyter-notebook', 'tqdm', 'android', 'enums', 'android-custom-view', 'attr', 'custom-view', 'android', 'android-emulator', 'android-service', 'monitoring', 'monitor', 'iphone', 'xcode', 'ios-simulator', 'ios5', 'freeze', 'javascript', 'visual-studio', 'interpreter', 'interactive-mode', 'visual-studio-code', 'flash', 'apache-flex', 'debugging', 'air', 'flash-builder', 'unix', 'ubuntu', 'command-line', 'go', 'terminal', 'selenium', 'selenium-webdriver', 'web-inspector', 'inspector', 'view-source', 'javascript', 'typescript', 'visual-studio-code', 'vscode-extensions', 'file-properties', 'c#', 'android', 'xamarin', 'xamarin.android', 'android-runonuithread', 'linux', 'bash', 'shell', 'terminal', 'paste', 'php', 'templates', 'model-view-controller', 'file-io', 'eval', 'javascript', 'debugging', 'runtime', 'methodology', 'liveedit', 'javascript', 'angularjs', 'promise', 'deferred', 'finally', 'javascript', 'vue.js', 'visual-studio-code', 'nuxt.js', 'prettier', 'git', 'jenkins', 'groovy', 'jenkins-plugins', 'jenkins-pipeline']\n",
      "['javascript', 'java', 'python', 'c#', 'c', 'android', 'c++', 'visual-studio-code', 'scala', 'selenium'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemple = test_df['title'][1]\n",
    "print(exemple)\n",
    "print(f\"real tags: {test_df['all_tags'][1]}\", '\\n')\n",
    "\n",
    "exemple_text = preprocess_text(exemple)\n",
    "\n",
    "_, predicted_tags = predict_tags_using_knn(df=train_df, exemple=exemple_text)\n",
    "print(predicted_tags, '\\n')\n",
    "\n",
    "# scala ok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notre objectif de prédiction de tags ressemble a un pb de classification multi-label,\n",
    "# où la matrice de confusion est extrêmement déséquilibrée :\n",
    "# 5 tags sont prédits positifs, contre environ 250 000 tags (si on travaille sur all_tags)\n",
    "# predits negatifs. Autrement dit :\n",
    "\n",
    "# On peut utiliser la precision pour évaluer notre modèle. C'est même exactement l'outil qu'il nous faut :\n",
    "# \"précision = la proportion de prédictions correctes parmi les points que l’on a prédits positifs.\"\n",
    "# En + c de loin le plus léger en ressources, puisqu'il ne s'occupe que des 5 tags prédits.\n",
    "\n",
    "# En revanche je pense que le recall aura bcp moins de variance ici, il sera \"écrasé\" par\n",
    "# le nombre de tags predits negatifs, sa valeur sera tjs très proche de zero.\n",
    "# (même remarque pour la spécificité et l'accuracy)\n",
    "# Et sans recall, pas de f1 score.\n",
    "# à vérifier ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n"
     ]
    }
   ],
   "source": [
    "# pourquoi on ne peut pas utiliser le score precision sckikit ici :\n",
    "from sklearn.metrics import precision_score as p_score\n",
    "\n",
    "# Assuming y_true is the ground truth (real tags) and y_pred is the predicted tags\n",
    "precision = p_score(['ok', 'ko'], ['ko', 'ok'], average='micro')  # You can use 'micro', 'macro', or 'weighted' depending on your use case\n",
    "print(f'Precision: {precision}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Il nous faut un score qui ne tient pas compte de l'ordre (ds lequel les tags st prédits)\n",
    "# Je ne savais pas en codant ces fonctions, mais par convention l'ordre des paramètres\n",
    "# dans un scorer (en tt cas pour sklearn) est plutôt l'inverse : y_true, y_pred\n",
    "# Corrigé pour précision, qu'on utilisera par la suite.\n",
    "# Jaccard étant symétrique, pas indispensable / urgent de modifier\n",
    "\n",
    "\n",
    "def precision_topics(predicted_tags:list, real_tags:list): # pour comparer 2 listes\n",
    "    if len(predicted_tags) > 0:\n",
    "        # precision = TP / (TP + FP)\n",
    "        tp = 0\n",
    "        for predicted_tag in predicted_tags:\n",
    "            if predicted_tag in real_tags:\n",
    "                tp += 1\n",
    "\n",
    "        fp = len(predicted_tags) - tp\n",
    "        precision = tp/(tp + fp)\n",
    "        # <=> precision = tp/len(predicted_tags)\n",
    "\n",
    "    else : # 0 prediction\n",
    "        if len(real_tags) > 0: # On n'a pas de prédiction alors quil y avait des tags à prédire\n",
    "            precision = 0\n",
    "        else: # Pas de prédiction, mais pas de target\n",
    "            precision = 1\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "print(precision_topics(predicted_tags1, exemple1))\n",
    "precision_topics(predicted_tags2, exemple2)\n",
    "\n",
    "# ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score(y_pred, y_true): # pour comparer 2 df ou 2 matrices de mm shape[0]\n",
    "    precision = 0\n",
    "    # print(\"Shapes of y_pred and y_true:\", y_pred.shape, y_true.shape)  # Add this line for debugging\n",
    "    for i in range(0, len(y_pred)):\n",
    "        precision += precision_topics(y_pred[i], y_true[i])\n",
    "    precision_moyenne = precision / len(y_pred)\n",
    "\n",
    "    return precision_moyenne\n",
    "\n",
    "\n",
    "# pour la gridsearchcv\n",
    "custom_precision_scorer = make_scorer(precision_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La precision nous interesse davantage ici, pour au moins 2 raisons :\n",
    "# 1) On n'a aucune raison de \"pénaliser\" le modele pour les faux negatifs\n",
    "# 2) Ici le recall va etre tres proche de zero, et sa variance sera tres faible\n",
    "# => bcp - parlant\n",
    "\n",
    "# Mais au cas où :\n",
    "\n",
    "\n",
    "def recall_topics(all_tags: list, predicted_tags: list):\n",
    "    # recall = TP / (TP + FN)\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    for real_tag in all_tags:\n",
    "        if real_tag in predicted_tags:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "# insister ds la doctype : y_all = all tags ici, != y_true (5-6 tags max)\n",
    "def recall_score(y_all, y_pred): # pour comparer 2 df ou 2 matrices de mm shape[0]\n",
    "    recall = 0\n",
    "    for i in range(0, len(y_pred)):\n",
    "        recall += recall_topics(y_all, y_pred[i]) # ca risque d'etre long a calculer\n",
    "    recall_moyen = recall / len(y_pred)\n",
    "\n",
    "    return recall_moyen\n",
    "\n",
    "\n",
    "custom_recall_scorer = make_scorer(recall_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f1 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm rearques que pour le recall\n",
    "\n",
    "\n",
    "def f1_topics(real_tags: list, predicted_tags: list, all_tags:list):\n",
    "    precision = precision_score(real_tags, predicted_tags)\n",
    "    recall = recall_topics(all_tags, predicted_tags)\n",
    "\n",
    "    # F1 score = 2 * (precision * recall) / (precision + recall)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred, y_all):\n",
    "    f1_score = 0\n",
    "    for i in range(0, len(y_pred)):\n",
    "        score += f1_topics(y_true[i], y_pred[i], y_all)\n",
    "    score_moyen = score / len(y_pred)\n",
    "\n",
    "    return score_moyen\n",
    "\n",
    "\n",
    "custom_f1_scorer = make_scorer(f1_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idem, ici la variance sera presque nulle (TP entre 0 et 5, TN = environ 250 000...)\n",
    "\n",
    "\n",
    "def accuracy_topics(real_tags: list, predicted_tags: list, all_tags:list):\n",
    "    # accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    tp = sum(1 for tag in predicted_tags if tag in real_tags)\n",
    "    tn = sum(1 for tag in all_tags if (tag not in predicted_tags) and (tag not in real_tags) )\n",
    "    fp = sum(1 for tag in predicted_tags if tag not in real_tags)\n",
    "    fn = sum(1 for tag in all_tags if (tag not in predicted_tags) and (tag in real_tags))\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def accuracy_score(y_true, y_pred, y_all):\n",
    "    score = 0\n",
    "    for i in range(0, len(y_pred)):\n",
    "        score += accuracy_topics(y_true[i], y_pred[i], y_all)\n",
    "    score_moyen = score / len(y_pred)\n",
    "\n",
    "    return score_moyen\n",
    "\n",
    "\n",
    "custom_accuracy_scorer = make_scorer(accuracy_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilisé en partie 3 pour évaluer la similarité des topics obtenus / la lda\n",
    "# ici pour comparer topics reels et topics predits.\n",
    "# devrait etre correlé a la precision non ?\n",
    "# st ts les 2 liés au nb de tp (true positifs)\n",
    "\n",
    "# Encore mieux que la simple précision, car + stable si on prédit un nb différent de topics\n",
    "# (voir + loin)\n",
    "\n",
    "\n",
    "def jaccard_similarity(topic1, topic2):\n",
    "    set1 = set(topic1)\n",
    "    set2 = set(topic2)\n",
    "\n",
    "    if len(set1.union(set2)):\n",
    "        return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "    else: # union nulle = 0 prediction, mais rien à prédire non plus...\n",
    "        return 1\n",
    "\n",
    "\n",
    "def jaccard_score(y_true, y_pred): # pour comparer 2 df ou 2 matrices de mm shape[0]\n",
    "    jacc = 0\n",
    "    for i in range(0, len(y_pred)):\n",
    "        jacc += jaccard_similarity(y_true[i], y_pred[i])\n",
    "    jacc_moyen = jacc / len(y_pred)\n",
    "\n",
    "    return jacc_moyen\n",
    "\n",
    "\n",
    "# pour la gridsearchcv\n",
    "custom_jacc_scorer = make_scorer(jaccard_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 0 : ['find', 'class', 'com', 'google', 'firebase', 'provider']\n",
      "real tags : ['java', 'android', 'android-studio', 'error-handling', 'compiler-errors']\n",
      "predicted : ['java', 'spring', 'rest', 'gradle', 'spring-boot']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 1 : ['get', 'lang', 'noclassdeffounderror', 'scala', 'run', 'code']\n",
      "real tags : ['java', 'scala', 'maven', 'noclassdeffounderror', 'scala-ide']\n",
      "predicted : ['c#', '.net', 'wpf', 'code-behind', 'itemspanel']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 2 : ['django', 'bulk', 'create', 'ignore', 'duplicate']\n",
      "real tags : ['python', 'mysql', 'django', 'bulkinsert', 'bulk']\n",
      "predicted : ['django', 'android', 'android-layout', 'material-design', 'android-styles']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 3 : ['difference', 'environment', 'anaconda', 'environment']\n",
      "real tags : ['python', 'pycharm', 'anaconda', 'environment', 'virtual-environment']\n",
      "predicted : ['python', 'virtualenv', 'anaconda', 'conda', 'virtual-environment']\n",
      "precision : 0.6\n",
      "jaccard : 0.42857142857142855 \n",
      "\n",
      "doc 4 : ['form', 'action', 'onsubmit', 'issue']\n",
      "real tags : ['javascript', 'html', 'forms', 'action', 'onsubmit']\n",
      "predicted : ['ios', 'storyboard', 'swift3', 'autoresize', 'xcode8']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 5 : ['get', 'error', 'break', 'pipe']\n",
      "real tags : ['linux', 'bash', 'grep', 'cat', 'broken-pipe']\n",
      "predicted : ['c#', 'sql-server', 'wcf', 'transactions', 'transactionscope']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 6 : ['xamarin', 'show', 'image', 'base64', 'string']\n",
      "real tags : ['c#', 'wpf', 'xaml', 'xamarin', 'xamarin.ios']\n",
      "predicted : ['base64', 'image', 'python', 'image-processing', 'opencv']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 7 : ['pas', 'int', 'array', 'pointer', 'parameter', 'function']\n",
      "real tags : ['c', 'arrays', 'function', 'pointers', 'parameters']\n",
      "predicted : ['c++', 'function', 'python', 'numpy', 'templates']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 8 : ['add', 'device', 'source', 'code']\n",
      "real tags : ['linux', 'kernel', 'emulation', 'hardware', 'qemu']\n",
      "predicted : ['c', 'ios', 'iphone', 'ios7', 'ios5']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 9 : ['pass', 'cooky', 'webclient']\n",
      "real tags : ['c#', 'cookies', 'httpwebrequest', 'webclient', 'html-agility-pack']\n",
      "predicted : ['ios', 'javascript', 'python', 'json', 'url']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 10 : ['write', 'comment', 'console']\n",
      "real tags : ['console', 'comments', 'kibana', 'elastic-stack', 'kibana-6']\n",
      "predicted : ['java', 'ios', 'swift', 'swift4', 'grand-central-dispatch']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 11 : ['button', 'c']\n",
      "real tags : ['c#', '.net', 'silverlight', 'graphics', 'gdi+']\n",
      "predicted : ['c#', '.net', 'winforms', 'windows', 'win-universal-app']\n",
      "precision : 0.4\n",
      "jaccard : 0.25 \n",
      "\n",
      "doc 12 : ['move', 'folder', 'drive', 'preserve', 'timestamps']\n",
      "real tags : ['timestamp', 'directory', 'move', 'ads', 'ntfs']\n",
      "predicted : ['php', 'apache', 'logging', 'xampp', 'xdebug']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 13 : ['regex', 'entityframework', 'string', 'processing', 'database']\n",
      "real tags : ['c#', '.net', 'regex', 'linq', 'entity-framework']\n",
      "predicted : ['string', 'regex', 'bash', 'sed', 'replace']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 14 : ['secure', 'cookie', 'prefix', 'set', 'domain']\n",
      "real tags : ['security', 'cookies', 'https', 'header', 'domain-name']\n",
      "predicted : ['c#', 'cookies', 'cocoa', 'swift', 'nstextview']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 15 : ['set', 'git', 'show', 'git', 'diff']\n",
      "real tags : ['git', 'bash', 'tabs', 'whitespace', 'spaces']\n",
      "predicted : ['git', 'github', 'repository', 'aptana', 'aptana3']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 16 : ['version', 'r', 'java', 'file', 'generate']\n",
      "real tags : ['android', 'eclipse', 'eclipse-plugin', 'adt', 'r.java-file']\n",
      "predicted : ['java', 'android', 'eclipse', 'adt', 'eclipse-adt']\n",
      "precision : 0.6\n",
      "jaccard : 0.42857142857142855 \n",
      "\n",
      "doc 17 : ['hibernate', 'generic']\n",
      "real tags : ['java', 'hibernate', 'generics', 'inheritance', 'orm']\n",
      "predicted : ['java', 'hibernate', 'jpa', 'sql', 'arrays']\n",
      "precision : 0.4\n",
      "jaccard : 0.25 \n",
      "\n",
      "doc 18 : ['scroll', 'issue', 'bootstrap', 'modal']\n",
      "real tags : ['javascript', 'jquery', 'html', 'twitter-bootstrap', 'bootstrap-modal']\n",
      "predicted : ['twitter-bootstrap-3', 'javascript', 'html', 'css', 'twitter-bootstrap']\n",
      "precision : 0.6\n",
      "jaccard : 0.42857142857142855 \n",
      "\n",
      "doc 19 : ['save', 'array', 'size']\n",
      "real tags : ['python', 'arrays', 'numpy', 'hdf5', 'h5py']\n",
      "predicted : ['numpy', 'arrays', 'python', 'python-2.7', 'size']\n",
      "precision : 0.6\n",
      "jaccard : 0.42857142857142855 \n",
      "\n",
      "doc 20 : ['c', 'capture', 'image', 'webcam']\n",
      "real tags : ['c#', 'image', 'file', 'webcam', 'capture']\n",
      "predicted : ['android', 'jquery', 'html', 'css', 'responsive-design']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 21 : ['component', 'sas', 'sc']\n",
      "real tags : ['css', 'reactjs', 'sass', 'styled-components', 'reflow']\n",
      "predicted : ['typescript', 'vue.js', 'java', 'spring', 'dependency-injection']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 22 : ['woocommerce', 'display', 'category', 'name']\n",
      "real tags : ['php', 'wordpress', 'woocommerce', 'categories', 'woocommerce-theming']\n",
      "predicted : ['python-3.x', 'python', 'amazon-web-services', 'amazon-ec2', 'boto3']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 23 : ['make', 'custom', 'style', 'work', 'device']\n",
      "real tags : ['android', 'android-layout', 'android-activity', 'android-manifest', 'android-styles']\n",
      "predicted : ['javascript', 'audio', 'bluetooth', 'communication', 'headset']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 24 : ['name', 'convention', 'case']\n",
      "real tags : ['java', 'naming-conventions', 'uppercase', 'camelcasing', 'abbreviation']\n",
      "predicted : ['ios', 'xcode', 'app-store', 'xcode4', 'cfbundleidentifier']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "Precision = 0.17600000000000002\n",
      "Jaccard similarity = 0.11523809523809522\n"
     ]
    }
   ],
   "source": [
    "# sur un sample du testing set\n",
    "\n",
    "\n",
    "def predict_tags_using_knn(train_df=train_df, feature=feature, target='all_tags', test_df=test_df, k=5,\n",
    "                           n=5):\n",
    "\n",
    "    # 1 PREPROCESSING\n",
    "    documents = train_df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    # print(len(gensim_dictionary))\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "    # print(len(corpus))\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    dense_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "    # print(dense_matrix.shape, '\\n')\n",
    "\n",
    "    # pas tres dense ici, c notre bow donc tres sparse en fait\n",
    "    # curieux d'appeler \"corpus2dense()\" une fonction qui retourne une matrice sparse\n",
    "    # pprint(dense_matrix[:10]) # vraiment tres dense, quasiment que des 0 ! Bref\n",
    "    # print('\\n')\n",
    "\n",
    "    target_values = train_df[target].values\n",
    "\n",
    "    # 2 MODEL TRAINING\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn_model.fit(dense_matrix, target_values)\n",
    "\n",
    "    # 3 PREDICTION\n",
    "    # Predictions completes en 1h ou 2...\n",
    "    # optimiser avec pandarallel ?\n",
    "    # use a sample en attendant\n",
    "\n",
    "    predictions=[]\n",
    "    min_range=0\n",
    "    max_range=25 # test.shape[0]=4767\n",
    "    for i in range(min_range, max_range):\n",
    "        query_document = test_df[feature][i]\n",
    "        print(f'doc {i} : {query_document}')\n",
    "        print(f'real tags : {test_df[target][i]}')\n",
    "        query_bow = gensim_dictionary.doc2bow(query_document)\n",
    "        query_vector = corpus2dense([query_bow], num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "        # Find nearest neighbors\n",
    "        _, indices = knn_model.kneighbors(query_vector)\n",
    "\n",
    "        # Aggregate tags from neighbors\n",
    "        neighbor_tags = [tag for i in indices.flatten() for tag in train_df.iloc[i][target]]\n",
    "\n",
    "        # Predict tags based on most common tags among neighbors\n",
    "        predicted_tags = [tag for tag, _ in Counter(neighbor_tags).most_common(n=n)]\n",
    "        # 5 tags/question en moyenne mais on peut suggérer +\n",
    "        predictions.append(predicted_tags)\n",
    "        print(f'predicted : {predicted_tags}')\n",
    "        print(f'precision : {precision_topics(test_df[target][i], predicted_tags)}')\n",
    "        print(f'jaccard : {jaccard_similarity(test_df[target][i], predicted_tags)}', '\\n')\n",
    "\n",
    "    true_tags = [tags for tags in test_df[target][min_range:max_range]]\n",
    "\n",
    "    # eval\n",
    "    scorer = precision_score\n",
    "    precision = scorer(true_tags, predictions)\n",
    "\n",
    "    scorer = jaccard_score\n",
    "    jaccard = scorer(true_tags, predictions)\n",
    "\n",
    "    return knn_model, precision, jaccard\n",
    "\n",
    "\n",
    "_, precision, jaccard = predict_tags_using_knn()\n",
    "\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Jaccard similarity = {jaccard}')\n",
    "\n",
    "\n",
    "# OK\n",
    "# 0.34 de precision pour 5 voisins ?? c enorme !\n",
    "# <=> jaccard 0.15 (/body), 0.11(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 logging mlfow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment knn_optimisation already exists.\n"
     ]
    }
   ],
   "source": [
    "# Si erreur, commande console\n",
    "# mlflow ui\n",
    "# depuis dossier notebooks\n",
    "\n",
    "experiment_id = create_mlflow_experiment(\n",
    "    experiment_name=\"knn_optimisation\",\n",
    "    artifact_location=\"./artifacts\",\n",
    "    tags={\"model\": \"knn\", \"preprocessing\": 'bow', \"feature\": \"title_nltk\", 'target': 'all_tags'},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: knn_optimisation\n",
      "doc 0 : ['find', 'class', 'com', 'google', 'firebase', 'provider']\n",
      "real tags : ['java', 'android', 'android-studio', 'error-handling', 'compiler-errors']\n",
      "predicted : ['java', 'javascript', 'php', 'laravel', 'c++']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 1 : ['get', 'lang', 'noclassdeffounderror', 'scala', 'run', 'code']\n",
      "real tags : ['java', 'scala', 'maven', 'noclassdeffounderror', 'scala-ide']\n",
      "predicted : ['c', 'java', 'python', 'c#', 'ios']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 2 : ['django', 'bulk', 'create', 'ignore', 'duplicate']\n",
      "real tags : ['python', 'mysql', 'django', 'bulkinsert', 'bulk']\n",
      "predicted : ['django', 'python', 'xcode', 'ios', 'swift']\n",
      "precision : 0.4\n",
      "jaccard : 0.25 \n",
      "\n",
      "doc 3 : ['difference', 'environment', 'anaconda', 'environment']\n",
      "real tags : ['python', 'pycharm', 'anaconda', 'environment', 'virtual-environment']\n",
      "predicted : ['python', 'anaconda', 'java', 'virtualenv', 'conda']\n",
      "precision : 0.4\n",
      "jaccard : 0.25 \n",
      "\n",
      "doc 4 : ['form', 'action', 'onsubmit', 'issue']\n",
      "real tags : ['javascript', 'html', 'forms', 'action', 'onsubmit']\n",
      "predicted : ['forms', 'javascript', 'ios', 'html', 'node.js']\n",
      "precision : 0.6\n",
      "jaccard : 0.42857142857142855 \n",
      "\n",
      "doc 5 : ['get', 'error', 'break', 'pipe']\n",
      "real tags : ['linux', 'bash', 'grep', 'cat', 'broken-pipe']\n",
      "predicted : ['python', 'ios', 'ubuntu', 'c#', 'sql-server']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 6 : ['xamarin', 'show', 'image', 'base64', 'string']\n",
      "real tags : ['c#', 'wpf', 'xaml', 'xamarin', 'xamarin.ios']\n",
      "predicted : ['base64', 'image', 'jquery', 'c#', 'javascript']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 7 : ['pas', 'int', 'array', 'pointer', 'parameter', 'function']\n",
      "real tags : ['c', 'arrays', 'function', 'pointers', 'parameters']\n",
      "predicted : ['c++', 'javascript', 'function', 'php', 'python']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 8 : ['add', 'device', 'source', 'code']\n",
      "real tags : ['linux', 'kernel', 'emulation', 'hardware', 'qemu']\n",
      "predicted : ['c', 'python', 'java', 'iphone', 'c#']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 9 : ['pass', 'cooky', 'webclient']\n",
      "real tags : ['c#', 'cookies', 'httpwebrequest', 'webclient', 'html-agility-pack']\n",
      "predicted : ['javascript', 'ios', 'cookies', 'node.js', 'c#']\n",
      "precision : 0.4\n",
      "jaccard : 0.25 \n",
      "\n",
      "doc 10 : ['write', 'comment', 'console']\n",
      "real tags : ['console', 'comments', 'kibana', 'elastic-stack', 'kibana-6']\n",
      "predicted : ['ios', 'swift', 'java', 'console', 'c']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 11 : ['button', 'c']\n",
      "real tags : ['c#', '.net', 'silverlight', 'graphics', 'gdi+']\n",
      "predicted : ['c#', 'c++', '.net', 'winforms', 'ios']\n",
      "precision : 0.4\n",
      "jaccard : 0.25 \n",
      "\n",
      "doc 12 : ['move', 'folder', 'drive', 'preserve', 'timestamps']\n",
      "real tags : ['timestamp', 'directory', 'move', 'ads', 'ntfs']\n",
      "predicted : ['c#', '.net', 'c++', 'c++11', 'move-semantics']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 13 : ['regex', 'entityframework', 'string', 'processing', 'database']\n",
      "real tags : ['c#', '.net', 'regex', 'linq', 'entity-framework']\n",
      "predicted : ['string', 'database', 'java', 'mysql', 'regex']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 14 : ['secure', 'cookie', 'prefix', 'set', 'domain']\n",
      "real tags : ['security', 'cookies', 'https', 'header', 'domain-name']\n",
      "predicted : ['cookies', 'javascript', 'java', 'android', 'c#']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 15 : ['set', 'git', 'show', 'git', 'diff']\n",
      "real tags : ['git', 'bash', 'tabs', 'whitespace', 'spaces']\n",
      "predicted : ['git', 'github', 'git-commit', 'git-diff', 'gitignore']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 16 : ['version', 'r', 'java', 'file', 'generate']\n",
      "real tags : ['android', 'eclipse', 'eclipse-plugin', 'adt', 'r.java-file']\n",
      "predicted : ['java', 'android', 'c#', 'xml', 'c']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 17 : ['hibernate', 'generic']\n",
      "real tags : ['java', 'hibernate', 'generics', 'inheritance', 'orm']\n",
      "predicted : ['java', 'hibernate', 'jpa', 'generics', 'language-design']\n",
      "precision : 0.6\n",
      "jaccard : 0.42857142857142855 \n",
      "\n",
      "doc 18 : ['scroll', 'issue', 'bootstrap', 'modal']\n",
      "real tags : ['javascript', 'jquery', 'html', 'twitter-bootstrap', 'bootstrap-modal']\n",
      "predicted : ['javascript', 'twitter-bootstrap', 'css', 'jquery', 'html']\n",
      "precision : 0.8\n",
      "jaccard : 0.6666666666666666 \n",
      "\n",
      "doc 19 : ['save', 'array', 'size']\n",
      "real tags : ['python', 'arrays', 'numpy', 'hdf5', 'h5py']\n",
      "predicted : ['arrays', 'numpy', 'python', 'objective-c', 'c']\n",
      "precision : 0.6\n",
      "jaccard : 0.42857142857142855 \n",
      "\n",
      "doc 20 : ['c', 'capture', 'image', 'webcam']\n",
      "real tags : ['c#', 'image', 'file', 'webcam', 'capture']\n",
      "predicted : ['android', 'c#', 'c++', 'java', 'png']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 21 : ['component', 'sas', 'sc']\n",
      "real tags : ['css', 'reactjs', 'sass', 'styled-components', 'reflow']\n",
      "predicted : ['javascript', 'reactjs', 'typescript', 'angular', 'vue.js']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 22 : ['woocommerce', 'display', 'category', 'name']\n",
      "real tags : ['php', 'wordpress', 'woocommerce', 'categories', 'woocommerce-theming']\n",
      "predicted : ['python', 'javascript', 'android', 'ios', 'java']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 23 : ['make', 'custom', 'style', 'work', 'device']\n",
      "real tags : ['android', 'android-layout', 'android-activity', 'android-manifest', 'android-styles']\n",
      "predicted : ['javascript', 'assembly', 'ios', 'jquery', 'google-chrome']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 24 : ['name', 'convention', 'case']\n",
      "real tags : ['java', 'naming-conventions', 'uppercase', 'camelcasing', 'abbreviation']\n",
      "predicted : ['sql', 'javascript', 'postgresql', 'java', 'python']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "run_id: 463b92f56e8b472cbfdf4149ff0dd3f5\n",
      "experiment_id: 882489375579023856\n",
      "status: RUNNING\n",
      "start_time: 1709304633982\n",
      "end_time: None\n",
      "precision_moyenne (debut testing set) : 0.264\n",
      "jaccard_moyen (debut testing set) : 0.17142857142857137 \n",
      "\n",
      "precision_moyenne (debut testing set) : 0.264\n",
      "jaccard_moyen (debut testing set) : 0.17142857142857137 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = get_mlflow_experiment(experiment_id=experiment_id)\n",
    "print(\"Name: {}\".format(experiment.name))\n",
    "\n",
    "# Define run_id variable before the with block\n",
    "run_id = []\n",
    "\n",
    "with mlflow.start_run(run_name=\"testing\", experiment_id=experiment_id) as run:\n",
    "    parameters = {\n",
    "        \"preprocessing\": 'bow',\n",
    "        \"feature\": 'title_nltk',\n",
    "        \"k neighbors\": 10,\n",
    "        \"target\": 'all_tags'\n",
    "    }\n",
    "    mlflow.log_params(parameters)\n",
    "\n",
    "    knn_ref, precision_ref, jaccard_ref = predict_tags_using_knn(k=19)\n",
    "\n",
    "    metrics = {\n",
    "            \"precision\": precision_ref,\n",
    "            \"jaccard\": jaccard_ref\n",
    "        }\n",
    "\n",
    "    # multiple metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    # log model\n",
    "    mlflow.sklearn.log_model(sk_model=knn_ref, artifact_path=\"special_knn\")\n",
    "\n",
    "    # log and register\n",
    "    # mlflow.sklearn.log_model(sk_model=knn, artifact_path=\"special_knn\", registered_model_name=\"special_knn\")\n",
    "\n",
    "    # print info about the run\n",
    "    print(\"run_id: {}\".format(run.info.run_id))\n",
    "    run_id.append(run.info.run_id)\n",
    "    print(\"experiment_id: {}\".format(run.info.experiment_id))\n",
    "    print(\"status: {}\".format(run.info.status))\n",
    "    print(\"start_time: {}\".format(run.info.start_time))\n",
    "    print(\"end_time: {}\".format(run.info.end_time))\n",
    "    # print(\"lifecycle_stage: {}\".format(run.info.lifecycle_stage)) # deprecated\n",
    "\n",
    "    print(f\"precision_moyenne (debut testing set) : {precision_ref}\")\n",
    "    print(f\"jaccard_moyen (debut testing set) : {jaccard_ref}\", '\\n')\n",
    "\n",
    "\n",
    "print(f\"precision_moyenne (debut testing set) : {precision_ref}\")\n",
    "print(f\"jaccard_moyen (debut testing set) : {jaccard_ref}\", '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_moyenne knn d'origine : 0.264\n",
      "jaccard_moyen knn d'origine : 0.17142857142857137 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test : load the model\n",
    "\n",
    "# load model\n",
    "# model_uri = f'runs:/{run_id}/random_forest_classifier'\n",
    "model_uri = f\"/home/ubuntu/Bureau/OC/Projet5_oudelet_kevin/1_4_notebooks/artifacts/{run_id[0]}/artifacts/special_knn\"\n",
    "\n",
    "knn_loaded = mlflow.sklearn.load_model(model_uri=model_uri)\n",
    "\n",
    "# Comparons :\n",
    "\n",
    "# knn d'origine...\n",
    "print(f\"precision_moyenne knn d'origine : {precision_ref}\")\n",
    "print(f\"jaccard_moyen knn d'origine : {jaccard_ref}\", '\\n')\n",
    "\n",
    "# Je n'arrive pas à comprendre comment j'arrive à acceder directement à precision_ref et jaccard_ref\n",
    "# alors que les variables st definies ds le scope local de la boucle while.\n",
    "# run_id, au meme scope, n'est pas accesible directement ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 0 : ['find', 'class', 'com', 'google', 'firebase', 'provider']\n",
      "real tags : ['java', 'android', 'android-studio', 'error-handling', 'compiler-errors']\n",
      "predicted : ['java', 'javascript', 'php', 'laravel', 'c++']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 1 : ['get', 'lang', 'noclassdeffounderror', 'scala', 'run', 'code']\n",
      "real tags : ['java', 'scala', 'maven', 'noclassdeffounderror', 'scala-ide']\n",
      "predicted : ['c', 'java', 'python', 'c#', 'ios']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 2 : ['django', 'bulk', 'create', 'ignore', 'duplicate']\n",
      "real tags : ['python', 'mysql', 'django', 'bulkinsert', 'bulk']\n",
      "predicted : ['django', 'python', 'xcode', 'ios', 'swift']\n",
      "precision : 0.4\n",
      "jaccard : 0.25 \n",
      "\n",
      "doc 3 : ['difference', 'environment', 'anaconda', 'environment']\n",
      "real tags : ['python', 'pycharm', 'anaconda', 'environment', 'virtual-environment']\n",
      "predicted : ['python', 'anaconda', 'java', 'virtualenv', 'conda']\n",
      "precision : 0.4\n",
      "jaccard : 0.25 \n",
      "\n",
      "doc 4 : ['form', 'action', 'onsubmit', 'issue']\n",
      "real tags : ['javascript', 'html', 'forms', 'action', 'onsubmit']\n",
      "predicted : ['forms', 'javascript', 'ios', 'html', 'node.js']\n",
      "precision : 0.6\n",
      "jaccard : 0.42857142857142855 \n",
      "\n",
      "doc 5 : ['get', 'error', 'break', 'pipe']\n",
      "real tags : ['linux', 'bash', 'grep', 'cat', 'broken-pipe']\n",
      "predicted : ['python', 'ios', 'ubuntu', 'c#', 'sql-server']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 6 : ['xamarin', 'show', 'image', 'base64', 'string']\n",
      "real tags : ['c#', 'wpf', 'xaml', 'xamarin', 'xamarin.ios']\n",
      "predicted : ['base64', 'image', 'jquery', 'c#', 'javascript']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 7 : ['pas', 'int', 'array', 'pointer', 'parameter', 'function']\n",
      "real tags : ['c', 'arrays', 'function', 'pointers', 'parameters']\n",
      "predicted : ['c++', 'javascript', 'function', 'php', 'python']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 8 : ['add', 'device', 'source', 'code']\n",
      "real tags : ['linux', 'kernel', 'emulation', 'hardware', 'qemu']\n",
      "predicted : ['c', 'python', 'java', 'iphone', 'c#']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 9 : ['pass', 'cooky', 'webclient']\n",
      "real tags : ['c#', 'cookies', 'httpwebrequest', 'webclient', 'html-agility-pack']\n",
      "predicted : ['javascript', 'ios', 'cookies', 'node.js', 'c#']\n",
      "precision : 0.4\n",
      "jaccard : 0.25 \n",
      "\n",
      "doc 10 : ['write', 'comment', 'console']\n",
      "real tags : ['console', 'comments', 'kibana', 'elastic-stack', 'kibana-6']\n",
      "predicted : ['ios', 'swift', 'java', 'console', 'c']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 11 : ['button', 'c']\n",
      "real tags : ['c#', '.net', 'silverlight', 'graphics', 'gdi+']\n",
      "predicted : ['c#', 'c++', '.net', 'winforms', 'ios']\n",
      "precision : 0.4\n",
      "jaccard : 0.25 \n",
      "\n",
      "doc 12 : ['move', 'folder', 'drive', 'preserve', 'timestamps']\n",
      "real tags : ['timestamp', 'directory', 'move', 'ads', 'ntfs']\n",
      "predicted : ['c#', '.net', 'c++', 'c++11', 'move-semantics']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 13 : ['regex', 'entityframework', 'string', 'processing', 'database']\n",
      "real tags : ['c#', '.net', 'regex', 'linq', 'entity-framework']\n",
      "predicted : ['string', 'database', 'java', 'mysql', 'regex']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 14 : ['secure', 'cookie', 'prefix', 'set', 'domain']\n",
      "real tags : ['security', 'cookies', 'https', 'header', 'domain-name']\n",
      "predicted : ['cookies', 'javascript', 'java', 'android', 'c#']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 15 : ['set', 'git', 'show', 'git', 'diff']\n",
      "real tags : ['git', 'bash', 'tabs', 'whitespace', 'spaces']\n",
      "predicted : ['git', 'github', 'git-commit', 'git-diff', 'gitignore']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 16 : ['version', 'r', 'java', 'file', 'generate']\n",
      "real tags : ['android', 'eclipse', 'eclipse-plugin', 'adt', 'r.java-file']\n",
      "predicted : ['java', 'android', 'c#', 'xml', 'c']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 17 : ['hibernate', 'generic']\n",
      "real tags : ['java', 'hibernate', 'generics', 'inheritance', 'orm']\n",
      "predicted : ['java', 'hibernate', 'jpa', 'generics', 'language-design']\n",
      "precision : 0.6\n",
      "jaccard : 0.42857142857142855 \n",
      "\n",
      "doc 18 : ['scroll', 'issue', 'bootstrap', 'modal']\n",
      "real tags : ['javascript', 'jquery', 'html', 'twitter-bootstrap', 'bootstrap-modal']\n",
      "predicted : ['javascript', 'twitter-bootstrap', 'css', 'jquery', 'html']\n",
      "precision : 0.8\n",
      "jaccard : 0.6666666666666666 \n",
      "\n",
      "doc 19 : ['save', 'array', 'size']\n",
      "real tags : ['python', 'arrays', 'numpy', 'hdf5', 'h5py']\n",
      "predicted : ['arrays', 'numpy', 'python', 'objective-c', 'c']\n",
      "precision : 0.6\n",
      "jaccard : 0.42857142857142855 \n",
      "\n",
      "doc 20 : ['c', 'capture', 'image', 'webcam']\n",
      "real tags : ['c#', 'image', 'file', 'webcam', 'capture']\n",
      "predicted : ['android', 'c#', 'c++', 'java', 'png']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 21 : ['component', 'sas', 'sc']\n",
      "real tags : ['css', 'reactjs', 'sass', 'styled-components', 'reflow']\n",
      "predicted : ['javascript', 'reactjs', 'typescript', 'angular', 'vue.js']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "doc 22 : ['woocommerce', 'display', 'category', 'name']\n",
      "real tags : ['php', 'wordpress', 'woocommerce', 'categories', 'woocommerce-theming']\n",
      "predicted : ['python', 'javascript', 'android', 'ios', 'java']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 23 : ['make', 'custom', 'style', 'work', 'device']\n",
      "real tags : ['android', 'android-layout', 'android-activity', 'android-manifest', 'android-styles']\n",
      "predicted : ['javascript', 'assembly', 'ios', 'jquery', 'google-chrome']\n",
      "precision : 0.0\n",
      "jaccard : 0.0 \n",
      "\n",
      "doc 24 : ['name', 'convention', 'case']\n",
      "real tags : ['java', 'naming-conventions', 'uppercase', 'camelcasing', 'abbreviation']\n",
      "predicted : ['sql', 'javascript', 'postgresql', 'java', 'python']\n",
      "precision : 0.2\n",
      "jaccard : 0.1111111111111111 \n",
      "\n",
      "precision_moyenne knn_loaded : 0.264\n",
      "jaccard_moyen knn_loaded : 0.17142857142857137 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ... vs knn_reloaded\n",
    "\n",
    "# same code for preprocessing\n",
    "\n",
    "target='all_tags'\n",
    "n=5\n",
    "\n",
    "documents = train_df[feature].tolist()\n",
    "gensim_dictionary = Dictionary(documents)\n",
    "corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "dense_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "target_values = train_df[target].values\n",
    "\n",
    "# no training, already done\n",
    "\n",
    "# eval\n",
    "\n",
    "predictions=[]\n",
    "min_range=0\n",
    "max_range=25 # test.shape[0]=4767\n",
    "for i in range(min_range, max_range):\n",
    "    query_document = test_df[feature][i]\n",
    "    print(f'doc {i} : {query_document}')\n",
    "    print(f'real tags : {test_df[target][i]}')\n",
    "    query_bow = gensim_dictionary.doc2bow(query_document)\n",
    "    query_vector = corpus2dense([query_bow], num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Find nearest neighbors\n",
    "    _, indices = knn_loaded.kneighbors(query_vector)\n",
    "\n",
    "    # Aggregate tags from neighbors\n",
    "    neighbor_tags = [tag for i in indices.flatten() for tag in train_df.iloc[i][target]]\n",
    "\n",
    "    # Predict tags based on most common tags among neighbors\n",
    "    predicted_tags = [tag for tag, _ in Counter(neighbor_tags).most_common(n=n)]\n",
    "    # 5 tags/question en moyenne mais on peut suggérer +\n",
    "    predictions.append(predicted_tags)\n",
    "    print(f'predicted : {predicted_tags}')\n",
    "    print(f'precision : {precision_topics(test_df[target][i], predicted_tags)}')\n",
    "    print(f'jaccard : {jaccard_similarity(test_df[target][i], predicted_tags)}', '\\n')\n",
    "\n",
    "true_tags = [tags for tags in test_df[target][min_range:max_range]]\n",
    "\n",
    "# eval\n",
    "scorer = precision_score\n",
    "precision = scorer(true_tags, predictions)\n",
    "\n",
    "scorer = jaccard_score\n",
    "jaccard = scorer(true_tags, predictions)\n",
    "\n",
    "# Resultat\n",
    "print(f\"precision_moyenne knn_loaded : {precision}\")\n",
    "print(f\"jaccard_moyen knn_loaded : {jaccard}\", '\\n')\n",
    "\n",
    "# Nickel, identique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Nested mlflow runs pour recherche manuelle du nb optimal de voisins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tried different values here, see ui\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=\"k_opti_small_values\", experiment_id=experiment_id) as parent:\n",
    "    precision = []\n",
    "    k_values = list(range(1, 25, 2))\n",
    "\n",
    "    for k in k_values:\n",
    "        with mlflow.start_run(run_name=f'child_{k}', nested=True) as child:\n",
    "            print(f'RUN ID child_{k}:', child.info.run_id)\n",
    "\n",
    "            parameters = {\n",
    "                \"preprocessing\": 'bow',\n",
    "                \"feature\": 'title_nltk',\n",
    "                \"k neighbors\": k,\n",
    "                \"target\": 'all_tags',\n",
    "                \"nb_tags_predicted\": 5\n",
    "            }\n",
    "            mlflow.log_params(parameters)\n",
    "\n",
    "            _, precision_moyenne, jaccard_moyen = predict_tags_using_knn(k=k)\n",
    "\n",
    "            metrics = {\n",
    "                \"precision\": precision_moyenne,\n",
    "                \"jaccard\": jaccard_moyen\n",
    "            }\n",
    "            # multiple metrics\n",
    "            mlflow.log_metrics(metrics)\n",
    "\n",
    "            precision.append((k, jaccard_moyen))\n",
    "\n",
    "    jaccard_moyen = [item[1] for item in precision]\n",
    "\n",
    "    # Plot precision scores against k values\n",
    "    plt.plot(k_values, jaccard_moyen, marker='o')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Jaccard Score for Different k Values')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(\"./artifacts/jaccard_plot_1_25.png\")\n",
    "    # Log the saved figure using MLflow\n",
    "    mlflow.log_artifact(\"./artifacts/jaccard_plot_1_25.png\")\n",
    "\n",
    "    # Show the plot (optional)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# k=13 sur la moitie des donnes, jaccard = 0.11\n",
    "# k=19 sur donnes completes, jaccard = 0.17\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Influence du nb de topics prédits sur la précision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"tags_opti\", experiment_id=experiment_id) as parent:\n",
    "    precision = []\n",
    "    nb_tags = [3, 4, 5, 10, 20]\n",
    "\n",
    "    for n in nb_tags:\n",
    "        with mlflow.start_run(run_name=f'child_{n}', nested=True) as child:\n",
    "            print(f'RUN ID child_{n}:', child.info.run_id)\n",
    "\n",
    "            parameters = {\n",
    "                \"preprocessing\": 'bow',\n",
    "                \"feature\": 'title_nltk',\n",
    "                \"k neighbors\": 13,\n",
    "                \"target\": 'all_tags',\n",
    "                \"nb_tags_predicted\": n\n",
    "            }\n",
    "            mlflow.log_params(parameters)\n",
    "\n",
    "            _, precision_moyenne, jaccard_moyen = predict_tags_using_knn(k=13, n=n)\n",
    "\n",
    "            metrics = {\n",
    "                \"precision\": precision_moyenne,\n",
    "                \"jaccard\": jaccard_moyen\n",
    "            }\n",
    "\n",
    "            # multiple metrics\n",
    "            mlflow.log_metrics(metrics)\n",
    "\n",
    "            precision.append((n, jaccard_moyen))\n",
    "\n",
    "    jaccard_moyen = [item[1] for item in precision]\n",
    "\n",
    "    # Plot precision scores against k values\n",
    "    plt.plot(nb_tags, jaccard_moyen, marker='o')\n",
    "    plt.xlabel('n')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision Score for Different n Values')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(\"./artifacts/jaccard_plot_3_4_tags.png\")\n",
    "    # Log the saved figure using MLflow\n",
    "    mlflow.log_artifact(\"./artifacts/jaccard_plot_3_4_tags.png\")\n",
    "\n",
    "    # Show the plot (optional)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Preprocessing (feature et target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous avons besoin de transformer la target pdt le preprocessing (-> bow)\n",
    "# mm si on ne s'sn sert pas vraiment, car grid_search.fit() n'accepte que des valeurs numériques.\n",
    "\n",
    "# Avantage : on peut utiliser des metriques classiques pour le score (ici r2),\n",
    "# mais ca n'a aucun sens metier interpretable.\n",
    "# peut tjs etre utile si fortement correlé à notre precision score custom\n",
    "# ou au score de similarité Jaccard\n",
    "\n",
    "# Compliqué à faire dans le pipeline sckikit, qui transforme les features mais pas la target.\n",
    "# TransformedTargetRegressor ne convient pas non plus ici : c'est un modele wrapper,\n",
    "# utilisé apres le pipeline.\n",
    "# trouvé qq \"solutions\" + ou - elegantes, mais rien de compatible à la fois avec sklearn et mlflow.\n",
    "\n",
    "# Ici convertir les tags en bag of words ou les one hot encoder revient exactement au meme, donc\n",
    "# autant utiliser le bow, on a deja le transformeur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plus tard je vais me rendre compte que ce serait bcp mieux de tjs utiliser le mm dico ici\n",
    "\n",
    "\n",
    "def token_list_into_bow(X):\n",
    "    documents = X.tolist()\n",
    "    # print(documents)\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    bow_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    return bow_matrix, gensim_dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 (tentative de) gridsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prend trop de ressources !\n",
    "# les runs mlflow sont + pratiques pour encapsuler le code\n",
    "\n",
    "# En plus je me demande si la gridsearch est une bonne idée pour le knn.\n",
    "# deja pour la lda et la nmf j'avais des doutes\n",
    "# a verifier mais pour ces modeles je pense qu'un dataset d'entrainement plus petit peut\n",
    "# serieusement impacter le nb optimal pour l'hyperparam\n",
    "# (voisins pour le knn, topics pour lda et nmf)\n",
    "\n",
    "# tester hyperopt ?\n",
    "\n",
    "\n",
    "def pipe_knn(train_df=train_df, feature=feature, target='all_tags', test_df=test_df, metric='cosine'):\n",
    "    # Load your training data and labels\n",
    "    X_train = train_df[feature].values\n",
    "    y_train = train_df[target].values\n",
    "\n",
    "    X_bow_matrix, _ = token_list_into_bow(X_train)\n",
    "    y_bow_matrix, _ = token_list_into_bow(y_train)\n",
    "\n",
    "    # Create a KNN Regressor\n",
    "    knn_regressor = KNeighborsRegressor(metric=metric)\n",
    "\n",
    "    # Create a pipeline with preprocessing and a knn regressor, to simplify gridsearch\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"knn_regressor\", knn_regressor)\n",
    "    ])\n",
    "\n",
    "    # Define hyperparameters and their possible values for grid search\n",
    "    param_grid = {\n",
    "        'knn_regressor__n_neighbors': [11, 13, 15],\n",
    "        'knn_regressor__weights': ['uniform'] # , 'distance'\n",
    "    }\n",
    "\n",
    "    # Create the GridSearchCV object with multiple scoring metrics\n",
    "    # scoring = {'neg_mean_squared_error': 'neg_mean_squared_error', 'r2': 'r2'}\n",
    "    grid_search = GridSearchCV(pipe, param_grid=param_grid,\n",
    "                            scoring='r2', cv=3, verbose=1) # add, refit='precision' for multiple scoring\n",
    "\n",
    "    # Fit the GridSearchCV object to your training data to perform hyperparameter tuning\n",
    "    grid_search.fit(X_bow_matrix, y_bow_matrix)\n",
    "\n",
    "    # Access the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "    # Create the KNN regressor with the best hyperparameters\n",
    "    best_knn_regressor = KNeighborsRegressor(# metric=metric,\n",
    "                                             n_neighbors=best_params['knn_regressor__n_neighbors'],\n",
    "                                             weights=best_params['knn_regressor__weights'])\n",
    "\n",
    "    # Create a pipeline with the preprocessor and the tuned knn regressor\n",
    "    pipeline_with_tuned_knn = Pipeline(steps=[\n",
    "        (\"knn_regressor\", best_knn_regressor)  # Use the tuned neighbor and weight values here\n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation (on training set) and display the scores for each split\n",
    "    # scoring = ['r2', 'neg_mean_squared_error']\n",
    "    cv_scores = cross_validate(pipeline_with_tuned_knn, X_bow_matrix, y_bow_matrix, cv=5, scoring='r2')\n",
    "    # print(\"Cross-Validation Scores (training):\", '\\n', cv_scores)\n",
    "    print(\"Cross-Validation Scores:\")\n",
    "    pprint(cv_scores)\n",
    "    for i, score in enumerate(cv_scores['test_score']):\n",
    "        print(f\"Split {i+1} : precision = {score}\")\n",
    "\n",
    "\n",
    "# crash le kernel\n",
    "# pipe_knn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_df = raw_data[::100]\n",
    "\n",
    "train_df, test_df = train_test_split(quick_df, test_size=0.1, random_state=42)\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On oublie la gridsearchcv, mais on garde le scorer\n",
    "# pour evaluer notre modele sur tt le training / testing set\n",
    "# (pas seulement les 25 premieres lignes du set)\n",
    "\n",
    "\n",
    "def pipe_knn(train_df=train_df, feature=feature, target='all_tags', test_df=test_df, metric='cosine'):\n",
    "    # Load your training data and labels\n",
    "    X_train = train_df[feature].values\n",
    "    y_train = train_df[target].values\n",
    "\n",
    "    X_bow_matrix, _ = token_list_into_bow(X_train)\n",
    "    y_bow_matrix, _ = token_list_into_bow(y_train)\n",
    "\n",
    "    # Create a KNN Regressor\n",
    "    knn_regressor = KNeighborsRegressor(metric=metric)\n",
    "\n",
    "    # Define hyperparameters and their possible values for tuning\n",
    "    params = {\n",
    "        'n_neighbors': 13,\n",
    "        'weights': 'uniform'\n",
    "    }\n",
    "\n",
    "    # Create the KNN regressor with the specified hyperparameters\n",
    "    knn_regressor.set_params(**params)\n",
    "\n",
    "    # Fit the KNN regressor to your training data\n",
    "    knn_regressor.fit(X_bow_matrix, y_bow_matrix)\n",
    "\n",
    "    # Evaluate the model using a scorer\n",
    "    scorer = make_scorer(r2_score)\n",
    "    score = scorer(knn_regressor, X_bow_matrix[::100], y_bow_matrix[::100])\n",
    "    print(\"R2 Score:\", score)\n",
    "\n",
    "    return knn_regressor\n",
    "\n",
    "\n",
    "# Demande bcp trop de ressources sur le training set complet. (31.5 GiB pour y_pred)\n",
    "# Il faut se contenter d'un sample.\n",
    "# Utiliser un set de validation ici ?\n",
    "\n",
    "# Du coup pour pouvoir utiliser jaccard en score, on a besoin de la méthode .predict() des classifiers.\n",
    "\n",
    "pipe_knn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Utilisation d'un classifier multilabel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 nb de tags pris en compte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser un regresseur permet de facilement travailler avec l'ensemble des tags, mais\n",
    "# nécessite une méthode .predict() custom, et les prédictions sont très lentes :\n",
    "# impossible d'évaluer notre modèle sur une partie importante du dataset,\n",
    "# on doit se contenter d'un petit sample.\n",
    "\n",
    "# Voyons si un regresseur multilabel peut résoudre ces 2 problèmes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruire le dictionnaire {tag : frequence}\n",
    "# pour travailler sur les n tags les + frequents\n",
    "\n",
    "all_tags = [tag for tags in raw_data['all_tags'] for tag in tags]\n",
    "print(f'Il y a {len(all_tags)} tags au total. \\n')\n",
    "print(f'Il y a {len(set(all_tags))} tags différents. \\n')\n",
    "\n",
    "# display(questions_tags)\n",
    "\n",
    "# Fréquence\n",
    "tag_frequencies_dict = {}\n",
    "for tag in all_tags:\n",
    "    tag_frequencies_dict[tag] = tag_frequencies_dict.get(tag, 0) + 1\n",
    "\n",
    "# Sort the dictionary items by values in descending order\n",
    "sorted_tag_frequencies = dict(sorted(tag_frequencies_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Extract tags and frequencies\n",
    "tags = list(sorted_tag_frequencies.keys())\n",
    "frequencies = list(sorted_tag_frequencies.values())\n",
    "\n",
    "df_freq = pd.DataFrame({'Tag': tags, 'Frequency': frequencies})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_most_frequent_tags(list_tags, df_freq=df_freq, lim=100):\n",
    "    \"\"\"\n",
    "    Keep only the n most frequent tags (default 100).\n",
    "\n",
    "    Parameters:\n",
    "    - liste_tokens (list): The input list of tokens to be processed.\n",
    "    - forbidden_list (set): A set of forbidden tokens, which includes stop words,\n",
    "      tokens exceeding upper frequency limits, and tokens falling below lower frequency limits.\n",
    "\n",
    "    Returns:\n",
    "    list: A filtered list of tokens that excludes stop words, tokens exceeding upper frequency limits,\n",
    "    and tokens falling below lower frequency limits.\n",
    "    \"\"\"\n",
    "\n",
    "    mandatory_list = df_freq['Tag'][:lim].tolist()\n",
    "    filtered_list = [token for token in list_tags if token in mandatory_list]\n",
    "\n",
    "    return filtered_list\n",
    "\n",
    "\n",
    "raw_data['top_tags_50'] = raw_data['all_tags'].apply(\n",
    "    lambda x: keep_most_frequent_tags(x)\n",
    ")\n",
    "\n",
    " # info targets vides\n",
    "print(f'nb de targets desormais vides : {raw_data.loc[raw_data[\"top_tags_50\"].apply(len) == 0, :].shape[0]} \\\n",
    "       (/ 50 000)', '\\n')\n",
    "\n",
    "display(raw_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiLabelBinarizer: Does not handle missing values directly. Missing values need to be handled\n",
    "# before applying the transformation.\n",
    "# not so sure about that\n",
    "\n",
    "\n",
    "print(raw_data.shape)\n",
    "\n",
    "data_50 = raw_data.loc[raw_data[\"top_tags_50\"].apply(len) >= 0, :].copy()\n",
    "\n",
    "print(data_50.shape, '\\n')\n",
    "\n",
    "display(data_50.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_df = data_50[::100]\n",
    "\n",
    "train_df, test_df = train_test_split(quick_df, test_size=0.1, random_state=42)\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 dictionnaire (gensim) standard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nécessaire pour predire sans data leak\n",
    "\n",
    "_, standard_dict_title = token_list_into_bow(train_df[feature])\n",
    "\n",
    "\n",
    "def token_list_into_bow_using_specific_dict(X, dict=standard_dict_title):\n",
    "    documents = X.tolist()\n",
    "    # print(documents)\n",
    "    gensim_dictionary = dict\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    bow_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    return bow_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Score : comparaison training / testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier(train_df=train_df, feature=feature, k=13, target='top_tags_50', test_df=test_df,\n",
    "                   dict=standard_dict_title):\n",
    "\n",
    "    # Load training data and labels\n",
    "    X_train = train_df[feature].values\n",
    "    X_test = test_df[feature].values\n",
    "    y_train = train_df[target].tolist()\n",
    "    y_test = test_df[target].tolist()\n",
    "\n",
    "    # training set\n",
    "    X_bow_matrix = token_list_into_bow_using_specific_dict(X_train, dict)\n",
    "    # testing set\n",
    "    X_test_bow_matrix = token_list_into_bow_using_specific_dict(X_test, dict)\n",
    "\n",
    "    # target encoding\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y_encoded = mlb.fit_transform(y_train)\n",
    "    y_test_encoded = mlb.fit_transform(y_test)\n",
    "\n",
    "    # Create a KNN classifier this time\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    # Define hyperparameters and their possible values for tuning\n",
    "    params = {\n",
    "        'n_neighbors': k\n",
    "    }\n",
    "\n",
    "    # Create the KNN regressor with the specified hyperparameters\n",
    "    knn.set_params(**params)\n",
    "\n",
    "    # Fit the KNN regressor to your training data\n",
    "    knn.fit(X_bow_matrix, y_encoded)\n",
    "\n",
    "    # Evaluate the model on training data using precision\n",
    "    print('SCORES TAGS ENCODÉS (MLB)', '\\n')\n",
    "    predictions = knn.predict(X_bow_matrix)\n",
    "\n",
    "    scorer = precision_score\n",
    "    score = scorer(predictions, y_encoded)\n",
    "    print(\"Precision score (training set): \", score)\n",
    "\n",
    "    # Evaluate the model on training data using jaccard similarity\n",
    "    scorer = jaccard_score\n",
    "    score = scorer(predictions, y_encoded)\n",
    "    print(\"Jaccard similarity (training set): \", score)\n",
    "    print('\\n')\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    predictions = knn.predict(X_test_bow_matrix)\n",
    "    scorer = precision_score\n",
    "    score = scorer(predictions, y_test_encoded)\n",
    "    print(\"Precision score (testing set): \", score)\n",
    "\n",
    "    scorer = jaccard_score\n",
    "    score = scorer(predictions, y_test_encoded)\n",
    "    print(\"Jaccard similarity (testing set): \", score)\n",
    "\n",
    "    return knn, mlb, dict\n",
    "\n",
    "\n",
    "# knn_100, mlb_100, dict_100 = knn_classifier()\n",
    "\n",
    "# pb precision ??\n",
    "# OK hypothèse : j'ai modifié la fonction precision_score() pour qu'elle ne tienne pas compte de l'ordre.\n",
    "# C'était + pratiquee pour comparer les tags predits aux tags reels (sous forme de listes de tokens)\n",
    "# Mais si on les compare sous forme encodée, alors l'ordre importe. Pour ma fonction presque tous les\n",
    "# mots sont identiques : 000000000000000000010000000000000000000...\n",
    "\n",
    "# Solution : utiliser notre fonction custom seulement pour comparer les tokens,\n",
    "# mais pas les representations bag of words.\n",
    "\n",
    "# checker exemples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier(train_df=train_df, feature=feature, k=13, target='top_tags_50', test_df=test_df,\n",
    "                   dict=standard_dict_title):\n",
    "\n",
    "    # Load training data and labels\n",
    "    X_train = train_df[feature].values\n",
    "    X_test = test_df[feature].values\n",
    "    y_train = train_df[target].tolist()\n",
    "    y_test = test_df[target].tolist()\n",
    "\n",
    "    # training set\n",
    "    X_bow_matrix = token_list_into_bow_using_specific_dict(X_train, dict)\n",
    "    # testing set\n",
    "    X_test_bow_matrix = token_list_into_bow_using_specific_dict(X_test, dict)\n",
    "\n",
    "    # target encoding\n",
    "    mlb_train = MultiLabelBinarizer()\n",
    "    y_encoded = mlb_train.fit_transform(y_train)\n",
    "\n",
    "    # Create a KNN classifier this time\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    # Define hyperparameters and their possible values for tuning\n",
    "    params = {\n",
    "        'n_neighbors': k\n",
    "    }\n",
    "\n",
    "    # Create the KNN regressor with the specified hyperparameters\n",
    "    knn.set_params(**params)\n",
    "\n",
    "    # Fit the KNN regressor to your training data\n",
    "    knn.fit(X_bow_matrix, y_encoded)\n",
    "\n",
    "    predictions_train = knn.predict(X_bow_matrix)\n",
    "    predictions_test = knn.predict(X_test_bow_matrix)\n",
    "\n",
    "    print('SCORES TAGS (TOKENS)', '\\n')\n",
    "\n",
    "    predicted_tags_train = mlb_train.inverse_transform(predictions_train)\n",
    "    predicted_tags_test = mlb_train.inverse_transform(predictions_test)\n",
    "\n",
    "    # Evaluate the model on training data using precision\n",
    "    scorer = precision_score\n",
    "    score = scorer(predicted_tags_train, y_train)\n",
    "    print(\"Precision score (training set): \", score)\n",
    "\n",
    "    # Evaluate the model on training data using jaccard similarity\n",
    "    scorer = jaccard_score\n",
    "    score = scorer(predicted_tags_train, y_train)\n",
    "    print(\"Jaccard similarity (training set): \", score)\n",
    "    print('\\n')\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    scorer = precision_score\n",
    "    score = scorer(predicted_tags_test, y_test)\n",
    "    print(\"Precision score (testing set): \", score)\n",
    "\n",
    "    scorer = jaccard_score\n",
    "    score = scorer(predicted_tags_test, y_test)\n",
    "    print(\"Jaccard similarity (testing set): \", score)\n",
    "\n",
    "\n",
    "    return knn, mlb_train, dict\n",
    "\n",
    "\n",
    "knn_50, mlb_50, dict_50 = knn_classifier()\n",
    "\n",
    "# Ah, enfin qqch de coherent !\n",
    "# bcp moins bon du coup, mais coherent\n",
    "\n",
    "# En fait la précision \"profite\" des cas, qui semblent nombreux ici, où un seul tag est prédit.\n",
    "# Jaccard est vraiment + robuste\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint (+ choix target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_top_tags_feature(raw_data=raw_data, n=50, slice=20):\n",
    "    raw_data['top_tags'] = raw_data['all_tags'].apply(\n",
    "        lambda x: keep_most_frequent_tags(x, lim=n)\n",
    "    )\n",
    "\n",
    "    # Pas sûr que la ligne suivante soit indispensable\n",
    "    data = raw_data.loc[raw_data['top_tags'].apply(len) >= 0, :].copy()\n",
    "    print(data.shape, '\\n')\n",
    "\n",
    "    slice_df = data[::slice]\n",
    "    # slice_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train_df, test_df = train_test_split(slice_df, test_size=0.1, random_state=42)\n",
    "\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "train_df, test_df = create_top_tags_feature(n=30, slice=100)\n",
    "_, standard_dict_title = token_list_into_bow(train_df[feature])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(raw_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_50, mlb_50, dict_50 = knn_classifier(train_df=train_df, k=19, target='top_tags', test_df=test_df)\n",
    "\n",
    "# Ici je teste differentes valeurs pour le nb de top_tags.\n",
    "# J'espérais qu'en prenant en compte davantage de tags, on aurait moins de targets vides\n",
    "# et donc (hopefully) moins de prédictions nulles.\n",
    "\n",
    "# Problèmes : le tps d'entrainement explose...\n",
    "# # Si on garde les 5000 tags les + frequents on passe de qq minutes (2 min pour 50 tags) à...\n",
    "# c'est long ! (+ d'une heure)\n",
    "# Pourtant les targets st tjs assez vides, et bcp de prédictions .predict() st tjs nulles.\n",
    "\n",
    "# ... et le score chute.\n",
    "\n",
    "# En fait mon knn bricolé au début était pas si mal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_query_into_bow_vector(query, dictionary):\n",
    "    query_token_list = preprocess_text(query)\n",
    "    query_bow = dictionary.doc2bow(query_token_list)\n",
    "    query_vector = corpus2dense([query_bow], num_terms=len(dictionary)).T\n",
    "\n",
    "    return query_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 predict...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "\n",
    "def predict_tags(test_data, model, mlb_encoder, dictionary):\n",
    "    # Transform test features\n",
    "    query = string_query_into_bow_vector(test_data, dictionary)\n",
    "\n",
    "    # Make predictions\n",
    "    predicted_labels = model.predict(query)\n",
    "\n",
    "    # Inverse transform predicted labels\n",
    "    predicted_tags_tuple = mlb_encoder.inverse_transform(predicted_labels)\n",
    "    predicted_tags = list(predicted_tags_tuple[0]) # mlb retourne un array\n",
    "\n",
    "    return query, predicted_tags\n",
    "\n",
    "\n",
    "def test_prediction(index=0):\n",
    "    test_data = test_df['title'][index]\n",
    "    _, predicted_tags = predict_tags(test_data, knn_50, mlb_50, dict_50)\n",
    "\n",
    "    print(f\"All tags : {test_df['all_tags'][index]}\")\n",
    "    print(f\"Target : {test_df['top_tags'][index]}\")\n",
    "    print(f\"Predicted : {predicted_tags}\", '\\n')\n",
    "\n",
    "    # scores\n",
    "    scorer = precision_topics\n",
    "    score = scorer(predicted_tags, test_df['top_tags'][index])\n",
    "    print(\"Precision score : \", score)\n",
    "\n",
    "    scorer = jaccard_similarity\n",
    "    score = scorer(predicted_tags, test_df['top_tags'][index])\n",
    "    print(\"Jaccard similarity : \", score)\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "test_prediction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    test_prediction(i)\n",
    "\n",
    "# bcp de predictions nulles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 ... VS predict_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def always_predict_tags(test_data, knn_model=knn_50, mlb_encoder=mlb_50, dictionary=dict_50, top_n=5):\n",
    "    # Transform test features\n",
    "    query = string_query_into_bow_vector(test_data, dictionary)\n",
    "\n",
    "    # Make predictions and get probabilities\n",
    "    probabilities = knn_model.predict_proba(query)\n",
    "    # print(probabilities[:3])\n",
    "\n",
    "    # Extract tags from the mlb_encoder\n",
    "    tags = mlb_encoder.classes_\n",
    "\n",
    "    # Create a dictionary to store results\n",
    "    results = {}\n",
    "\n",
    "    # Iterate over probabilities and tags\n",
    "    for i, tag in enumerate(tags):\n",
    "        # Extract the probability for the current tag\n",
    "        probability = probabilities[i][0][0]\n",
    "        # Store the probability with the tag as the key\n",
    "        results[tag] = probability\n",
    "\n",
    "    # Sort the dictionary items by values in descending order\n",
    "    sorted_results = dict(sorted(results.items(), key=lambda item: item[1], reverse=True))\n",
    "    predicted_tags = list(sorted_results)[:top_n]\n",
    "    # Construct a list of tuples containing tags and their corresponding probabilities\n",
    "    tags_with_probas = [(tag, sorted_results[tag]) for tag in predicted_tags]\n",
    "\n",
    "    return tags_with_probas\n",
    "\n",
    "\n",
    "def test_prediction(index=0):\n",
    "    predicted_tags = always_predict_tags(test_df['title'][index])\n",
    "    print(test_df['all_tags'][index])\n",
    "    print(test_df['top_tags'][index])\n",
    "    print(predicted_tags, '\\n')\n",
    "\n",
    "\n",
    "test_prediction()\n",
    "\n",
    "# bcp de probas == 1 alors qu'on n'avait pas de prediction (ou 1) sur cet exemple ??\n",
    "# On a exactement le problème inverse !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    test_prediction(i)\n",
    "\n",
    "# bcp trop de valeurs à 1\n",
    "# predit svt la mm chose (ordre alphabetique)\n",
    "\n",
    "# pour le knn on peut tjs utiliser notre predict custom, mais pour les autres modeles\n",
    "# il faudra une difference plus nette.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Autres classifieur multilabels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = create_top_tags_feature(n=100, slice=20)\n",
    "_, standard_dict_title = token_list_into_bow(train_df[feature])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pas nativement mutilabel -> approche one vs rest via MultiOutputClassifier()\n",
    "\n",
    "\n",
    "def logistic_regression_classifier(train_df=train_df, feature=feature, dict=standard_dict_title,\n",
    "                                   target='top_tags', test_df=test_df):\n",
    "\n",
    "    X_train = train_df[feature].values\n",
    "    X_test = test_df[feature].values\n",
    "    y_train = train_df[target].tolist()\n",
    "    y_test = test_df[target].tolist()\n",
    "\n",
    "    # training set\n",
    "    X_bow_matrix = token_list_into_bow_using_specific_dict(X_train, dict)\n",
    "    # testing set\n",
    "    X_test_bow_matrix = token_list_into_bow_using_specific_dict(X_test, dict)\n",
    "\n",
    "    # target encoding\n",
    "    mlb_train = MultiLabelBinarizer()\n",
    "    y_encoded = mlb_train.fit_transform(y_train)\n",
    "\n",
    "    # Create a Logistic Regression classifier\n",
    "    logistic_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "    # Create a MultiOutputClassifier with the Logistic Regression classifier\n",
    "    multi_output_classifier = MultiOutputClassifier(logistic_reg)\n",
    "\n",
    "    # Fit the MultiOutputClassifier to your training data\n",
    "    multi_output_classifier.fit(X_bow_matrix, y_encoded)\n",
    "    print('training fini', '\\n')\n",
    "\n",
    "    predictions_train = multi_output_classifier.predict(X_bow_matrix)\n",
    "    predictions_test = multi_output_classifier.predict(X_test_bow_matrix)\n",
    "\n",
    "    print('SCORES TAGS (TOKENS)', '\\n')\n",
    "\n",
    "    predicted_tags_train = mlb_train.inverse_transform(predictions_train)\n",
    "    predicted_tags_test = mlb_train.inverse_transform(predictions_test)\n",
    "\n",
    "    # Evaluate the model on training data using precision\n",
    "    scorer = precision_score\n",
    "    score = scorer(predicted_tags_train, y_train)\n",
    "    print(\"Precision score (training set): \", score)\n",
    "\n",
    "    # Evaluate the model on training data using jaccard similarity\n",
    "    scorer = jaccard_score\n",
    "    score = scorer(predicted_tags_train, y_train)\n",
    "    print(\"Jaccard similarity (training set): \", score)\n",
    "    print('\\n')\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    scorer = precision_score\n",
    "    score = scorer(predicted_tags_test, y_test)\n",
    "    print(\"Precision score (testing set): \", score)\n",
    "\n",
    "    scorer = jaccard_score\n",
    "    score = scorer(predicted_tags_test, y_test)\n",
    "    print(\"Jaccard similarity (testing set): \", score)\n",
    "\n",
    "\n",
    "    return multi_output_classifier, mlb_train\n",
    "\n",
    "\n",
    "multi_output_classifier, mlb_50 = logistic_regression_classifier()\n",
    "\n",
    "# create_top_tags_feature(n=100, slice=3)\n",
    "\n",
    "# Precision score (training set):  0.5929012654012678\n",
    "# Jaccard similarity (training set):  0.44822494172494076\n",
    "\n",
    "# Precision score (testing set):  0.45291588000839084\n",
    "# Jaccard similarity (testing set):  0.32760196589649054\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "\n",
    "def test_prediction(index, model):\n",
    "    query = test_df['title'][index]\n",
    "    _, predicted_tags = predict_tags(query, model, mlb_50, standard_dict_title)\n",
    "    print(test_df['all_tags'][index])\n",
    "    print(test_df['top_tags'][index])\n",
    "    print(predicted_tags, '\\n')\n",
    "\n",
    "\n",
    "test_prediction(0, multi_output_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,20):\n",
    "    test_prediction(i, multi_output_classifier)\n",
    "\n",
    "# Pas mal au début, bcp de vide ensuite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict proba\n",
    "\n",
    "\n",
    "def test_prediction_proba(index, model):\n",
    "    query = test_df['title'][index]\n",
    "    predicted_tags = always_predict_tags(query, model, mlb_50, standard_dict_title)\n",
    "    print(test_df['all_tags'][index])\n",
    "    print(test_df['top_tags'][index])\n",
    "    print(predicted_tags, '\\n')\n",
    "\n",
    "\n",
    "for i in range(0,20):\n",
    "    test_prediction_proba(i, multi_output_classifier)\n",
    "\n",
    "# Par rapport aux résultats du knn, au moins ici il varie les réponses\n",
    "# mais les prédictions semblent très mauvaises.\n",
    "\n",
    "# Je ne comprends pas pourquoi on ne retrouve pas les mm prédictions qd on utilise\n",
    "# .predict() ou .predict_proba() ?? Ce sont 2 méthodes du mm modèle..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 SGDClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = create_top_tags_feature(n=100, slice=20)\n",
    "_, standard_dict_title = token_list_into_bow(train_df[feature])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using default SVM\n",
    "\n",
    "\n",
    "def sgd_classifier(train_df=train_df, feature=feature, target='top_tags',\n",
    "                   dict=standard_dict_title, test_df=test_df):\n",
    "\n",
    "    # Load your training data and labels\n",
    "    X_train = train_df[feature].values\n",
    "    X_test = test_df[feature].values\n",
    "    y_train = train_df[target].tolist()\n",
    "    y_test = test_df[target].tolist()\n",
    "\n",
    "    # training set\n",
    "    X_bow_matrix = token_list_into_bow_using_specific_dict(X_train, dict)\n",
    "    # testing set\n",
    "    X_test_bow_matrix = token_list_into_bow_using_specific_dict(X_test, dict)\n",
    "\n",
    "    # target encoding\n",
    "    mlb_train = MultiLabelBinarizer()\n",
    "    y_encoded = mlb_train.fit_transform(y_train)\n",
    "\n",
    "    # Create an SGDClassifier with default parameters\n",
    "    sgd_clf = SGDClassifier(random_state=42)  # default loss='hinge', gives a linear SVM\n",
    "\n",
    "    # Create a MultiOutputClassifier with the SGD classifier\n",
    "    multi_output_classifier = MultiOutputClassifier(sgd_clf)\n",
    "\n",
    "    # Fit the MultiOutputClassifier to your training data\n",
    "    multi_output_classifier.fit(X_bow_matrix, y_encoded)\n",
    "    print('training fini', '\\n')\n",
    "\n",
    "    predictions_train = multi_output_classifier.predict(X_bow_matrix)\n",
    "    predictions_test = multi_output_classifier.predict(X_test_bow_matrix)\n",
    "\n",
    "    print('SCORES TAGS (TOKENS)', '\\n')\n",
    "\n",
    "    predicted_tags_train = mlb_train.inverse_transform(predictions_train)\n",
    "    predicted_tags_test = mlb_train.inverse_transform(predictions_test)\n",
    "\n",
    "    # Evaluate the model on training data using precision\n",
    "    scorer = precision_score\n",
    "    score = scorer(predicted_tags_train, y_train)\n",
    "    print(\"Precision score (training set): \", score)\n",
    "\n",
    "    # Evaluate the model on training data using jaccard similarity\n",
    "    scorer = jaccard_score\n",
    "    score_train = scorer(predicted_tags_train, y_train)\n",
    "    print(\"Jaccard similarity (training set): \", score_train)\n",
    "    print('\\n')\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    scorer = precision_score\n",
    "    score = scorer(predicted_tags_test, y_test)\n",
    "    print(\"Precision score (testing set): \", score)\n",
    "\n",
    "    scorer = jaccard_score\n",
    "    score_test = scorer(predicted_tags_test, y_test)\n",
    "    print(\"Jaccard similarity (testing set): \", score_test)\n",
    "\n",
    "    return multi_output_classifier, mlb_train, score_train, score_test\n",
    "\n",
    "\n",
    "multi_output_classifier_svm, mlb_50, jaccard_train, jaccard_test = sgd_classifier()\n",
    "\n",
    "# tres bon score, bcp + rapide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict (best score)\n",
    "\n",
    "for i in range(0,20):\n",
    "    test_prediction(i, multi_output_classifier_svm)\n",
    "\n",
    "# Pas mal du tout !\n",
    "# Encore bcp de prédictions vides\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict proba\n",
    "# test_prediction_proba(0, multi_output_classifier_svm)\n",
    "\n",
    "# Ne fonctionne pas, la SVM du SGDClassifier ne possède pas la méthode predict_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using logistic reg from SGDClassifier (loss='log_loss')\n",
    "\n",
    "\n",
    "def sgd_classifier(train_df=train_df, feature=feature, target='top_tags',\n",
    "                   dict=standard_dict_title, test_df=test_df):\n",
    "\n",
    "    X_train = train_df[feature].values\n",
    "    X_test = test_df[feature].values\n",
    "    y_train = train_df[target].tolist()\n",
    "    y_test = test_df[target].tolist()\n",
    "\n",
    "    # training set\n",
    "    X_bow_matrix = token_list_into_bow_using_specific_dict(X_train, dict)\n",
    "    # testing set\n",
    "    X_test_bow_matrix = token_list_into_bow_using_specific_dict(X_test, dict)\n",
    "\n",
    "    # target encoding\n",
    "    mlb_train = MultiLabelBinarizer()\n",
    "    y_encoded = mlb_train.fit_transform(y_train)\n",
    "\n",
    "    # Create an SGDClassifier with default parameters\n",
    "    sgd_clf = SGDClassifier(random_state=42, loss='log_loss')  # for logistic regression\n",
    "\n",
    "    # Create a MultiOutputClassifier with the SGD classifier\n",
    "    multi_output_classifier = MultiOutputClassifier(sgd_clf)\n",
    "\n",
    "    # Fit the MultiOutputClassifier to your training data\n",
    "    multi_output_classifier.fit(X_bow_matrix, y_encoded)\n",
    "    print('training fini', '\\n')\n",
    "\n",
    "    predictions_train = multi_output_classifier.predict(X_bow_matrix)\n",
    "    predictions_test = multi_output_classifier.predict(X_test_bow_matrix)\n",
    "\n",
    "    print('SCORES TAGS (TOKENS)', '\\n')\n",
    "\n",
    "    predicted_tags_train = mlb_train.inverse_transform(predictions_train)\n",
    "    predicted_tags_test = mlb_train.inverse_transform(predictions_test)\n",
    "\n",
    "    # Evaluate the model on training data using precision\n",
    "    scorer = precision_score\n",
    "    score = scorer(predicted_tags_train, y_train)\n",
    "    print(\"Precision score (training set): \", score)\n",
    "\n",
    "    # Evaluate the model on training data using jaccard similarity\n",
    "    scorer = jaccard_score\n",
    "    score_train = scorer(predicted_tags_train, y_train)\n",
    "    print(\"Jaccard similarity (training set): \", score_train)\n",
    "    print('\\n')\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    scorer = precision_score\n",
    "    score = scorer(predicted_tags_test, y_test)\n",
    "    print(\"Precision score (testing set): \", score)\n",
    "\n",
    "    scorer = jaccard_score\n",
    "    score_test = scorer(predicted_tags_test, y_test)\n",
    "    print(\"Jaccard similarity (testing set): \", score_test)\n",
    "\n",
    "    return multi_output_classifier, mlb_train, score_train, score_test\n",
    "\n",
    "\n",
    "multi_output_classifier_lr, mlb_50, _, _ = sgd_classifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "for i in range(0,20):\n",
    "    test_prediction(i, multi_output_classifier_lr)\n",
    "\n",
    "# similaire à la LR testée + haut (4.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_proba\n",
    "\n",
    "for i in range(0,20):\n",
    "    test_prediction_proba(i, multi_output_classifier_lr)\n",
    "\n",
    "# idem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Random forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, test_df = create_top_n_tags_feature(n=50, slice=50)\n",
    "# _, standard_dict_title = token_list_into_bow(train_df[feature])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_classifier(train_df=train_df, feature=feature, target='top_tags',\n",
    "                             dict=standard_dict_title, test_df=test_df):\n",
    "\n",
    "    # Load your training data and labels\n",
    "    X_train = train_df[feature].values\n",
    "    X_test = test_df[feature].values\n",
    "    y_train = train_df[target].tolist()\n",
    "    y_test = test_df[target].tolist()\n",
    "\n",
    "    # training set\n",
    "    X_bow_matrix = token_list_into_bow_using_specific_dict(X_train, dict)\n",
    "    # testing set\n",
    "    X_test_bow_matrix = token_list_into_bow_using_specific_dict(X_test, dict)\n",
    "\n",
    "    # target encoding\n",
    "    mlb_train = MultiLabelBinarizer()\n",
    "    y_encoded = mlb_train.fit_transform(y_train)\n",
    "\n",
    "    # Create a Random Forest Classifier with default parameters\n",
    "    rf_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Create a MultiOutputClassifier with the Random Forest Classifier\n",
    "    multi_output_classifier = MultiOutputClassifier(rf_clf)\n",
    "\n",
    "    # Fit the MultiOutputClassifier to your training data\n",
    "    multi_output_classifier.fit(X_bow_matrix, y_encoded)\n",
    "    print('training fini', '\\n')\n",
    "\n",
    "    predictions_train = multi_output_classifier.predict(X_bow_matrix)\n",
    "    predictions_test = multi_output_classifier.predict(X_test_bow_matrix)\n",
    "\n",
    "    print('SCORES TAGS (TOKENS)', '\\n')\n",
    "\n",
    "    predicted_tags_train = mlb_train.inverse_transform(predictions_train)\n",
    "    predicted_tags_test = mlb_train.inverse_transform(predictions_test)\n",
    "\n",
    "    # Evaluate the model on training data using precision\n",
    "    scorer = precision_score\n",
    "    score = scorer(predicted_tags_train, y_train)\n",
    "    print(\"Precision score (training set): \", score)\n",
    "\n",
    "    # Evaluate the model on training data using jaccard similarity\n",
    "    scorer = jaccard_score\n",
    "    score = scorer(predicted_tags_train, y_train)\n",
    "    print(\"Jaccard similarity (training set): \", score)\n",
    "    print('\\n')\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    scorer = precision_score\n",
    "    score = scorer(predicted_tags_test, y_test)\n",
    "    print(\"Precision score (testing set): \", score)\n",
    "\n",
    "    scorer = jaccard_score\n",
    "    score = scorer(predicted_tags_test, y_test)\n",
    "    print(\"Jaccard similarity (testing set): \", score)\n",
    "\n",
    "    return multi_output_classifier, mlb_train\n",
    "\n",
    "\n",
    "multi_output_classifier_rf, mlb_50_rf = random_forest_classifier()\n",
    "\n",
    "# + lent\n",
    "# score wow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "for i in range(0,20):\n",
    "    test_prediction(i, multi_output_classifier_rf)\n",
    "\n",
    "# Plutôt pas mal... qd on a une réponse !\n",
    "# (tjs le mm problème)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict proba\n",
    "\n",
    "for i in range(0,20):\n",
    "    test_prediction_proba(i, multi_output_classifier_rf)\n",
    "\n",
    "# nope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Conclusion / classifiers multilabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La SVM a le meilleur score sur le validation set (testing set ici pour simplifier,\n",
    "# on utilisera un véritable set de validation ds la partie suivante).\n",
    "\n",
    "# C'est aussi le modèle qui semble donner le moins de prédictions nulles (méthode .predict),\n",
    "# d'après les qq exemples observés. Mais il en propose encore bcp.\n",
    "\n",
    "# Les résultats fournis par la méthode .predict_proba sont difficilement exploitables.\n",
    "\n",
    "# J'en reviens à l'idée d'une fonction .predict custom, comme pour le knn (regresseur) utilisé\n",
    "# en partie 1. Mais pas sûr comment faire qqch d'équivalent pour les autres modèles\n",
    "# (les \"forcer\" à proposer une réponse).\n",
    "\n",
    "# Problème : la performance du knn modifié est difficile à évaluer (prend trop de tps).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Drift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['CreationDate'] = pd.to_datetime(raw_data['CreationDate'])\n",
    "\n",
    "raw_data['year_month'] = raw_data['CreationDate'].map(lambda dt: dt.strftime('%Y-%m'))\n",
    "\n",
    "print(raw_data['year_month'].min())\n",
    "print(raw_data['year_month'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_month = raw_data.groupby('year_month').size().to_frame(\"count_per_month\").reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 3))  # Adjust the figsize as needed\n",
    "\n",
    "grouped_month.plot(kind='bar', x='year_month', y='count_per_month', fontsize=12, ax=ax, width=0.8)\n",
    "plt.title('nb questions par mois', fontsize=18, pad=20)\n",
    "plt.legend(loc='upper right', fontsize=12)\n",
    "plt.xticks(fontsize=5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Concernant l’analyse de la stabilité du modèle, il est attendu de prendre un dataset par mois de questions\n",
    "# sur un an, et de mesurer l’évolution des mesures et scores du modèle défini, afin d’en conclure une\n",
    "# stabilité ou pas d’un modèle de référence établi sur des questions antérieures à cette période.\"\"\n",
    "\n",
    "# test rapide avec svm\n",
    "\n",
    "month = []\n",
    "\n",
    "unique_values = np.sort(raw_data['year_month'].unique())\n",
    "print(len(unique_values))\n",
    "print(unique_values)\n",
    "\n",
    "# On ne va pas utiliser le premier mois (mai 2011) parce que d'après le graph il y a peu de questions ce mois-là.\n",
    "# Pas idéal pour entrainer notre modele,\n",
    "\n",
    "for mois in unique_values[1:13]:\n",
    "\n",
    "    month.append(raw_data.loc[raw_data['year_month'] == mois, :].copy())\n",
    "\n",
    "print(len(month))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(month[0].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(month[11].head()) # last one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ttsplit\n",
    "\n",
    "train_df, test_df = train_test_split(month[0], test_size=0.1, random_state=42)\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# month 0\n",
    "print('Mois 0 (entrainement) :', '\\n')\n",
    "\n",
    "svm, mlb, jaccard_train_0, jaccard_test_0 = sgd_classifier(train_df=train_df, feature=feature, target='top_tags',\n",
    "                                                       dict=standard_dict_title, test_df=test_df)\n",
    "\n",
    "jaccard_train = [jaccard_train_0]\n",
    "jaccard_test = [jaccard_test_0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mlflow ?\n",
    "# months 1-11\n",
    "\n",
    "for i in range(1, 12):\n",
    "    train_df, test_df = train_test_split(month[i], test_size=0.1, random_state=42)\n",
    "\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    X_train = train_df[feature].values\n",
    "    y_train = train_df['top_tags'].tolist()\n",
    "\n",
    "    X_test = test_df[feature].values\n",
    "    y_test = test_df['top_tags'].tolist()\n",
    "\n",
    "    # Training set\n",
    "    X_bow_matrix = token_list_into_bow_using_specific_dict(X_train, standard_dict_title)\n",
    "    X_test_bow_matrix = token_list_into_bow_using_specific_dict(X_test, standard_dict_title)\n",
    "\n",
    "    # Target encoding\n",
    "    # Plus necessaire ici, le modele est deja entraine\n",
    "    # y_encoded = mlb.fit_transform(y_train)\n",
    "\n",
    "    # Evaluation\n",
    "    predictions_train = svm.predict(X_bow_matrix)\n",
    "    predictions_test = svm.predict(X_test_bow_matrix)\n",
    "\n",
    "    predicted_tags_train = mlb.inverse_transform(predictions_train)\n",
    "    predicted_tags_test = mlb.inverse_transform(predictions_test)\n",
    "\n",
    "    # Evaluate the model on training data using jaccard similarity\n",
    "    scorer = jaccard_score\n",
    "    score_train = scorer(predicted_tags_train, y_train)\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    score_test = scorer(predicted_tags_test, y_test)\n",
    "\n",
    "    jaccard_train.append(score_train)\n",
    "    jaccard_test.append(score_test)\n",
    "\n",
    "\n",
    "for i in range(0, 12):\n",
    "    print(f'Mois {i} : jaccard train = {jaccard_train[i]}, jaccard test = {jaccard_test[i]}')\n",
    "\n",
    "\n",
    "# graph\n",
    "# tester evidently AI ?\n",
    "# use mlflow recipe ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Jaccard similarity scores\n",
    "plt.plot(range(0, 12), jaccard_train, color='grey', label='Jaccard Train')\n",
    "plt.plot(range(0, 12), jaccard_test, color='blue', label='Jaccard Test')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Jaccard Similarity')\n",
    "plt.title('Model Drift: Jaccard Similarity over Months')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Assez stable, le trend semble descendant mais légèrement\n",
    "# \"creux\" apres 8 mois, le score jaccard qui tourne generalement autour de 0.2 tombe à 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pb : dim bow trop importante,\n",
    "# Ca n'a aucun sens d'entrainer un réseau neuronal sur autant de poids / biais\n",
    "# avec mes ressources\n",
    "# (mm avec + de ressources, sans doute pas une idée tres efficace..)\n",
    "# (en + la plupart seraient = 0, pas idéal)\n",
    "\n",
    "# add Dimensionality Reduction ? (pca? lda/nmf?)\n",
    "\n",
    "# On comparera plutôt les modèles classiques et le NN (réseau neuronaux) sur les embeddings, denses,\n",
    "# en partie 4_2 (notebook suivant)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_p5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
