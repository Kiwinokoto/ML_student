{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Catégorisez automatiquement des questions**\n",
    "\n",
    "### partie 4/8 : Prédiction de tags, approche supervisée + tracking mlflow\n",
    "\n",
    "#### <br> Notebook d’exploration et de pré-traitement des questions, comprenant une analyse univariée et multivariée, un nettoyage des questions, un feature engineering de type bag of words avec réduction de dimension (du vocabulaire et des tags) \n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Librairies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python version 3.11.5 (main, Sep 11 2023, 13:23:44) [GCC 11.2.0]\n",
      "pyLDAvis version 3.4.0\n",
      "\n",
      "Number of CPU cores: 8\n",
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "# finir d'organiser, check inutiles ?\n",
    "\n",
    "#\n",
    "import os, sys, random\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "from pandarallel import pandarallel\n",
    "from pprint import pprint\n",
    "import json\n",
    "import requests\n",
    "from collections import Counter\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "# NLP\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import corpus2dense\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim import similarities\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "#\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.metrics import make_scorer, PredictionErrorDisplay, r2_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "print('\\nPython version ' + sys.version)\n",
    "print('pyLDAvis version ' + pyLDAvis.__version__)\n",
    "\n",
    "# Modify if necessary\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"\\nNumber of CPU cores: {num_cores}\")\n",
    "pandarallel.initialize(progress_bar=False, nb_workers=6)\n",
    "\n",
    "#\n",
    "import pickle\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.models.signature import infer_signature, ModelSignature #, Schema, ParamSchema\n",
    "from mlflow.types import Schema, ParamSchema, ParamSpec, ColSpec\n",
    "\n",
    "# os.environ['MLFLOW_TRACKING_URI'] = './'\n",
    "\n",
    "# ! REQUIRES CONSOLE COMMAND : mlflow ui\n",
    "# depuis dossier notebooks\n",
    "\n",
    "# Utilisable seulement en local...\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = MlflowClient(tracking_uri=\"http://127.0.0.1:5000\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_values(df):\n",
    "    \"\"\"Generates a DataFrame containing the count and proportion of missing values for each feature.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with columns for the feature name, count of missing values,\n",
    "        count of non-missing values, proportion of missing values, and data type for each feature.\n",
    "    \"\"\"\n",
    "    # Count the missing values for each column\n",
    "    missing = df.isna().sum()\n",
    "\n",
    "    # Calculate the percentage of missing values\n",
    "    percent_missing = df.isna().mean() * 100\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    missings_df = pd.DataFrame({\n",
    "        'column_name': df.columns,\n",
    "        'missing': missing,\n",
    "        'present': df.shape[0] - missing,  # Count of non-missing values\n",
    "        'percent_missing': percent_missing.round(2),  # Rounded to 2 decimal places\n",
    "        'type': df.dtypes\n",
    "    })\n",
    "\n",
    "    # Sort the DataFrame by the count of missing values\n",
    "    missings_df.sort_values('missing', inplace=True)\n",
    "\n",
    "    return missings_df\n",
    "\n",
    "# with pd.option_context('display.max_rows', 1000):\n",
    "#   display(get_missing_values(df))\n",
    "\n",
    "\n",
    "def quick_look(df, miss=True):\n",
    "    \"\"\"\n",
    "    Display a quick overview of a DataFrame, including shape, head, tail, unique values, and duplicates.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame to inspect.\n",
    "        check_missing (bool, optional): Whether to check and display missing values (default is True).\n",
    "\n",
    "    The function provides a summary of the DataFrame, including its shape, the first and last rows, the count of unique values per column, and the number of duplicates.\n",
    "    If `check_missing` is set to True, it also displays missing value information.\n",
    "    \"\"\"\n",
    "    print(f'shape : {df.shape}')\n",
    "\n",
    "    display(df.head())\n",
    "    display(df.tail())\n",
    "\n",
    "    print('uniques :')\n",
    "    display(df.nunique())\n",
    "\n",
    "    print('Doublons ? ', df.duplicated(keep='first').sum(), '\\n')\n",
    "\n",
    "    if miss:\n",
    "        display(get_missing_values(df))\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    #Cleaning\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # Tokenization\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(\" \".join(tokens))  # Apply RegexpTokenizer to the entire list\n",
    "\n",
    "        # Remove punctuation (make sure, RegexpTokenizer should have done it already)\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in tokenization: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Get part of speech for each token\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    for token, pos_tag in pos_tags:\n",
    "        # ! Uncommenting next line may crash the cell\n",
    "        # print(f\"Token: {token}, POS Tag: {pos_tag}\")\n",
    "        if pos_tag.startswith('V'):\n",
    "            # On garde\n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(token, pos='v'))\n",
    "            # Returns the input word unchanged if it cannot be found in WordNet.\n",
    "        elif pos_tag.startswith('N'):\n",
    "            # On garde\n",
    "            try:\n",
    "                lemmatized_tokens.append(lemmatizer.lemmatize(token, pos='n'))\n",
    "            except Exception as e:\n",
    "                print(f\"Error lemmatizing verb {token}: {e}\")\n",
    "        # Sinon on supprime\n",
    "\n",
    "    # Read forbidden words (stopwords, too frequent, too rare) from the file\n",
    "    with open('./../0_data/cleaned_data/forbidden_words.txt', 'r') as file:\n",
    "        forbidden = [line.strip() for line in file]\n",
    "\n",
    "    filtered_list = [token for token in lemmatized_tokens if token not in forbidden]\n",
    "\n",
    "    # keep uniques\n",
    "    seen_tokens = set()\n",
    "    unique_tokens = []\n",
    "\n",
    "    for token in filtered_list:\n",
    "        if token not in seen_tokens:\n",
    "            seen_tokens.add(token)\n",
    "            if len(token) > 2:\n",
    "                unique_tokens.append(token)\n",
    "\n",
    "    return unique_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlflow_experiment(\n",
    "    experiment_name: str, artifact_location: str, tags: dict[str, str]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a new mlflow experiment with the given name and artifact location.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    experiment_name: str\n",
    "        The name of the experiment to create.\n",
    "    artifact_location: str\n",
    "        The artifact location of the experiment to create.\n",
    "    tags: dict[str,Any]\n",
    "        The tags of the experiment to create.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    experiment_id: str\n",
    "        The id of the created experiment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        experiment_id = mlflow.create_experiment(\n",
    "            name=experiment_name, artifact_location=artifact_location, tags=tags\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Experiment {experiment_name} already exists.\")\n",
    "        experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "    mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "    return experiment_id\n",
    "\n",
    "\n",
    "def get_mlflow_experiment(\n",
    "    experiment_id: str = None, experiment_name: str = None\n",
    ") -> mlflow.entities.Experiment:\n",
    "    \"\"\"\n",
    "    Retrieve the mlflow experiment with the given id or name.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    experiment_id: str\n",
    "        The id of the experiment to retrieve.\n",
    "    experiment_name: str\n",
    "        The name of the experiment to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    experiment: mlflow.entities.Experiment\n",
    "        The mlflow experiment with the given id or name.\n",
    "    \"\"\"\n",
    "    if experiment_id is not None:\n",
    "        experiment = mlflow.get_experiment(experiment_id)\n",
    "    elif experiment_name is not None:\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    else:\n",
    "        raise ValueError(\"Either experiment_id or experiment_name must be provided.\")\n",
    "\n",
    "    return experiment\n",
    "\n",
    "\n",
    "def turn_str_back_into_list(df):\n",
    "    \"\"\"Correct the type change due to .csv export\"\"\"\n",
    "\n",
    "    df['title_nltk'] = df['title_nltk'].apply(ast.literal_eval)\n",
    "    df['body_nltk'] = df['body_nltk'].apply(ast.literal_eval)\n",
    "    df['title_spacy'] = df['title_spacy'].apply(ast.literal_eval)\n",
    "    df['body_spacy'] = df['body_spacy'].apply(ast.literal_eval)\n",
    "    df['all_tags'] = df['all_tags'].apply(ast.literal_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, this needs mlfow ui console command first -> unusable on remote server\n",
    "# all_experiments = client.search_experiments()\n",
    "# pprint(all_experiments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>all_tags</th>\n",
       "      <th>title_nltk</th>\n",
       "      <th>body_nltk</th>\n",
       "      <th>title_spacy</th>\n",
       "      <th>body_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42893</th>\n",
       "      <td>2017-02-23 11:34:31</td>\n",
       "      <td>Do we need clear MDC after HTTP request in Spring</td>\n",
       "      <td>According to this answer thread local variable...</td>\n",
       "      <td>[java, spring, logging, log4j, logback]</td>\n",
       "      <td>[need, mdc, request, spring]</td>\n",
       "      <td>[need, mdc, request, spring, accord, answer, t...</td>\n",
       "      <td>[need, request]</td>\n",
       "      <td>[accord, answer, thread, variable, use, clear,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42894</th>\n",
       "      <td>2011-10-13 20:57:32</td>\n",
       "      <td>How to make i18n with Handlebars.js (mustache ...</td>\n",
       "      <td>I'm currently using Handlebars.js (associated ...</td>\n",
       "      <td>[javascript, jquery, internationalization, han...</td>\n",
       "      <td>[make, i18n, handlebar, template]</td>\n",
       "      <td>[make, i18n, handlebar, template, associate, b...</td>\n",
       "      <td>[template]</td>\n",
       "      <td>[associate, web, app, client, render, issue, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42895</th>\n",
       "      <td>2012-09-06 00:16:46</td>\n",
       "      <td>How can I make R read my environmental variables?</td>\n",
       "      <td>I am running R on EC2 spot instances and I nee...</td>\n",
       "      <td>[linux, r, ubuntu, amazon-ec2, environment-var...</td>\n",
       "      <td>[make, read, variable]</td>\n",
       "      <td>[make, read, variable, run, spot, instance, ne...</td>\n",
       "      <td>[read, variable]</td>\n",
       "      <td>[run, spot, instance, need, terminate, cancel,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42896</th>\n",
       "      <td>2021-03-23 03:50:50</td>\n",
       "      <td>How to prevent react-query from fetching initi...</td>\n",
       "      <td>I'm using react-query v3.13 to fetch data from...</td>\n",
       "      <td>[javascript, reactjs, fetch, react-query, swr]</td>\n",
       "      <td>[prevent, query, fetch, enable]</td>\n",
       "      <td>[prevent, query, fetch, enable, data, want, po...</td>\n",
       "      <td>[prevent, react, query, fetch, enable]</td>\n",
       "      <td>[react, query, fetch, datum, want, api, point,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42897</th>\n",
       "      <td>2016-03-17 04:19:15</td>\n",
       "      <td>Inserting into table with an Identity column w...</td>\n",
       "      <td>I have a table A_tbl in my database. I have cr...</td>\n",
       "      <td>[sql, sql-server, database, ssms, database-rep...</td>\n",
       "      <td>[insert, table, identity, column, replication,...</td>\n",
       "      <td>[insert, table, identity, column, replication,...</td>\n",
       "      <td>[insert, table, column, replication, cause, er...</td>\n",
       "      <td>[table, database, create, trigger, capture, in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              CreationDate                                              title  \\\n",
       "42893  2017-02-23 11:34:31  Do we need clear MDC after HTTP request in Spring   \n",
       "42894  2011-10-13 20:57:32  How to make i18n with Handlebars.js (mustache ...   \n",
       "42895  2012-09-06 00:16:46  How can I make R read my environmental variables?   \n",
       "42896  2021-03-23 03:50:50  How to prevent react-query from fetching initi...   \n",
       "42897  2016-03-17 04:19:15  Inserting into table with an Identity column w...   \n",
       "\n",
       "                                                    body  \\\n",
       "42893  According to this answer thread local variable...   \n",
       "42894  I'm currently using Handlebars.js (associated ...   \n",
       "42895  I am running R on EC2 spot instances and I nee...   \n",
       "42896  I'm using react-query v3.13 to fetch data from...   \n",
       "42897  I have a table A_tbl in my database. I have cr...   \n",
       "\n",
       "                                                all_tags  \\\n",
       "42893            [java, spring, logging, log4j, logback]   \n",
       "42894  [javascript, jquery, internationalization, han...   \n",
       "42895  [linux, r, ubuntu, amazon-ec2, environment-var...   \n",
       "42896     [javascript, reactjs, fetch, react-query, swr]   \n",
       "42897  [sql, sql-server, database, ssms, database-rep...   \n",
       "\n",
       "                                              title_nltk  \\\n",
       "42893                       [need, mdc, request, spring]   \n",
       "42894                  [make, i18n, handlebar, template]   \n",
       "42895                             [make, read, variable]   \n",
       "42896                    [prevent, query, fetch, enable]   \n",
       "42897  [insert, table, identity, column, replication,...   \n",
       "\n",
       "                                               body_nltk  \\\n",
       "42893  [need, mdc, request, spring, accord, answer, t...   \n",
       "42894  [make, i18n, handlebar, template, associate, b...   \n",
       "42895  [make, read, variable, run, spot, instance, ne...   \n",
       "42896  [prevent, query, fetch, enable, data, want, po...   \n",
       "42897  [insert, table, identity, column, replication,...   \n",
       "\n",
       "                                             title_spacy  \\\n",
       "42893                                    [need, request]   \n",
       "42894                                         [template]   \n",
       "42895                                   [read, variable]   \n",
       "42896             [prevent, react, query, fetch, enable]   \n",
       "42897  [insert, table, column, replication, cause, er...   \n",
       "\n",
       "                                              body_spacy  \n",
       "42893  [accord, answer, thread, variable, use, clear,...  \n",
       "42894  [associate, web, app, client, render, issue, w...  \n",
       "42895  [run, spot, instance, need, terminate, cancel,...  \n",
       "42896  [react, query, fetch, datum, want, api, point,...  \n",
       "42897  [table, database, create, trigger, capture, in...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4767, 8)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./../0_data/cleaned_data/train_bow_uniques.csv', sep=',')\n",
    "test = pd.read_csv('./../0_data/cleaned_data/test_bow_uniques.csv', sep=',')\n",
    "\n",
    "turn_str_back_into_list(train)\n",
    "turn_str_back_into_list(test)\n",
    "\n",
    "display(train.tail())\n",
    "\n",
    "train.shape\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Often gives good results if enough data\n",
    "# fast\n",
    "# Accepts basically any input, as long as it is numerical\n",
    "\n",
    "# => Perfect for testing different embeddings !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dummy knn : il copie sur le + proche voisin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['javascript', 'internet-explorer', 'class', 'internet-explorer-8', 'classname']\n",
      "['javascript', 'internet-explorer', 'class', 'internet-explorer-8', 'classname'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Notre baseline\n",
    "\n",
    "def predict_tags_using_dummy_knn(df, feature, target, k=1, exemple=None):\n",
    "    documents = df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    dense_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Ici on ne va pas demander au knn de faire de prediction,\n",
    "    # On veut juste qu'il trouve les voisins.\n",
    "    # Mais la fonction fit a besoin de targets en param\n",
    "    target_values = df[target].values\n",
    "\n",
    "    # Initialize kNN model\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=k, metric='cosine', algorithm='brute')\n",
    "    # print(knn_model.n_neighbors)\n",
    "\n",
    "    knn_model.fit(dense_matrix, target_values)\n",
    "\n",
    "    # Example query\n",
    "    query_document = exemple\n",
    "    query_bow = gensim_dictionary.doc2bow(query_document)\n",
    "    query_vector = corpus2dense([query_bow], num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Find nearest neighbors\n",
    "    _, indices = knn_model.kneighbors(query_vector)\n",
    "\n",
    "    # Aggregate tags from neighbors\n",
    "    neighbor_tags = [tag for i in indices.flatten() for tag in df.iloc[i][target]]\n",
    "\n",
    "    print(neighbor_tags)\n",
    "\n",
    "    # Predict tags based on most common tags among neighbors\n",
    "    predicted_tags = [tag for tag, _ in Counter(neighbor_tags).most_common(n=10)]\n",
    "    # 5 tags/question en moyenne mais on peut suggérer +\n",
    "    # ici a ameliorer\n",
    "\n",
    "    return predicted_tags, knn_model\n",
    "\n",
    "\n",
    "exemple = [\"your\", 'text', 'document', 'javascript']\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "predicted_tags, knn_test = predict_tags_using_dummy_knn(train, 'title_nltk', 'all_tags', exemple=exemple)\n",
    "print(predicted_tags, '\\n')\n",
    "\n",
    "# javascript ok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 knn basic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add grid search cv\n",
    "# add score\n",
    "\n",
    "def predict_tags_using_knn(df, feature, target, k=50, exemple=None):\n",
    "    documents = df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    dense_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Ici on ne va pas demander au knn de faire de prediction,\n",
    "    # On veut juste qu'il trouve les voisins.\n",
    "    # Mais la fonction fit a besoin de targets en param\n",
    "    target_values = df[target].values\n",
    "\n",
    "    # Initialize kNN model\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=k, metric='cosine', algorithm='brute')\n",
    "    knn_model.fit(dense_matrix, target_values)\n",
    "\n",
    "    # Example query\n",
    "    query_document = exemple\n",
    "    query_bow = gensim_dictionary.doc2bow(query_document)\n",
    "    query_vector = corpus2dense([query_bow], num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Find nearest neighbors\n",
    "    _, indices = knn_model.kneighbors(query_vector)\n",
    "\n",
    "    # Aggregate tags from neighbors\n",
    "    neighbor_tags = [tag for i in indices.flatten() for tag in df.iloc[i][target]]\n",
    "\n",
    "    print(neighbor_tags)\n",
    "\n",
    "    # Predict tags based on most common tags among neighbors\n",
    "    predicted_tags = [tag for tag, _ in Counter(neighbor_tags).most_common(n=10)]\n",
    "    # 5 tags/question en moyenne mais on peut suggérer +\n",
    "    # ici a ameliorer\n",
    "\n",
    "    return query_vector, predicted_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 predictions (input = list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c#', '.net', 'ms-word', 'openxml', 'openxml-sdk', 'javascript', 'internet-explorer', 'class', 'internet-explorer-8', 'classname', 'java', 'javascript', 'ajax', 'selenium', 'htmlunit-driver', 'c#', '.net', 'windows', 'f#', 'console', 'javascript', 'jquery', 'jquery-plugins', 'text-to-speech', 'html5-audio', 'javascript', 'html', 'css', 'text', 'truncate', 'javascript', 'jquery', 'css', 'dom', 'document', 'javascript', 'html', 'string', 'text', 'extract', 'javascript', 'dom', 'substring', 'indexof', 'getselection', 'javascript', 'html', 'function', 'text', 'onclick', 'c#', '.net', 'html', 'pdf', 'extract', 'javascript', 'jquery', 'css', 'copy', 'cut', 'javascript', 'html', 'url', 'base64', 'data-uri', 'python', 'module', 'preprocessor', 'nlp', 'stemming', 'javascript', 'php', 'jquery', 'curl', 'http-headers', 'python', 'text', 'replace', 'ms-word', 'python-docx', 'c#', 'javascript', 'html', 'http', 'dom', 'javascript', 'jquery', 'ruby-on-rails', 'tdd', 'jasmine', 'ios', 'swift', 'height', 'uilabel', 'frame', 'javascript', 'html', 'performance', 'three.js', 'webgl', 'ios', 'objective-c', 'uilabel', 'autolayout', 'uistoryboard', 'linux', 'assembly', 'x86-64', 'calling-convention', 'abi', 'javascript', 'jquery', 'html', 'css', 'bootstrap-4', 'localization', 'internationalization', 'translation', 'stripe-payments', 'stripe.net', 'html', 'spring', 'reactjs', 'spring-boot', 'thymeleaf', 'javascript', 'html', 'function', 'anchor', 'href', 'javascript', 'jquery', 'html', 'dom', 'document-ready', 'javascript', 'jquery', 'textarea', 'hyperlink', 'addition', 'html', 'sublimetext2', 'sublimetext', 'indentation', 'reformat', 'javascript', 'arrays', 'object', 'arguments', 'slice', 'javascript', 'jquery', 'ajax', 'asp.net-mvc-3', 'url', 'javascript', 'iphone', 'css', 'ios', 'uiwebview', 'javascript', 'unicode', 'diacritics', 'combining-marks', 'zalgo', 'javascript', 'firebase', 'google-cloud-platform', 'google-cloud-firestore', 'typeerror', 'javascript', 'function', 'oop', 'if-statement', 'conditional-statements', 'javascript', 'html', 'full-text-search', 'local-storage', 'client-side', 'javascript', 'node.js', 'backbone.js', 'ember.js', 'javascript-framework', 'c#', 'javascript', 'jquery', 'asp.net-mvc', 'razor', 'javascript', 'oop', 'functional-programming', 'polymorphism', 'parametric-polymorphism', 'html', 'meta-tags', 'semantics', 'semantic-web', 'semantic-markup', 'mongodb', 'mongodb-query', 'aggregation-framework', 'spring-data-mongodb', 'full-text-indexing', 'python', 'django', 'orm', 'mongodb', 'mongoengine', 'javascript', 'c#', 'asp.net-mvc', 'dictionary', 'asp.net-web-api', 'javascript', 'css', 'encapsulation', 'styling', 'web-component', 'ios', 'ocr', 'xcode4.5', 'tesseract', 'leptonica', 'javascript', 'reactjs', 'functional-programming', 'immutability', 'immutable.js', 'html', 'css', 'text', 'autocomplete', 'sublimetext', 'javascript', 'object', 'recursion', 'comparison', 'equality', 'javascript', 'arrays', 'algorithm', 'big-o', 'time-complexity', 'objective-c', 'ios', 'xcode', 'event-handling', 'uibutton']\n",
      "['javascript', 'html', 'jquery', 'css', 'c#', 'text', 'ios', 'dom', '.net', 'function'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemple1 = [\"your\", 'text', 'document', 'javascript']\n",
    "_, predicted_tags1 = predict_tags_using_knn(train, 'title_nltk', 'all_tags', exemple=exemple1)\n",
    "print(predicted_tags1, '\\n')\n",
    "\n",
    "# javascript ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'module', 'preprocessor', 'nlp', 'stemming', 'c#', '.net', 'ms-word', 'openxml', 'openxml-sdk', 'python', 'parsing', 'text', 'file-io', 'python-2.7', 'python-2.7', 'ubuntu', 'python-3.x', 'spatial-index', 'r-tree', 'python', 'plot', 'tree', 'data-visualization', 'visualization', 'c#', '.net', 'windows', 'f#', 'console', 'python', 'html', 'web-scraping', 'text', 'beautifulsoup', 'python', 'python-3.x', 'algorithm', 'sorting', 'mergesort', 'python', 'python-2.7', 'reflection', 'delegation', 'message-passing', 'python', 'python-3.x', 'annotations', 'lint', 'type-hinting', 'python', 'pdf', 'python-3.7', 'pypdf', 'pdf-extraction', 'python', 'selenium', 'selenium-webdriver', 'xpath', 'webdriverwait', 'python', 'macos', 'python-3.x', 'sublimetext2', 'sublimetext', 'python-3.x', 'pdf', 'text', 'extract', 'pdfminer', 'python', 'documentation', 'python-3.7', 'docstring', 'python-dataclasses', 'python', 'text', 'stemming', 'plural', 'singular', 'nlp', 'cluster-analysis', 'data-mining', 'k-means', 'text-mining', 'python', 'image', 'opencv', 'image-processing', 'computer-vision', 'python', 'regex', 'performance', 'perl', 'text-processing', 'python', 'windows', 'user-interface', 'text', 'screen', 'python', 'shell', 'encoding', 'utf-8', 'python-2.x', 'python', 'html', 'excel', 'pandas', 'dataframe', 'python', 'python-3.x', 'python-2.7', 'text-extraction', 'pdfminer', 'python', 'python-2.7', 'tkinter', 'callback', 'tkinter.text', 'python', 'utf-8', 'python-unicode', 'windows-1252', 'cp1252', 'python', 'image', 'text', 'python-imaging-library', 'bold', 'python', 'parsing', 'data-structures', 'dictionary', 'nested', 'python', 'text', 'replace', 'docx', 'zip', 'python', 'selenium', 'xpath', 'selenium-webdriver', 'webdriver', 'python', 'plugins', 'sublimetext2', 'distutils', 'python-requests', 'python', 'text', 'replace', 'ms-word', 'python-docx', 'python', 'utf-8', 'character-encoding', 'locale', 'default', 'python', 'django', 'autocomplete', 'sublimetext2', 'sublimetext', 'c#', '.net', 'html', 'pdf', 'extract', 'python', 'python-3.x', 'file', 'text', 'count', 'python', 'file', 'encryption', 'aes', 'pycrypto', 'python', 'list', 'file', 'ascii', 'newline', 'python', 'performance', 'search', 'profiling', 'large-files', 'python', 'sql-server', 'sql-server-2008', 'python-2.7', 'pymssql', 'html', 'sublimetext2', 'sublimetext', 'indentation', 'reformat', 'mongodb', 'mongodb-query', 'aggregation-framework', 'spring-data-mongodb', 'full-text-indexing', 'python', 'excel', 'com', 'pywin32', 'win32com', 'python', 'loops', 'if-statement', 'break', 'conditional-operator', 'java', 'python', 'jython', 'pip', 'easy-install', 'python', 'unit-testing', 'testing', 'mocking', 'patch', 'python', 'loops', 'for-loop', 'infinite-loop', 'infinite', 'python-2.7', 'exception', 'python-3.x', 'traceback', 'raise', 'python', 'debugging', 'multiprocessing', 'pycharm', 'winpdb', 'python', 'packaging', 'remote-access', 'download', 'software-update', 'python', 'constructor', 'destructor', 'with-statement', 'contextmanager']\n",
      "['python', 'text', 'python-3.x', 'python-2.7', 'html', 'sublimetext2', 'c#', '.net', 'pdf', 'sublimetext'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemple2 = [\"your\", 'text', 'document', 'python']\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "_, predicted_tags2 = predict_tags_using_knn(train, 'title_nltk', 'all_tags', exemple=exemple2)\n",
    "print(predicted_tags2, '\\n')\n",
    "\n",
    "# python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['find', 'class', 'com', 'google', 'firebase', 'provider']\n",
      "['java', 'spring', 'rest', 'gradle', 'spring-boot', 'java', 'android', 'firebase', 'gradle', 'android-gradle-plugin', 'android', 'android-studio', 'firebase', 'android-gradle-plugin', 'google-play-services', 'android', 'android-intent', 'arraylist', 'unmarshalling', 'parcelable', 'php', 'class', 'laravel', 'alias', 'autoloader', 'php', 'sql', 'laravel', 'laravel-5', 'laravel-artisan', 'java', 'spring', 'spring-boot', 'spring-security', 'spring-security-oauth2', 'java', 'maven', 'maven-2', 'maven-3', 'protocol-buffers', 'android', 'google-maps', 'dictionary', 'android-mapview', 'inflate', 'android', 'android-studio', 'flutter', 'sdk', 'android-sdk-manager', 'java', 'spring', 'junit', 'spring-boot', 'spring-data', 'php', 'laravel', 'https', 'laravel-valet', 'valet', 'spring', 'maven', 'spring-mvc', 'spring-boot', 'spring-profiles', 'json', 'angular', 'typescript', 'jwt', 'guard', 'c#', '.net', 'visual-studio-2012', 'compression', 'zip', 'c#', 'asp.net', 'google-api', 'google-oauth', 'google-api-dotnet-client', 'ios', 'ipad', 'frameworks', 'header', 'gpuimage', 'android', 'firebase', 'gradle', 'android-gradle-plugin', 'crashlytics', 'java', 'jar', 'maven-2', 'manifest', 'program-entry-point', 'gcc', 'ubuntu', 'linker', 'shared-libraries', 'ld', 'java', 'interface', 'go', 'static-methods', 'abstract', 'javascript', 'node.js', 'typescript', 'nestjs', 'class-validator', 'c#', 'android', 'firebase', 'unity-game-engine', 'firebase-authentication', 'c#', '.net', 'visual-studio', 'json.net', 'nuget', 'android', 'firebase', 'android-studio', 'android-gradle-plugin', 'jcenter', 'iphone', 'objective-c', 'ios', 'xcode4.3', 'cocoapods', 'javascript', 'node.js', 'mongodb', 'mongoose', 'mongodb-query', 'database', 'security', 'firebase', 'firebase-authentication', 'firebase-security', 'php', 'laravel', 'amazon-web-services', 'laravel-5', 'laravelcollective', 'c#', '.net', 'web-config', 'class-library', 'configurationmanager', 'android', 'unity-game-engine', 'abi', 'arcore', 'unsatisfiedlinkerror', 'javascript', 'android', 'reactjs', 'react-native', 'gradle', 'python', 'tensorflow', 'keras', 'transfer-learning', 'vgg-net', 'angularjs', 'routes', 'angular-ui-router', 'single-page-application', 'angularjs-routing', 'java', 'linux', 'ubuntu', 'netbeans', 'command-line', 'firebase', 'flutter', 'dart', 'firebase-authentication', 'google-authentication', 'java', 'android', 'android-studio', 'gradle', 'build', 'c++', 'ubuntu', 'sdl', 'sdl-2', 'sdl-image', 'google-maps', 'angular', 'typescript', 'angular-cli', 'angular2-google-maps', 'python', 'class', 'inheritance', 'python-3.x', 'language-design', 'java', 'android', 'design-patterns', 'inheritance', 'parcelable', 'javascript', 'node.js', 'angular', 'typescript', 'angular7', 'c++', 'gcc', 'vtable', 'virtual-inheritance', 'vtt', 'javascript', 'firebase', 'google-drive-api', 'firebase-authentication', 'adobe-indesign', 'ios', 'objective-c', 'xcode', 'storyboard', 'xib', 'java', 'maven', 'build', 'interop', 'kotlin', 'c++', 'c++11', 'bit-fields', 'bitmask', 'enum-class', 'java', 'json', 'rest', 'jersey', 'jax-rs', 'android', 'firebase', 'flutter', 'google-play-services', 'flutter-dependencies', 'android', 'android-intent', 'classnotfoundexception', 'unmarshalling', 'parcel']\n",
      "['android', 'java', 'firebase', 'gradle', 'c#', 'javascript', 'spring', 'spring-boot', 'android-gradle-plugin', 'android-studio'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemple = test['title_nltk'][0]\n",
    "print(exemple)\n",
    "\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "_, predicted_tags = predict_tags_using_knn(train, 'title_nltk', 'all_tags', exemple=exemple)\n",
    "print(predicted_tags, '\\n')\n",
    "\n",
    "# firebase peut etre predit\n",
    "# grand succes !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Predictions (input = text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find class \"com.google.firebase.provider.FirebaseInitProvider\"\n",
      "['java', 'spring', 'rest', 'gradle', 'spring-boot', 'java', 'android', 'firebase', 'gradle', 'android-gradle-plugin', 'android', 'android-studio', 'firebase', 'android-gradle-plugin', 'google-play-services', 'android', 'android-intent', 'arraylist', 'unmarshalling', 'parcelable', 'php', 'class', 'laravel', 'alias', 'autoloader', 'php', 'sql', 'laravel', 'laravel-5', 'laravel-artisan', 'java', 'spring', 'spring-boot', 'spring-security', 'spring-security-oauth2', 'java', 'maven', 'maven-2', 'maven-3', 'protocol-buffers', 'android', 'google-maps', 'dictionary', 'android-mapview', 'inflate', 'android', 'android-studio', 'flutter', 'sdk', 'android-sdk-manager', 'java', 'spring', 'junit', 'spring-boot', 'spring-data', 'php', 'laravel', 'https', 'laravel-valet', 'valet', 'spring', 'maven', 'spring-mvc', 'spring-boot', 'spring-profiles', 'json', 'angular', 'typescript', 'jwt', 'guard', 'c#', '.net', 'visual-studio-2012', 'compression', 'zip', 'c#', 'asp.net', 'google-api', 'google-oauth', 'google-api-dotnet-client', 'ios', 'ipad', 'frameworks', 'header', 'gpuimage', 'android', 'firebase', 'gradle', 'android-gradle-plugin', 'crashlytics', 'java', 'jar', 'maven-2', 'manifest', 'program-entry-point', 'gcc', 'ubuntu', 'linker', 'shared-libraries', 'ld', 'java', 'interface', 'go', 'static-methods', 'abstract', 'javascript', 'node.js', 'typescript', 'nestjs', 'class-validator', 'c#', 'android', 'firebase', 'unity-game-engine', 'firebase-authentication', 'c#', '.net', 'visual-studio', 'json.net', 'nuget', 'android', 'firebase', 'android-studio', 'android-gradle-plugin', 'jcenter', 'iphone', 'objective-c', 'ios', 'xcode4.3', 'cocoapods', 'javascript', 'node.js', 'mongodb', 'mongoose', 'mongodb-query', 'database', 'security', 'firebase', 'firebase-authentication', 'firebase-security', 'php', 'laravel', 'amazon-web-services', 'laravel-5', 'laravelcollective', 'c#', '.net', 'web-config', 'class-library', 'configurationmanager', 'android', 'unity-game-engine', 'abi', 'arcore', 'unsatisfiedlinkerror', 'javascript', 'android', 'reactjs', 'react-native', 'gradle', 'python', 'tensorflow', 'keras', 'transfer-learning', 'vgg-net', 'angularjs', 'routes', 'angular-ui-router', 'single-page-application', 'angularjs-routing', 'java', 'linux', 'ubuntu', 'netbeans', 'command-line', 'firebase', 'flutter', 'dart', 'firebase-authentication', 'google-authentication', 'java', 'android', 'android-studio', 'gradle', 'build', 'c++', 'ubuntu', 'sdl', 'sdl-2', 'sdl-image', 'google-maps', 'angular', 'typescript', 'angular-cli', 'angular2-google-maps', 'python', 'class', 'inheritance', 'python-3.x', 'language-design', 'java', 'android', 'design-patterns', 'inheritance', 'parcelable', 'javascript', 'node.js', 'angular', 'typescript', 'angular7', 'c++', 'gcc', 'vtable', 'virtual-inheritance', 'vtt', 'javascript', 'firebase', 'google-drive-api', 'firebase-authentication', 'adobe-indesign', 'ios', 'objective-c', 'xcode', 'storyboard', 'xib', 'java', 'maven', 'build', 'interop', 'kotlin', 'c++', 'c++11', 'bit-fields', 'bitmask', 'enum-class', 'java', 'json', 'rest', 'jersey', 'jax-rs', 'android', 'firebase', 'flutter', 'google-play-services', 'flutter-dependencies', 'android', 'android-intent', 'classnotfoundexception', 'unmarshalling', 'parcel']\n",
      "['android', 'java', 'firebase', 'gradle', 'c#', 'javascript', 'spring', 'spring-boot', 'android-gradle-plugin', 'android-studio'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemple = test['title'][0]\n",
    "print(exemple)\n",
    "\n",
    "exemple_text = preprocess_text(exemple)\n",
    "\n",
    "_, predicted_tags = predict_tags_using_knn(train, 'title_nltk', 'all_tags', exemple=exemple_text)\n",
    "print(predicted_tags, '\\n')\n",
    "\n",
    "# ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemple = test['title'][1]\n",
    "print(exemple)\n",
    "\n",
    "exemple_text = preprocess_text(exemple)\n",
    "\n",
    "_, predicted_tags = predict_tags_using_knn(train, 'title_nltk', 'all_tags', exemple=exemple)\n",
    "print(predicted_tags, '\\n')\n",
    "\n",
    "# scala ok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notre objectif de prédiction de tags ressemble a un pb de classification multi-label,\n",
    "# où la matrice de confusion est extrêmement déséquilibrée :\n",
    "# 5 tags sont prédits positifs, contre environ 250 000 tags (si on travaille sur all_tags)\n",
    "# predits negatifs. Autrement dit :\n",
    "\n",
    "# On peut utiliser la precision pour évaluer notre modèle. C'est même exactement l'outil qu'il nous faut :\n",
    "# \"précision = la proportion de prédictions correctes parmi les points que l’on a prédits positifs.\"\n",
    "# En + c de loin le plus léger en ressources, puisqu'il ne s'occupe que des 5 tags prédits.\n",
    "\n",
    "# En revanche je pense que le recall n'a pas vraiment de sens ici, il sera \"écrasé\" par\n",
    "# le nombre de tags predits negatifs, sa valeur sera tjs très proche de zero.\n",
    "# (même remarque pour la spécificité et l'accuracy)\n",
    "# Et sans recall, pas de f1 score.\n",
    "# à vérifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n"
     ]
    }
   ],
   "source": [
    "# pourquoi on ne peut pas utiliser le score precision sckikit ici :\n",
    "from sklearn.metrics import precision_score as p_score\n",
    "\n",
    "# Assuming y_true is the ground truth (real tags) and y_pred is the predicted tags\n",
    "precision = p_score(['ok', 'ko'], ['ko', 'ok'], average='micro')  # You can use 'micro', 'macro', or 'weighted' depending on your use case\n",
    "print(f'Precision: {precision}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_130447/636444049.py:5: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if predicted_tag in real_tags:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision_topics(real_tags:list, predicted_tags:list): # pour comparer 2 listes\n",
    "    # precision = TP / (TP + FP)\n",
    "    tp = 0\n",
    "    for predicted_tag in predicted_tags:\n",
    "        if predicted_tag in real_tags:\n",
    "            tp += 1\n",
    "\n",
    "    fp = len(predicted_tags) - tp\n",
    "    precision = tp/(tp + fp)\n",
    "    # <=> precision = tp/len(predicted_tags)\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "print(precision_topics(exemple1, predicted_tags1))\n",
    "precision_topics(exemple2, predicted_tags2)\n",
    "\n",
    "# ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score(y_true, y_pred): # pour comparer 2 df ou 2 matrices de mm shape[0]\n",
    "    precision = 0\n",
    "    for i in range(0, len(y_pred)):\n",
    "        precision += precision_topics(y_true[i], y_pred[i])\n",
    "    precision_moyenne = precision / len(y_pred)\n",
    "\n",
    "    return precision_moyenne\n",
    "\n",
    "\n",
    "# pour la gridsearchcv\n",
    "custom_precision_scorer = make_scorer(precision_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La precision nous interesse davantage ici, pour au moins 2 raisons :\n",
    "# 1) On n'a aucune raison de \"pénaliser\" le modele pour les faux negatifs\n",
    "# 2) Ici le recall va etre tres proche de zero, et sa variance sera tres faible\n",
    "# => bcp - parlant\n",
    "\n",
    "def recall_topics(all_tags: list, predicted_tags: list):\n",
    "    # recall = TP / (TP + FN)\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    for real_tag in all_tags:\n",
    "        if real_tag in predicted_tags:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "# insister ds la doctype : y_all = all tags ici, != y_true (5-6 tags max)\n",
    "def recall_score(y_all, y_pred): # pour comparer 2 df ou 2 matrices de mm shape[0]\n",
    "    recall = 0\n",
    "    for i in range(0, len(y_pred)):\n",
    "        recall += recall_topics(y_all, y_pred[i]) # ca risque d'etre long a calculer\n",
    "    recall_moyen = recall / len(y_pred)\n",
    "\n",
    "    return recall_moyen\n",
    "\n",
    "\n",
    "custom_recall_scorer = make_scorer(recall_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f1 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm rearques que pour le recall\n",
    "\n",
    "def f1_topics(real_tags: list, predicted_tags: list, all_tags:list):\n",
    "    precision = precision_score(real_tags, predicted_tags)\n",
    "    recall = recall_topics(all_tags, predicted_tags)\n",
    "\n",
    "    # F1 score = 2 * (precision * recall) / (precision + recall)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred, y_all):\n",
    "    f1_score = 0\n",
    "    for i in range(0, len(y_pred)):\n",
    "        score += f1_topics(y_true[i], y_pred[i], y_all)\n",
    "    score_moyen = score / len(y_pred)\n",
    "\n",
    "    return score_moyen\n",
    "\n",
    "\n",
    "custom_f1_scorer = make_scorer(f1_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idem, ici la variance sera presque nulle (TP entre 0 et 5, TN = environ 250 000...)\n",
    "\n",
    "def accuracy_topics(real_tags: list, predicted_tags: list, all_tags:list):\n",
    "    # accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    tp = sum(1 for tag in predicted_tags if tag in real_tags)\n",
    "    tn = sum(1 for tag in all_tags if (tag not in predicted_tags) and (tag not in real_tags) )\n",
    "    fp = sum(1 for tag in predicted_tags if tag not in real_tags)\n",
    "    fn = sum(1 for tag in all_tags if (tag not in predicted_tags) and (tag in real_tags))\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def accuracy_score(y_true, y_pred, y_all):\n",
    "    score = 0\n",
    "    for i in range(0, len(y_pred)):\n",
    "        score += accuracy_topics(y_true[i], y_pred[i], y_all)\n",
    "    score_moyen = score / len(y_pred)\n",
    "\n",
    "    return score_moyen\n",
    "\n",
    "\n",
    "custom_accuracy_scorer = make_scorer(accuracy_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilisé en partie 3 pour évaluer la similarité des topics obtenus / la lda\n",
    "# ici pour comparer topics reels et topics predits.\n",
    "# devrait etre correlé a la precision non ?\n",
    "# st ts les 2 liés au nb de tp (true positifs)\n",
    "\n",
    "def jaccard_similarity(topic1, topic2):\n",
    "    set1 = set(topic1)\n",
    "    set2 = set(topic2)\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "\n",
    "def jaccard_score(y_true, y_pred): # pour comparer 2 df ou 2 matrices de mm shape[0]\n",
    "    jacc = 0\n",
    "    for i in range(0, len(y_pred)):\n",
    "        jacc += jaccard_similarity(y_true[i], y_pred[i])\n",
    "    jacc_moyen = jacc / len(y_pred)\n",
    "\n",
    "    return jacc_moyen\n",
    "\n",
    "\n",
    "# pour la gridsearchcv\n",
    "custom_jacc_scorer = make_scorer(jaccard_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8513\n",
      "42898\n",
      "(42898, 8513) \n",
      "\n",
      "array([[1., 1., 1., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)\n",
      "\n",
      "\n",
      "doc 2500 : ['let', 'support', 'element', 'try', 'get', 'application', 'run', 'stop', 'template', 'error', 'thead', 'tbody', 'row', 'index', 'datepicker', 'sec', 'html', 'syntaxerror', 'compiler', 'parse', 'set', 'foreach', 'figure', 'need', 'follow', 'topic', 'issue', 'state', 'bootstrap', 'beta', 'doesnt', 'work', 'http', 'github', 'com', 'software', 'help', 'appreciate']\n",
      "real tags : ['html', 'angular', 'npm', 'frontend', 'ngx-bootstrap']\n",
      "predicted : ['c#', '.net', 'null', 'int', 'nullable']\n",
      "0.0 \n",
      "\n",
      "doc 2501 : ['java', 'string', 'modify', 'search', 'representation', 'get', 'material', 'look', 'com', 'cpp', 'misc', 'multi', 'article', 'php', 'say', 'text', 'support', 'modification', 'serialization', 'wikipedia', 'org', 'wiki', 'data', 'byte', 'char', 'memory', 'please', 'let', 'know', 'one']\n",
      "real tags : ['java', 'string', 'encoding', 'utf-8', 'utf-16']\n",
      "predicted : ['c#', '.net', 'string', 'size', 'byte']\n",
      "0.2 \n",
      "\n",
      "doc 2502 : ['test', 'website', 'end', 'develop', 'php', 'service', 'production', 'performance', 'load', 'side', 'client', 'want', 'know', 'response', 'time', 'object', 'etc', 'apache', 'web', 'server', 'request', 'handle', 'start', 'source', 'tool', 'platform', 'purpose', 'user', 'example', 'check', 'metric']\n",
      "real tags : ['php', 'apache', 'performance', 'performance-testing', 'load-testing']\n",
      "predicted : ['php', 'laravel', 'error-handling', 'exception', 'laravel-5']\n",
      "0.2 \n",
      "\n",
      "doc 2503 : ['way', 'convert', 'recommend', 'pdf', 'attach', 'email', 'prefer', 'store', 'memory', 'byte', 'disk', 'thanks', 'help']\n",
      "real tags : ['c#', '.net', 'wpf', 'pdf', 'flowdocument']\n",
      "predicted : ['double', 'c++', 'type-conversion', 'c#', 'math']\n",
      "0.2 \n",
      "\n",
      "doc 2504 : ['webstorm', 'resolve', 'directory', 'start', 'bug', 'turn', 'fix', 'feature', 'parser', 'attempt', 'string', 'suppose', 'reference', 'file', 'project', 'example', 'end', 'code', 'app', 'server', 'serve', 'root', 'web', 'context', 'side', 'behavior', 'impact', 'template', 'route', 'imagine', 'way', 'change', 'treat', 'disable', 'try', 'inspector', 'find', 'option', 'help', 'appreciate', 'thanks']\n",
      "real tags : ['javascript', 'html', 'angularjs', 'ide', 'webstorm']\n",
      "predicted : ['find', 'replace', 'ide', 'sublimetext2', 'global']\n",
      "0.2 \n",
      "\n",
      "doc 2505 : ['swift', 'url', 'default', 'browser', 'open', 'system', 'language', 'osx', 'platform', 'find', 'lot', 'uiapplication', 'sharedapplication', 'string', 'work', 'launch', 'service', 'example', 'deprecate']\n",
      "real tags : ['xcode', 'macos', 'swift', 'nsurl', 'openurl']\n",
      "predicted : ['ios', 'swift', 'xcode', 'cocoa-touch', 'ios9']\n",
      "0.4 \n",
      "\n",
      "doc 2506 : ['style', 'javafx', 'menu', 'item', 'get', 'menubar', 'setup', 'follow', 'vbox', 'text', 'menuitem', 'project', 'quit', 'produce', 'file', 'bar', 'show', 'label', 'activate', 'display', 'try', 'treat', 'contextmenu', 'context', 'color', 'red', 'anything', 'surprise', 'button', 'change', 'select', 'call', 'seem', 'exist', 'question', 'container', 'hold', 'clarification', 'help', 'clarify', 'element', 'look', 'add', 'image', 'outline', 'component', 'wish']\n",
      "real tags : ['css', 'javafx', 'menu', 'javafx-2', 'javafx-8']\n",
      "predicted : ['ios', 'objective-c', 'uitabbarcontroller', 'uitabbar', 'uitabbaritem']\n",
      "0.0 \n",
      "\n",
      "doc 2507 : ['download', 'image', 'azure', 'machine', 'upload', 'account', 'credential', 'know', 'wget', 'help', 'occur', 'error', 'code', 'message', 'specify', 'resource', 'exist', 'requestid', 'time', 'idea', 'try', 'migrate', 'get', 'trouble', 'work']\n",
      "real tags : ['powershell', 'azure', 'cloud', 'virtual-machine', 'azure-storage']\n",
      "predicted : ['c#', 'date', 'ios', 'datetime', 'utc']\n",
      "0.0 \n",
      "\n",
      "doc 2508 : ['protocol', 'buffer', 'import', 'recognize', 'intellij', 'attempt', 'message', 'try', 'protobuf', 'code', 'generate', 'java', 'compiles', 'run', 'expect', 'idea', 'edition', 'editor', 'plugin', 'april', 'gradle', 'build', 'file', 'look', 'plugins', 'com', 'google', 'version', 'group', 'tech', 'sourcecompatibility', 'def', 'repositories', 'test', 'dependency', 'implementation', 'grpc', 'org', 'apache', 'tomcat', 'annotation', 'api', 'advance', 'need', 'jsonformat', 'shade', 'testimplementation', 'compile', 'name', 'testcompile', 'junit', 'jupiter', 'engine', 'mockito', 'core', 'artifact', 'gen', 'ides', 'eclipse', 'netbeans', 'sourcesets', 'srcdirs', 'source', 'proto', 'task', 'type', 'service', 'server', 'project', 'builddir', 'tmp', 'client', 'time', 'revision', 'kotlin', 'dsl', 'groovy', 'july', 'jvm', 'oracle', 'corporation', 'mac', 'x86_64', 'example', 'fail', 'package', 'option', 'describe', 'column', 'metadata', 'string', 'int32', 'size', 'enum', 'integer', 'float', 'text', 'skip', 'sit', 'system', 'src']\n",
      "real tags : ['java', 'gradle', 'intellij-idea', 'protocol-buffers', 'grpc']\n",
      "predicted : ['java', 'intellij-idea', 'junit', 'cucumber', 'cucumber-junit']\n",
      "0.4 \n",
      "\n",
      "doc 2509 : ['execute', 'foreach', 'window', 'tool', 'probe', 'state', 'debug', 'process', 'learn', 'question', 'mark', 'bit', 'show', 'post', 'know', 'query', 'include', 'expression', 'fail', 'statement', 'thing', 'get', 'error', 'contain', 'term', 'gallery', 'say', 'work', 'programmer', 'leave', 'time', 'look', 'something']\n",
      "real tags : ['c#', 'linq', 'visual-studio', 'lambda', 'immediate-window']\n",
      "predicted : ['android', 'android-studio', 'c#', 'datetime', 'date']\n",
      "0.2 \n",
      "\n",
      "doc 2510 : ['distinct', 'function', 'postgres', 'problem', 'schema', 'photo', 'tag', 'comment', 'query', 'want', 'multiply', 'row', 'get', 'select', 'name', 'leave', 'join', 'thing', 'mean', 'group', 'copy', 'array', 'city', 'make', 'json', 'like', 'sql']\n",
      "real tags : ['sql', 'json', 'postgresql', 'distinct', 'aggregate-functions']\n",
      "predicted : ['sql', 'oracle', 'plsql', 'oracle11g', 'oracle-sqldeveloper']\n",
      "0.2 \n",
      "\n",
      "precision moyenne = 0.10606060606060606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10606060606060606"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add grid search cv\n",
    "# add recall, f1 score ?\n",
    "\n",
    "def predict_tags_using_knn(train_df=train, feature='body_nltk', target='all_tags', test_df=test, k=5, n=5,\n",
    "                           scorer=precision_score):\n",
    "    # 1 PREPROCESSING\n",
    "    documents = train_df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    print(len(gensim_dictionary))\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "    print(len(corpus))\n",
    "    # taille corpus ?\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    dense_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "    # taille matrice ? afficher\n",
    "    print(dense_matrix.shape, '\\n')\n",
    "    # pas tres dense ici, c notre bow donc tres sparse en fait\n",
    "    # curieux d'appeler \"corpus2dense()\" une fonction qui retourne une matrice sparse\n",
    "    pprint(dense_matrix[:10]) # vraiment tres dense, quasiment que des 0 ! Bref\n",
    "    print('\\n')\n",
    "\n",
    "    # Ici on ne va pas demander au knn de faire de prediction,\n",
    "    # On veut juste qu'il trouve les voisins.\n",
    "    # Mais la fonction fit a besoin de targets en param\n",
    "    target_values = train_df[target].values\n",
    "\n",
    "    # 2 MODEL TRAINING\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn_model.fit(dense_matrix, target_values)\n",
    "\n",
    "    # 3 PREDICTION\n",
    "    # Predictions completes en 1h ou 2\n",
    "    # optimiser avec pandarallel ?\n",
    "    # use a sample en attendant\n",
    "    predictions=[]\n",
    "    min_range=2500\n",
    "    max_range=2511 # test.shape[0]=4767\n",
    "    for i in range(min_range, max_range):\n",
    "        query_document = test_df[feature][i]\n",
    "        print(f'doc {i} : {query_document}')\n",
    "        print(f'real tags : {test[target][i]}')\n",
    "        query_bow = gensim_dictionary.doc2bow(query_document)\n",
    "        query_vector = corpus2dense([query_bow], num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "        # Find nearest neighbors\n",
    "        _, indices = knn_model.kneighbors(query_vector)\n",
    "\n",
    "        # Aggregate tags from neighbors\n",
    "        neighbor_tags = [tag for i in indices.flatten() for tag in train_df.iloc[i][target]]\n",
    "\n",
    "        # Predict tags based on most common tags among neighbors\n",
    "        predicted_tags = [tag for tag, _ in Counter(neighbor_tags).most_common(n=n)]\n",
    "        # 5 tags/question en moyenne mais on peut suggérer +\n",
    "        predictions.append(predicted_tags)\n",
    "        print(f'predicted : {predicted_tags}')\n",
    "        print(precision_topics(test_df[target][i], predicted_tags), '\\n')\n",
    "\n",
    "    true_tags = [tags for tags in test_df[target][min_range:max_range]]\n",
    "\n",
    "    # mean_precision = precision_score(true_tags, predictions)\n",
    "    mean_precision = scorer(true_tags, predictions)\n",
    "\n",
    "    print(f'precision moyenne = {mean_precision}')\n",
    "\n",
    "    return mean_precision\n",
    "\n",
    "\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "predict_tags_using_knn(scorer=jaccard_score)\n",
    "\n",
    "# 0.34 de precision ?? c enorme !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Recherche manuelle du nb optimal de voisins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=45 \n",
      "\n",
      "\n",
      "\n",
      "k=46 \n",
      "\n",
      "\n",
      "\n",
      "k=47 \n",
      "\n",
      "\n",
      "\n",
      "k=48 \n",
      "\n",
      "\n",
      "\n",
      "k=49 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prend du temps\n",
    "# décommenter pour tester\n",
    "\n",
    "precision = []\n",
    "for k in range(45, 50): # pour un classifieur binaire il faut k impair, mais ici comme on veut\n",
    "    print(f'k={k}', '\\n')\n",
    "    # precision.append((k, predict_tags_using_knn(k=k)))\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  meilleur score = 0\n"
     ]
    }
   ],
   "source": [
    "# Nécessite cellule précédente décommentée\n",
    "\n",
    "# body\n",
    "# 0.33 pour k=5\n",
    "# 0.37 pour k proche de 50 !!\n",
    "\n",
    "record_max = 0\n",
    "for k, result in precision:\n",
    "    print(f'For {k} neighbors : precision moyenne = {result}')\n",
    "    if result > record_max:\n",
    "        record_max = result\n",
    "\n",
    "print('\\n', f' meilleur score = {record_max}')\n",
    "\n",
    "# sur 'title_nltk' :\n",
    "# best : 0.29 pour k=47-48\n",
    "# = 1 quart des tags predits correctement\n",
    "# pas mal !\n",
    "\n",
    "# mieux sur body que sur title (heureusement !)\n",
    "\n",
    "# tester predire seulement 3 ou 4 tags ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Influence du nb de topics prédits sur la précision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=50 \n",
      "\n",
      "\n",
      "\n",
      "k=60 \n",
      "\n",
      "\n",
      "\n",
      "k=70 \n",
      "\n",
      "\n",
      "\n",
      "k=80 \n",
      "\n",
      "\n",
      "\n",
      "k=90 \n",
      "\n",
      "\n",
      "\n",
      "k=100 \n",
      "\n",
      "\n",
      "\n",
      "k=110 \n",
      "\n",
      "\n",
      "\n",
      "k=120 \n",
      "\n",
      "\n",
      "\n",
      "k=130 \n",
      "\n",
      "\n",
      "\n",
      "k=140 \n",
      "\n",
      "\n",
      "\n",
      "k=150 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# idem, takes time\n",
    "# On fera une véritable recherche d'optimisation + tard avec MLFlow\n",
    "\n",
    "precision4 = []\n",
    "precision3 = []\n",
    "for k in range(50, 151, 10): # pour un classifieur binaire il faut k impair, mais ici comme on veut\n",
    "    # encore que ?? les nb impairs semblent obtenir de meilleurs resultats\n",
    "    print(f'k={k}', '\\n')\n",
    "    # precision4.append((k, predict_tags_using_knn(k=k, n=4)))\n",
    "    # precision3.append((k, predict_tags_using_knn(k=k, n=3)))\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  meilleur score (4 tags predits) = 0\n",
      "\n",
      "  meilleur score (3 tags predits) = 0\n"
     ]
    }
   ],
   "source": [
    "# pas touche !\n",
    "# > 0.5 !!\n",
    "\n",
    "# factoriser ?\n",
    "score_max4 = 0\n",
    "for k, result in precision4:\n",
    "    print(f'For {k} neighbors : precision moyenne = {result}')\n",
    "    if result > score_max4:\n",
    "        score_max4 = result\n",
    "\n",
    "score_max3 = 0\n",
    "for k, result in precision3:\n",
    "    print(f'For {k} neighbors : precision moyenne = {result}')\n",
    "    if result > score_max3:\n",
    "        score_max3 = result\n",
    "\n",
    "print('\\n', f' meilleur score (4 tags predits) = {score_max4}')\n",
    "print('\\n', f' meilleur score (3 tags predits) = {score_max3}')\n",
    "\n",
    "# 26\n",
    "\n",
    "# tester k > 100 ?\n",
    "# test en reduisant le corpus de tags ? ou pas (conserver la richesse du corpus ?)\n",
    "# tester ce qui prend le + de tps (1 predict = environ 10 secondes, trop long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  meilleur score (4 tags predits) = 0\n",
      "\n",
      "  meilleur score (3 tags predits) = 0\n"
     ]
    }
   ],
   "source": [
    "# yes !\n",
    "\n",
    "score_max4 = 0\n",
    "for k, result in precision4:\n",
    "    print(f'For {k} neighbors : precision moyenne = {result}')\n",
    "    if result > score_max4:\n",
    "        score_max4 = result\n",
    "\n",
    "\n",
    "score_max3 = 0\n",
    "for k, result in precision3:\n",
    "    print(f'For {k} neighbors : precision moyenne = {result}')\n",
    "    if result > score_max3:\n",
    "        score_max3 = result\n",
    "\n",
    "\n",
    "print('\\n', f' meilleur score (4 tags predits) = {score_max4}')\n",
    "print('\\n', f' meilleur score (3 tags predits) = {score_max3}')\n",
    "\n",
    "# near 60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Preprocessing (feature et target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous avons besoin de transformer la target pdt le preprocessing (-> bow)\n",
    "# mm si on ne s'sn sert pas vraiment, car grid_search.fit() n'accepte que des valeurs numériques.\n",
    "\n",
    "# Avantage : on peut utiliser des metriques classiques pour le score (ici r2),\n",
    "# mais ca n'a aucun sens metier interpretable.\n",
    "# peut tjs etre utile si fortement correlé à notre precision score custom\n",
    "# ou au score de similarité Jaccard\n",
    "\n",
    "# Compliqué à faire dans le pipeline sckikit, qui transforme les features mais pas la target.\n",
    "# TransformedTargetRegressor ne convient pas non plus ici : c'est un modele wrapper,\n",
    "# utilisé apres le pipeline.\n",
    "# trouvé qq \"solutions\" + ou - elegantes, mais rien de compatible à la fois avec sklearn et mlflow.\n",
    "\n",
    "# Ici convertir les tags en bag of words ou les one hot encoder revient exactement au meme, donc\n",
    "# autant utiliser le bow, on a deja le transformeur.\n",
    "# En + avec la plupart des embedding on peut considerer que la prediction de topics devient bien plus\n",
    "# un probleme de regression que de classification, les valeurs predites par les modeles sont des vecteurs\n",
    "# dans un espace techniquement fini, mais immense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_list_into_bow(X):\n",
    "    documents = X.tolist()\n",
    "    # print(documents)\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    bow_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    return bow_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 (tentative de) gridsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prend trop de ressources ! Il est tps d'utiliser les nested runs de mlflow.\n",
    "# Parce qu'une gridsearch qui ne peut tester qu'une seule valeur pour un seul hyperparam a la fois,\n",
    "# ce n'est pas tres utile...\n",
    "\n",
    "# En plus je me demande si la gridsearch est une bonne idée pour le knn.\n",
    "# deja pour la lda et la nmf j'avais des doutes\n",
    "# a verifier mais pour ces modeles je pense qu'un dataset d'entrainement plus petit peut\n",
    "# serieusement impacter le nb optimal pour l'hyperparam\n",
    "# (voisins pour le knn, topics pour lda et nmf)\n",
    "\n",
    "def pipe_knn(train_df=train, feature='title_nltk', target='all_tags', test_df=test, metric='cosine', graph=True):\n",
    "    # Load your training data and labels\n",
    "    X_train = train_df[feature].values\n",
    "    y_train = train_df[target].values\n",
    "\n",
    "    X_bow_matrix = token_list_into_bow(X_train)\n",
    "    y_bow_matrix = token_list_into_bow(y_train)\n",
    "\n",
    "    # Create a KNN Regressor\n",
    "    knn_regressor = KNeighborsRegressor(metric=metric)\n",
    "\n",
    "    # Create a pipeline with preprocessing and a knn regressor, to simplify gridsearch\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"knn_regressor\", knn_regressor)\n",
    "    ])\n",
    "\n",
    "    # Define hyperparameters and their possible values for grid search\n",
    "    param_grid = {\n",
    "        'knn_regressor__n_neighbors': [1],\n",
    "        'knn_regressor__weights': ['uniform'] # , 'distance'\n",
    "    }\n",
    "\n",
    "    # Create the GridSearchCV object with multiple scoring metrics\n",
    "    # scoring = {'neg_mean_squared_error': 'neg_mean_squared_error', 'r2': 'r2'}\n",
    "    grid_search = GridSearchCV(pipe, param_grid=param_grid,\n",
    "                            scoring='r2', cv=5, verbose=1) # add, refit='precision' for multiple scoring\n",
    "\n",
    "    # Fit the GridSearchCV object to your training data to perform hyperparameter tuning\n",
    "    grid_search.fit(X_bow_matrix, y_bow_matrix)\n",
    "\n",
    "    # Access the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "    # Create the KNN regressor with the best hyperparameters\n",
    "    best_knn_regressor = KNeighborsRegressor(# metric=metric,\n",
    "                                             n_neighbors=best_params['knn_regressor__n_neighbors'],\n",
    "                                             weights=best_params['knn_regressor__weights'])\n",
    "\n",
    "    # Create a pipeline with the preprocessor and the tuned knn regressor\n",
    "    pipeline_with_tuned_knn = Pipeline(steps=[\n",
    "        (\"knn_regressor\", best_knn_regressor)  # Use the tuned neighbor and weight values here\n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation (on training set) and display the scores for each split\n",
    "    # scoring = ['r2', 'neg_mean_squared_error']\n",
    "    cv_scores = cross_validate(pipeline_with_tuned_knn, X_bow_matrix, y_bow_matrix, cv=5, scoring='r2')\n",
    "    # print(\"Cross-Validation Scores (training):\", '\\n', cv_scores)\n",
    "    print(\"Cross-Validation Scores:\")\n",
    "    pprint(cv_scores)\n",
    "    for i, score in enumerate(cv_scores['test_score']):\n",
    "        print(f\"Split {i+1} : precision = {score}\")\n",
    "\n",
    "\n",
    "# pipe_knn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 test mlflow run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment knn_optimisation already exists.\n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "API request to http://localhost:5000/api/2.0/mlflow/experiments/get-by-name failed with exception HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=knn_optimisation (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f77b615ee90>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    716\u001b[0m     conn,\n\u001b[1;32m    717\u001b[0m     method,\n\u001b[1;32m    718\u001b[0m     url,\n\u001b[1;32m    719\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    720\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    721\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    722\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    723\u001b[0m )\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connectionpool.py:416\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connection.py:244\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    243\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28msuper\u001b[39m(HTTPConnection, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/http/client.py:1286\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/http/client.py:1332\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1331\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders(body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/http/client.py:1281\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/http/client.py:1041\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m \n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/http/client.py:979\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 979\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f77b6157a50>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connectionpool.py:827\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    824\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    825\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    826\u001b[0m     )\n\u001b[0;32m--> 827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    828\u001b[0m         method,\n\u001b[1;32m    829\u001b[0m         url,\n\u001b[1;32m    830\u001b[0m         body,\n\u001b[1;32m    831\u001b[0m         headers,\n\u001b[1;32m    832\u001b[0m         retries,\n\u001b[1;32m    833\u001b[0m         redirect,\n\u001b[1;32m    834\u001b[0m         assert_same_host,\n\u001b[1;32m    835\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    836\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[1;32m    837\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[1;32m    838\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    839\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw\n\u001b[1;32m    841\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connectionpool.py:827\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    824\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    825\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    826\u001b[0m     )\n\u001b[0;32m--> 827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    828\u001b[0m         method,\n\u001b[1;32m    829\u001b[0m         url,\n\u001b[1;32m    830\u001b[0m         body,\n\u001b[1;32m    831\u001b[0m         headers,\n\u001b[1;32m    832\u001b[0m         retries,\n\u001b[1;32m    833\u001b[0m         redirect,\n\u001b[1;32m    834\u001b[0m         assert_same_host,\n\u001b[1;32m    835\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    836\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[1;32m    837\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[1;32m    838\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    839\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw\n\u001b[1;32m    841\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: HTTPConnectionPool.urlopen at line 827 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connectionpool.py:827\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    824\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    825\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    826\u001b[0m     )\n\u001b[0;32m--> 827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    828\u001b[0m         method,\n\u001b[1;32m    829\u001b[0m         url,\n\u001b[1;32m    830\u001b[0m         body,\n\u001b[1;32m    831\u001b[0m         headers,\n\u001b[1;32m    832\u001b[0m         retries,\n\u001b[1;32m    833\u001b[0m         redirect,\n\u001b[1;32m    834\u001b[0m         assert_same_host,\n\u001b[1;32m    835\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    836\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[1;32m    837\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[1;32m    838\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    839\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw\n\u001b[1;32m    841\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connectionpool.py:799\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    797\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 799\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[1;32m    800\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39me, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    801\u001b[0m )\n\u001b[1;32m    802\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/experiments/create (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f77b6157a50>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/utils/rest_utils.py:107\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_http_response_with_retries(\n\u001b[1;32m    108\u001b[0m         method,\n\u001b[1;32m    109\u001b[0m         url,\n\u001b[1;32m    110\u001b[0m         max_retries,\n\u001b[1;32m    111\u001b[0m         backoff_factor,\n\u001b[1;32m    112\u001b[0m         backoff_jitter,\n\u001b[1;32m    113\u001b[0m         retry_codes,\n\u001b[1;32m    114\u001b[0m         raise_on_status,\n\u001b[1;32m    115\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    116\u001b[0m         verify\u001b[38;5;241m=\u001b[39mhost_creds\u001b[38;5;241m.\u001b[39mverify,\n\u001b[1;32m    117\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m to:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/utils/request_utils.py:197\u001b[0m, in \u001b[0;36m_get_http_response_with_retries\u001b[0;34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m allow_redirects \u001b[38;5;241m=\u001b[39m env_value \u001b[38;5;28;01mif\u001b[39;00m allow_redirects \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m allow_redirects\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method, url, allow_redirects\u001b[38;5;241m=\u001b[39mallow_redirects, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/requests/adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/experiments/create (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f77b6157a50>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mcreate_mlflow_experiment\u001b[0;34m(experiment_name, artifact_location, tags)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     experiment_id \u001b[38;5;241m=\u001b[39m mlflow\u001b[38;5;241m.\u001b[39mcreate_experiment(\n\u001b[1;32m     23\u001b[0m         name\u001b[38;5;241m=\u001b[39mexperiment_name, artifact_location\u001b[38;5;241m=\u001b[39martifact_location, tags\u001b[38;5;241m=\u001b[39mtags\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/tracking/fluent.py:1560\u001b[0m, in \u001b[0;36mcreate_experiment\u001b[0;34m(name, artifact_location, tags)\u001b[0m\n\u001b[1;32m   1520\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;124;03mCreate an experiment.\u001b[39;00m\n\u001b[1;32m   1522\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m    Creation timestamp: 1662004217511\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m MlflowClient()\u001b[38;5;241m.\u001b[39mcreate_experiment(name, artifact_location, tags)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/tracking/client.py:570\u001b[0m, in \u001b[0;36mMlflowClient.create_experiment\u001b[0;34m(self, name, artifact_location, tags)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create an experiment.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \n\u001b[1;32m    530\u001b[0m \u001b[38;5;124;03m:param name: The experiment name. Must be unique.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m    Lifecycle_stage: active\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracking_client\u001b[38;5;241m.\u001b[39mcreate_experiment(name, artifact_location, tags)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/client.py:235\u001b[0m, in \u001b[0;36mTrackingServiceClient.create_experiment\u001b[0;34m(self, name, artifact_location, tags)\u001b[0m\n\u001b[1;32m    233\u001b[0m _validate_experiment_artifact_location(artifact_location)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore\u001b[38;5;241m.\u001b[39mcreate_experiment(\n\u001b[1;32m    236\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    237\u001b[0m     artifact_location\u001b[38;5;241m=\u001b[39martifact_location,\n\u001b[1;32m    238\u001b[0m     tags\u001b[38;5;241m=\u001b[39m[ExperimentTag(key, value) \u001b[38;5;28;01mfor\u001b[39;00m (key, value) \u001b[38;5;129;01min\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mitems()] \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[1;32m    239\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/store/tracking/rest_store.py:98\u001b[0m, in \u001b[0;36mRestStore.create_experiment\u001b[0;34m(self, name, artifact_location, tags)\u001b[0m\n\u001b[1;32m     95\u001b[0m req_body \u001b[38;5;241m=\u001b[39m message_to_json(\n\u001b[1;32m     96\u001b[0m     CreateExperiment(name\u001b[38;5;241m=\u001b[39mname, artifact_location\u001b[38;5;241m=\u001b[39martifact_location, tags\u001b[38;5;241m=\u001b[39mtag_protos)\n\u001b[1;32m     97\u001b[0m )\n\u001b[0;32m---> 98\u001b[0m response_proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_endpoint(CreateExperiment, req_body)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response_proto\u001b[38;5;241m.\u001b[39mexperiment_id\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/store/tracking/rest_store.py:59\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body)\u001b[0m\n\u001b[1;32m     58\u001b[0m response_proto \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mResponse()\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call_endpoint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_host_creds(), endpoint, method, json_body, response_proto)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/utils/rest_utils.py:218\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[1;32m    217\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n\u001b[0;32m--> 218\u001b[0m     response \u001b[38;5;241m=\u001b[39m http_request(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs)\n\u001b[1;32m    219\u001b[0m response \u001b[38;5;241m=\u001b[39m verify_rest_response(response, endpoint)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/utils/rest_utils.py:129\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI request to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed with exception \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mMlflowException\u001b[0m: API request to http://localhost:5000/api/2.0/mlflow/experiments/create failed with exception HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/experiments/create (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f77b6157a50>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    716\u001b[0m     conn,\n\u001b[1;32m    717\u001b[0m     method,\n\u001b[1;32m    718\u001b[0m     url,\n\u001b[1;32m    719\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    720\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    721\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    722\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    723\u001b[0m )\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connectionpool.py:416\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connection.py:244\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    243\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28msuper\u001b[39m(HTTPConnection, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/http/client.py:1286\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/http/client.py:1332\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1331\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders(body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/http/client.py:1281\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/http/client.py:1041\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m \n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/http/client.py:979\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 979\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f77b615ee90>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connectionpool.py:827\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    824\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    825\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    826\u001b[0m     )\n\u001b[0;32m--> 827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    828\u001b[0m         method,\n\u001b[1;32m    829\u001b[0m         url,\n\u001b[1;32m    830\u001b[0m         body,\n\u001b[1;32m    831\u001b[0m         headers,\n\u001b[1;32m    832\u001b[0m         retries,\n\u001b[1;32m    833\u001b[0m         redirect,\n\u001b[1;32m    834\u001b[0m         assert_same_host,\n\u001b[1;32m    835\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    836\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[1;32m    837\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[1;32m    838\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    839\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw\n\u001b[1;32m    841\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connectionpool.py:827\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    824\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    825\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    826\u001b[0m     )\n\u001b[0;32m--> 827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    828\u001b[0m         method,\n\u001b[1;32m    829\u001b[0m         url,\n\u001b[1;32m    830\u001b[0m         body,\n\u001b[1;32m    831\u001b[0m         headers,\n\u001b[1;32m    832\u001b[0m         retries,\n\u001b[1;32m    833\u001b[0m         redirect,\n\u001b[1;32m    834\u001b[0m         assert_same_host,\n\u001b[1;32m    835\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    836\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[1;32m    837\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[1;32m    838\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    839\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw\n\u001b[1;32m    841\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: HTTPConnectionPool.urlopen at line 827 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connectionpool.py:827\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    824\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    825\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    826\u001b[0m     )\n\u001b[0;32m--> 827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    828\u001b[0m         method,\n\u001b[1;32m    829\u001b[0m         url,\n\u001b[1;32m    830\u001b[0m         body,\n\u001b[1;32m    831\u001b[0m         headers,\n\u001b[1;32m    832\u001b[0m         retries,\n\u001b[1;32m    833\u001b[0m         redirect,\n\u001b[1;32m    834\u001b[0m         assert_same_host,\n\u001b[1;32m    835\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    836\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[1;32m    837\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[1;32m    838\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    839\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw\n\u001b[1;32m    841\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/connectionpool.py:799\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    797\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 799\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[1;32m    800\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39me, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    801\u001b[0m )\n\u001b[1;32m    802\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=knn_optimisation (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f77b615ee90>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/utils/rest_utils.py:107\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_http_response_with_retries(\n\u001b[1;32m    108\u001b[0m         method,\n\u001b[1;32m    109\u001b[0m         url,\n\u001b[1;32m    110\u001b[0m         max_retries,\n\u001b[1;32m    111\u001b[0m         backoff_factor,\n\u001b[1;32m    112\u001b[0m         backoff_jitter,\n\u001b[1;32m    113\u001b[0m         retry_codes,\n\u001b[1;32m    114\u001b[0m         raise_on_status,\n\u001b[1;32m    115\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    116\u001b[0m         verify\u001b[38;5;241m=\u001b[39mhost_creds\u001b[38;5;241m.\u001b[39mverify,\n\u001b[1;32m    117\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m to:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/utils/request_utils.py:197\u001b[0m, in \u001b[0;36m_get_http_response_with_retries\u001b[0;34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m allow_redirects \u001b[38;5;241m=\u001b[39m env_value \u001b[38;5;28;01mif\u001b[39;00m allow_redirects \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m allow_redirects\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method, url, allow_redirects\u001b[38;5;241m=\u001b[39mallow_redirects, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/requests/adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=knn_optimisation (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f77b615ee90>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m experiment_id \u001b[38;5;241m=\u001b[39m create_mlflow_experiment(\n\u001b[1;32m      2\u001b[0m     experiment_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mknn_optimisation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     artifact_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./artifacts\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     tags\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodele\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mknn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnlp\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m      5\u001b[0m )\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mcreate_mlflow_experiment\u001b[0;34m(experiment_name, artifact_location, tags)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m     experiment_id \u001b[38;5;241m=\u001b[39m mlflow\u001b[38;5;241m.\u001b[39mget_experiment_by_name(experiment_name)\u001b[38;5;241m.\u001b[39mexperiment_id\n\u001b[1;32m     29\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_experiment(experiment_name\u001b[38;5;241m=\u001b[39mexperiment_name)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m experiment_id\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/tracking/fluent.py:1395\u001b[0m, in \u001b[0;36mget_experiment_by_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_experiment_by_name\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Experiment]:\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;124;03m    Retrieve an experiment by experiment name from the backend store\u001b[39;00m\n\u001b[1;32m   1368\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;124;03m        Creation timestamp: 1662004217511\u001b[39;00m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MlflowClient()\u001b[38;5;241m.\u001b[39mget_experiment_by_name(name)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/tracking/client.py:520\u001b[0m, in \u001b[0;36mMlflowClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Experiment]:\n\u001b[1;32m    490\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;124;03m    Retrieve an experiment by experiment name from the backend store\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;124;03m        Lifecycle_stage: active\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracking_client\u001b[38;5;241m.\u001b[39mget_experiment_by_name(name)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/client.py:221\u001b[0m, in \u001b[0;36mTrackingServiceClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    :param name: The experiment name.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m    :return: :py:class:`mlflow.entities.Experiment`\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore\u001b[38;5;241m.\u001b[39mget_experiment_by_name(name)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/store/tracking/rest_store.py:306\u001b[0m, in \u001b[0;36mRestStore.get_experiment_by_name\u001b[0;34m(self, experiment_name)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     req_body \u001b[38;5;241m=\u001b[39m message_to_json(GetExperimentByName(experiment_name\u001b[38;5;241m=\u001b[39mexperiment_name))\n\u001b[0;32m--> 306\u001b[0m     response_proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_endpoint(GetExperimentByName, req_body)\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Experiment\u001b[38;5;241m.\u001b[39mfrom_proto(response_proto\u001b[38;5;241m.\u001b[39mexperiment)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MlflowException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/store/tracking/rest_store.py:59\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body)\u001b[0m\n\u001b[1;32m     57\u001b[0m endpoint, method \u001b[38;5;241m=\u001b[39m _METHOD_TO_INFO[api]\n\u001b[1;32m     58\u001b[0m response_proto \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mResponse()\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call_endpoint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_host_creds(), endpoint, method, json_body, response_proto)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/utils/rest_utils.py:215\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    214\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n\u001b[0;32m--> 215\u001b[0m     response \u001b[38;5;241m=\u001b[39m http_request(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/mlflow/utils/rest_utils.py:129\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUrlException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01miu\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI request to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed with exception \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mMlflowException\u001b[0m: API request to http://localhost:5000/api/2.0/mlflow/experiments/get-by-name failed with exception HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=knn_optimisation (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f77b615ee90>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "# Si erreur, commande console\n",
    "# mlflow ui\n",
    "# depuis dossier notebooks\n",
    "\n",
    "experiment_id = create_mlflow_experiment(\n",
    "    experiment_name=\"knn_optimisation\",\n",
    "    artifact_location=\"./artifacts\",\n",
    "    tags={\"modele\": \"knn\", \"feature\": \"title\", 'nlp': 'nltk'},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: knn_optimisation\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "experiment = get_mlflow_experiment(experiment_id=experiment_id)\n",
    "print(\"Name: {}\".format(experiment.name))\n",
    "\n",
    "with mlflow.start_run(run_name=\"testing\", experiment_id=experiment_id) as run:\n",
    "\n",
    "    # log model using autolog\n",
    "    # mlflow.autolog()\n",
    "    mlflow.sklearn.autolog()\n",
    "    pipe_knn()\n",
    "\n",
    "    # print run info\n",
    "    print(\"run_id: {}\".format(run.info.run_id))\n",
    "    print(\"experiment_id: {}\".format(run.info.experiment_id))\n",
    "    print(\"Artifact Location: {}\".format(experiment.artifact_location))\n",
    "    print(\"status: {}\".format(run.info.status))\n",
    "    print(\"start_time: {}\".format(run.info.start_time))\n",
    "    print(\"end_time: {}\".format(run.info.end_time))\n",
    "    print(\"lifecycle_stage: {}\".format(run.info.lifecycle_stage))\n",
    "\n",
    "\n",
    "# J'esperais qu'mlflow allait nous permettre de contourner le probleme de\n",
    "# l'entrainement du modele, qui demande bcp d'espace memoire.\n",
    "# probleme : mm sans l'ui, le tracking/logging mlflow consomment enormement !\n",
    "# la solution a l'air pire que le probleme...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annexes, tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def predict_tags_using_rf(train_df=train[::100], feature='title_nltk', target='all_tags', test_df=test, n_estimators=50):\n",
    "    documents = train_df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    dense_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Convert multi-label tags into binary format\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y_encoded = mlb.fit_transform(train_df[target])\n",
    "\n",
    "    # Fit Random Forest model\n",
    "    rf_model = RandomForestRegressor(n_estimators=n_estimators, random_state=42)\n",
    "    rf_model.fit(dense_matrix, y_encoded)\n",
    "\n",
    "    # Predictions\n",
    "    predictions = []\n",
    "    min_range = 3000\n",
    "    max_range = 3005\n",
    "    for i in range(min_range, max_range):\n",
    "        query_document = test_df[feature][i]\n",
    "        print(f'doc {i} : {query_document}')\n",
    "        print(f'real tags : {test[target][i]}')\n",
    "        query_bow = gensim_dictionary.doc2bow(query_document)\n",
    "        query_vector = corpus2dense([query_bow], num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "        # Predict tags using Random Forest\n",
    "        prediction_encoded = rf_model.predict(query_vector.reshape(1, -1))\n",
    "\n",
    "        # Convert back to original tag format\n",
    "        predicted_tags = mlb.inverse_transform(prediction_encoded.reshape(1, -1))\n",
    "\n",
    "        predictions.append(predicted_tags)\n",
    "        print(f'predicted : {predictions[-1]}', '\\n')\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "predictions_rf = predict_tags_using_rf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_p5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
