{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# trop ?\n",
    "import os, sys, random\n",
    "import ast\n",
    "# from zipfile import ZipFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# NLP\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# modeles\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import corpus2dense\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim import similarities\n",
    "\n",
    "#\n",
    "from sklearn.metrics import make_scorer, PredictionErrorDisplay, r2_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "import mlflow.pyfunc\n",
    "\n",
    "\n",
    "def turn_str_back_into_list(df):\n",
    "    \"\"\"Correct the type change due to .csv export\"\"\"\n",
    "\n",
    "    df['title_nltk'] = df['title_nltk'].apply(ast.literal_eval)\n",
    "    df['body_nltk'] = df['body_nltk'].apply(ast.literal_eval)\n",
    "    df['title_spacy'] = df['title_spacy'].apply(ast.literal_eval)\n",
    "    df['body_spacy'] = df['body_spacy'].apply(ast.literal_eval)\n",
    "    df['all_tags'] = df['all_tags'].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "train = pd.read_csv('./../0_data/cleaned_data/train_bow_uniques.csv', sep=',')\n",
    "test = pd.read_csv('./../0_data/cleaned_data/test_bow_uniques.csv', sep=',')\n",
    "\n",
    "turn_str_back_into_list(train)\n",
    "turn_str_back_into_list(test)\n",
    "\n",
    "\n",
    "class SpecialKnn(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"A special model \"\"\"\n",
    "\n",
    "    def __init__(self, k, n=5):\n",
    "        \"\"\"\n",
    "        Constructor method. Initializes the model with the specified value `n`.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        k : int\n",
    "        \"\"\"\n",
    "        self.k = k # nb voisins, shortcut pour l'attribut .n_neighbors\n",
    "        self.n = n # nb tags predits\n",
    "        self.knn = KNeighborsRegressor(n_neighbors=k)\n",
    "        self.dict_X = Dictionary()\n",
    "        self.dict_y = Dictionary()\n",
    "\n",
    "    def load_context(self, context):\n",
    "        # when instance is created\n",
    "        # on l'utilisera + tard ?\n",
    "        pass\n",
    "\n",
    "\n",
    "    def fit(self, train_df, feature, target):\n",
    "        X_train = train_df[feature].values\n",
    "        y_train = train_df[target].values\n",
    "\n",
    "        self.dict_X, X_bow_matrix = token_list_into_bow(X_train)\n",
    "        self.dict_y, y_bow_matrix = token_list_into_bow(y_train)\n",
    "\n",
    "        # Create a KNN Regressor\n",
    "        self.knn.fit(X_bow_matrix, y_bow_matrix)\n",
    "\n",
    "\n",
    "    def predict_tokens(self, input_text, train_df=train, target='all_tags'):\n",
    "        \"\"\"Prediction method for the custom model.\"\"\"\n",
    "        # Example query\n",
    "        query_tokens = preprocess_text(input_text)\n",
    "        # print(query_tokens)\n",
    "        query_bow = self.dict_X.doc2bow(query_tokens)\n",
    "        query_vector = corpus2dense([query_bow], num_terms=len(self.dict_X)).T\n",
    "\n",
    "        # Find nearest neighbors\n",
    "        _, indices = self.knn.kneighbors(query_vector)\n",
    "\n",
    "        # Aggregate tags from neighbors\n",
    "        neighbor_tags = [tag for i in indices.flatten() for tag in train_df.iloc[i][target]]\n",
    "\n",
    "        # Predict tags based on most common tags among neighbors\n",
    "        predicted_tags = [tag for tag, _ in Counter(neighbor_tags).most_common(n=5)]\n",
    "        # 5 tags/question en moyenne mais on peut suggÃ©rer +\n",
    "        # ici a ameliorer\n",
    "\n",
    "        return predicted_tags\n",
    "\n",
    "\n",
    "# recup model\n",
    "model = pickle.load(open('artifacts/knn_model.pkl', 'rb'))\n",
    "\n",
    "# fonctions preprocessing (remplacer par un import)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    #Cleaning\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # Tokenization\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(\" \".join(tokens))  # Apply RegexpTokenizer to the entire list\n",
    "\n",
    "        # Remove punctuation (make sure, RegexpTokenizer should have done it already)\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in tokenization: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Get part of speech for each token\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    for token, pos_tag in pos_tags:\n",
    "        # ! Uncommenting next line may crash the cell\n",
    "        # print(f\"Token: {token}, POS Tag: {pos_tag}\")\n",
    "        if pos_tag.startswith('V'):\n",
    "            # On garde\n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(token, pos='v'))\n",
    "            # Returns the input word unchanged if it cannot be found in WordNet.\n",
    "        elif pos_tag.startswith('N'):\n",
    "            # On garde\n",
    "            try:\n",
    "                lemmatized_tokens.append(lemmatizer.lemmatize(token, pos='n'))\n",
    "            except Exception as e:\n",
    "                print(f\"Error lemmatizing verb {token}: {e}\")\n",
    "        # Sinon on supprime\n",
    "\n",
    "    # Read forbidden words (stopwords, too frequent, too rare) from the file\n",
    "    with open('./forbidden_words.txt', 'r') as file:\n",
    "        forbidden = [line.strip() for line in file]\n",
    "\n",
    "    filtered_list = [token for token in lemmatized_tokens if token not in forbidden]\n",
    "\n",
    "    # keep uniques\n",
    "    seen_tokens = set()\n",
    "    unique_tokens = []\n",
    "\n",
    "    for token in filtered_list:\n",
    "        if token not in seen_tokens:\n",
    "            seen_tokens.add(token)\n",
    "            if len(token) > 2:\n",
    "                unique_tokens.append(token)\n",
    "\n",
    "    return unique_tokens\n",
    "\n",
    "def token_list_into_bow(X):\n",
    "    documents = X.tolist()\n",
    "    # print(documents)\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    bow_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    return gensim_dictionary, bow_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c#', 'javascript', 'types', 'c++', 'c']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get the data from the form\n",
    "    query_text = \"type someting here\"\n",
    "\n",
    "    # ! security check here\n",
    "\n",
    "    # Convert the data to uppercase\n",
    "    # uppercase_text = query_text.upper()\n",
    "\n",
    "    # use model to predict tags\n",
    "    topics = model.predict_tokens(query_text)\n",
    "\n",
    "    # Return the result\n",
    "    print(str(topics))\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle errors\n",
    "    print(str(e))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_p5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
