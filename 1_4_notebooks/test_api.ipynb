{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator KNeighborsRegressor from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "# import pprint\n",
    "\n",
    "# trop ?\n",
    "import os, sys, random\n",
    "import ast\n",
    "# from zipfile import ZipFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# NLP\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# modeles\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import corpus2dense\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim import similarities\n",
    "\n",
    "#\n",
    "from sklearn.metrics import make_scorer, PredictionErrorDisplay, r2_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "import mlflow.pyfunc\n",
    "\n",
    "import utils\n",
    "from utils import SpecialKnn\n",
    "\n",
    "\n",
    "def turn_str_back_into_list(df):\n",
    "    \"\"\"Correct the type change due to .csv export\"\"\"\n",
    "\n",
    "    df['title_nltk'] = df['title_nltk'].apply(ast.literal_eval)\n",
    "    df['body_nltk'] = df['body_nltk'].apply(ast.literal_eval)\n",
    "    df['title_spacy'] = df['title_spacy'].apply(ast.literal_eval)\n",
    "    df['body_spacy'] = df['body_spacy'].apply(ast.literal_eval)\n",
    "    df['all_tags'] = df['all_tags'].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "train = pd.read_csv('./../0_data/cleaned_data/train_bow_uniques.csv', sep=',')\n",
    "test = pd.read_csv('./../0_data/cleaned_data/test_bow_uniques.csv', sep=',')\n",
    "\n",
    "turn_str_back_into_list(train)\n",
    "turn_str_back_into_list(test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# recup model\n",
    "model = pickle.load(open('main_knn.pkl', 'rb'))\n",
    "\n",
    "# fonctions preprocessing (remplacer par un import)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_df\u001b[38;5;241m=\u001b[39mtrain, feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_nltk\u001b[39m\u001b[38;5;124m'\u001b[39m, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_tags\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "model.fit(train_df=train, feature='title_nltk', target='all_tags')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c#', 'javascript', 'types', 'c++', 'c']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get the data from the form\n",
    "    query_text = \"type someting here\"\n",
    "\n",
    "    # ! security check here\n",
    "\n",
    "    # Convert the data to uppercase\n",
    "    # uppercase_text = query_text.upper()\n",
    "\n",
    "    # use model to predict tags\n",
    "    topics = model.predict_tokens(query_text)\n",
    "\n",
    "    # Return the result\n",
    "    print(str(topics))\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle errors\n",
    "    print(str(e))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_p5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
