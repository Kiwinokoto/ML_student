{"cells":[{"attachments":{},"cell_type":"markdown","id":"c10447d1","metadata":{"id":"c10447d1"},"source":["# **Catégorisez automatiquement des questions**\n","\n","## partie 1/8 : analyse exploratoire\n","\n","### <br> Notebook d’exploration et de pré-traitement des questions, comprenant une analyse univariée et multivariée, un nettoyage des questions, un feature engineering de type bag of words avec réduction de dimension (du vocabulaire et des tags) \n","\n","<br>\n"]},{"cell_type":"markdown","metadata":{},"source":["## 1 Objectifs, imports\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1 Contexte\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Nous avons téléchargé 50 000 questions posées sur stack overflow,\n","# ainsi que les tags associés.\n","\n","# Objectifs de notebook :\n","\n","# Comprendre nos données (forme, structure)\n","# premier nettoyage -> reduction dim\n","# Avoir une première représentation de nos données\n","# Rechercher des patterns\n","# se faire une idée de la difficulté de la tâche (prédiction de tags)\n","# (encore et surtout) : transformations en vue de reduction dim\n","\n","# => traitement de strings avec nltk, regex, etc..\n","# tester spacy\n","\n","# Dans le cadre de ce projet :\n","# Nous avons en input un fichier csv contenant 50 000 titres, questions (body), tags, etc...\n","# Nous avons besoin en output de représentations spécifiques, pour nos différents modèles.\n","\n","# Pour les modèles non-supervisé, il nous faut :\n","# - un bag of words pour la LDA (/algos similaires)\n","# - un TS-IDF pour la NMF\n","# Dans les deux cas, cela implique un cleaning \"aggressif\", visant à conserver uniquement\n","# les mots qui ont du sens pour ce projet (= qui aident la prédiction de tag par les modèles).\n","# encore et tjs, la reduction dim !\n","\n","# / supervisés ?\n"]},{"attachments":{},"cell_type":"markdown","id":"8cf10133","metadata":{"id":"8cf10133"},"source":["### 1.2 Importation des librairies, réglages\n"]},{"cell_type":"code","execution_count":null,"id":"6ffe8b0d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9566,"status":"ok","timestamp":1688504146442,"user":{"displayName":"Kevin Oudelet","userId":"05301463766297982835"},"user_tz":-120},"id":"6ffe8b0d","outputId":"4abc7d5f-4fe6-46db-fa59-05891e583e93"},"outputs":[],"source":["import os, sys, random\n","from zipfile import ZipFile\n","import numpy as np\n","import pandas as pd\n","from pandarallel import pandarallel\n","\n","# Visualisation\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from wordcloud import WordCloud\n","from PIL import Image\n","\n","# NLP\n","from bs4 import BeautifulSoup\n","import re, string\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","from collections import Counter\n","# if necessary\n","# !python -m spacy download en_core_web_sm\n","# !python -m spacy download en_core_web_md\n","import spacy\n","\n","print('\\nPython version ' + sys.version)\n","print('pandas version ' + pd.__version__)\n","print('sns version ' + sns.__version__)\n","\n","plt.style.use('ggplot')\n","pd.set_option('display.max_columns', 200)\n","sns.set(font_scale=1)\n","\n","# Make sure we downloaded the models successfully\n","nlp = spacy.load(\"en_core_web_sm\")\n","nlp = spacy.load(\"en_core_web_md\")\n","\n","# Modify if necessary\n","num_cores = os.cpu_count()\n","print(f\"\\nNumber of CPU cores: {num_cores}\")\n","pandarallel.initialize(progress_bar=False, nb_workers=6)\n"]},{"cell_type":"markdown","id":"6dc1e6dd","metadata":{},"source":["### 1.3 Fonctions\n"]},{"cell_type":"code","execution_count":null,"id":"76f32233","metadata":{},"outputs":[],"source":["# Principalement des fonctions pour l'affichage des graphiques pdt l'EDA\n","\n","def quick_look(df, miss=True):\n","    \"\"\"\n","    Display a quick overview of a DataFrame, including shape, head, tail, unique values, and duplicates.\n","\n","    Args:\n","        df (pandas.DataFrame): The input DataFrame to inspect.\n","        check_missing (bool, optional): Whether to check and display missing values (default is True).\n","\n","    The function provides a summary of the DataFrame, including its shape, the first and last rows, the count of unique values per column, and the number of duplicates.\n","    If `check_missing` is set to True, it also displays missing value information.\n","    \"\"\"\n","    print(f'shape : {df.shape}')\n","\n","    display(df.head())\n","    display(df.tail())\n","\n","    print('uniques :')\n","    display(df.nunique())\n","\n","    print('Doublons ? ', df.duplicated(keep='first').sum(), '\\n')\n","\n","    if miss:\n","        display(get_missing_values(df))\n","\n","\n","def lerp(a, b, t):\n","    \"\"\"\n","    Linear interpolation between two values 'a' and 'b' at a parameter 't'.\n","    A very useful little function, used here to position annotations in plots.\n","    Got it coding with Radu :)\n","\n","    Given two values 'a' and 'b', and a parameter 't',\n","    this function calculates the linear interpolation between 'a' and 'b' at 't'.\n","\n","    Parameters:\n","    a (float or int): The start value.\n","    b (float or int): The end value.\n","    t (float): The interpolation parameter (typically in the range [0, 1], but can be outside).\n","\n","    Returns:\n","    float or int: The interpolated value at parameter 't'.\n","    \"\"\"\n","    return a + (b - a) * t\n","\n","\n","def generate_random_pastel_colors(n):\n","    \"\"\"\n","    Generates a list of n random pastel colors, represented as RGBA tuples.\n","\n","    Parameters:\n","    n (int): The number of pastel colors to generate.\n","\n","    Returns:\n","    list: A list of RGBA tuples representing random pastel colors.\n","\n","    Example:\n","    >>> generate_random_pastel_colors(2)\n","    [(0.749, 0.827, 0.886, 1.0), (0.886, 0.749, 0.827, 1.0)]\n","    \"\"\"\n","    colors = []\n","    for _ in range(n):\n","        # Generate random pastels\n","        red = round(random.randint(150, 250) / 255.0, 3)\n","        green = round(random.randint(150, 250) / 255.0, 3)\n","        blue = round(random.randint(150, 250) / 255.0, 3)\n","\n","        # Create an RGB color tuple and add it to the list\n","        color = (red,green,blue, 1.0)\n","        colors.append(color)\n","\n","    return colors\n","\n","print(generate_random_pastel_colors(2))\n","\n","\n","def get_missing_values(df):\n","    \"\"\"Generates a DataFrame containing the count and proportion of missing values for each feature.\n","\n","    Args:\n","        df (pandas.DataFrame): The input DataFrame to analyze.\n","\n","    Returns:\n","        pandas.DataFrame: A DataFrame with columns for the feature name, count of missing values,\n","        count of non-missing values, proportion of missing values, and data type for each feature.\n","    \"\"\"\n","    # Count the missing values for each column\n","    missing = df.isna().sum()\n","\n","    # Calculate the percentage of missing values\n","    percent_missing = df.isna().mean() * 100\n","\n","    # Create a DataFrame to store the results\n","    missings_df = pd.DataFrame({\n","        'column_name': df.columns,\n","        'missing': missing,\n","        'present': df.shape[0] - missing,  # Count of non-missing values\n","        'percent_missing': percent_missing.round(2),  # Rounded to 2 decimal places\n","        'type': df.dtypes\n","    })\n","\n","    # Sort the DataFrame by the count of missing values\n","    missings_df.sort_values('missing', inplace=True)\n","\n","    return missings_df\n","\n","# with pd.option_context('display.max_rows', 1000):\n","#   display(get_missing_values(df))\n","\n","\n","def hist_distrib(dataframe, feature, bins, decimal_places, density=True):\n","    \"\"\"\n","    Visualize the empirical distribution of a numerical feature using a histogram.\n","    Calcul des principaux indicateurs de tendance centrale, dispersion et forme.\n","\n","    Args:\n","        dataframe (pandas.DataFrame): The input DataFrame containing the feature.\n","        feature (str): The name of the numerical feature to visualize.\n","        bins (int): The number of bins for the histogram.\n","        decimal_places (int): The number of decimal places for rounding numeric values.\n","        density (bool, optional): Whether to display the histogram as a density plot (default is True).\n","\n","    Returns:\n","        float: The skewness of the feature's distribution.\n","\n","    The function generates a histogram of the feature, displays various statistics, and returns the skewness of the distribution.\n","    \"\"\"\n","    # Calculate central tendencies and dispersion\n","    mode_value = round(dataframe[feature].mode()[0], decimal_places)\n","    mode_non_zero = \"N/A\"\n","    if (dataframe[feature] != 0).any():\n","        mode_non_zero = round(dataframe.loc[dataframe[feature] != 0, feature].mode()[0], decimal_places)\n","    median_value = round(dataframe[feature].median(), decimal_places)\n","    mean_value = round(dataframe[feature].mean(), decimal_places)\n","\n","    # Calculate dispersion\n","    var_emp = round(dataframe[feature].var(ddof=0), decimal_places)\n","    coeff_var = round(dataframe[feature].std(ddof=0), decimal_places)\n","\n","    # Calculate shape indicators\n","    skewness_value = round(dataframe[feature].skew(), 2)\n","    kurtosis_value = round(dataframe[feature].kurtosis(), 2)\n","\n","    # Create the plot\n","    fig, ax = plt.subplots(figsize=(12, 5))\n","    dataframe[feature].hist(density=density, bins=bins, ax=ax)\n","\n","    # Adjust placement for annotations\n","    yt = plt.yticks()\n","    y_position = lerp(yt[0][0], yt[0][-1], 0.8)\n","    y_increment = y_position / 20\n","    xt = plt.xticks()\n","    x_position = lerp(xt[0][0], xt[0][-1], 0.7)\n","\n","    # Add annotations with horizontal and vertical alignment\n","    annotation_fs = 13\n","    color = 'g'\n","    ax.annotate(f'Mode: {mode_value}', xy=(x_position, y_position), fontsize=annotation_fs,\n","                xytext=(x_position, y_position), color=color, ha='left', va='bottom')\n","    ax.annotate(f'Mode +: {mode_non_zero}', xy=(x_position, y_position - y_increment), fontsize=annotation_fs,\n","                xytext=(x_position, y_position - y_increment), color=color, ha='left', va='bottom')\n","    ax.annotate(f'Median: {median_value}', xy=(x_position, y_position - 2 * y_increment), fontsize=annotation_fs,\n","                xytext=(x_position, y_position - 2 * y_increment), color=color, ha='left', va='bottom')\n","    ax.annotate(f'Mean: {mean_value}', xy=(x_position, y_position - 3 * y_increment), fontsize=annotation_fs,\n","                xytext=(x_position, y_position - 3 * y_increment), color=color, ha='left', va='bottom')\n","    ax.annotate(f'Var Emp: {var_emp}', xy=(x_position, y_position - 5 * y_increment), fontsize=annotation_fs,\n","                xytext=(x_position, y_position - 5 * y_increment), color=color, ha='left', va='bottom')\n","    ax.annotate(f'Coeff Var: {coeff_var}', xy=(x_position, y_position - 6 * y_increment), fontsize=annotation_fs,\n","                xytext=(x_position, y_position - 6 * y_increment), color=color, ha='left', va='bottom')\n","    ax.annotate(f'Skewness: {skewness_value}', xy=(x_position, y_position - 8 * y_increment), fontsize=annotation_fs,\n","                xytext=(x_position, y_position - 8 * y_increment), color=color, ha='left', va='bottom')\n","    ax.annotate(f'Kurtosis: {kurtosis_value}', xy=(x_position, y_position - 9 * y_increment), fontsize=annotation_fs,\n","                xytext=(x_position, y_position - 9 * y_increment), color=color, ha='left', va='bottom')\n","\n","    # Label the x-axis and y-axis\n","    ax.set_xlabel(feature, fontsize=12)\n","    ax.set_ylabel('Frequency', fontsize=12)\n","\n","    # Show the plot\n","    plt.title(f'Distribution of {feature}', pad=20, fontsize=18)\n","    plt.xticks(fontsize=12)\n","    plt.yticks(fontsize=12)\n","    plt.show()\n","\n","    return skewness_value\n","\n","\n","def boxplot_distrib(dataframe, feature):\n","    \"\"\"\n","    Affiche un boxplot, pour visualiser les tendances centrales et la dispersion d'une variable.\n","\n","    Args:\n","        dataframe (pandas.DataFrame): The input DataFrame containing the feature.\n","        feature (str): The name of the numerical feature to visualize.\n","\n","    The function generates a box plot of the feature to display central tendencies (median and mean) and dispersion.\n","    \"\"\"\n","    fig, ax = plt.subplots(figsize=(10, 4))\n","\n","    medianprops = {'color':\"blue\"}\n","    meanprops = {'marker':'o', 'markeredgecolor':'black',\n","            'markerfacecolor':'firebrick'}\n","\n","    dataframe.boxplot(feature, vert=False, showfliers=False, medianprops=medianprops, patch_artist=True, showmeans=True, meanprops=meanprops)\n","\n","    plt.xticks(fontsize=12)\n","    plt.yticks(fontsize=12)\n","    plt.show()\n","\n","\n","def courbe_lorenz(dataframe, feature):\n","    \"\"\"\n","    Affiche une courbe de Lorenz, pour visualiser la concentration d'une variable\n","    Calcule l'indice de Gini\n","    Visualize a Lorenz curve to assess the concentration of a variable and calculate the Gini coefficient.\n","\n","    Args:\n","        dataframe (pandas.DataFrame): The input DataFrame containing the feature.\n","        feature (str): The name of the numerical feature to visualize.\n","\n","    The function generates a Lorenz curve to assess the concentration of the feature and calculates the Gini coefficient.\n","    \"\"\"\n","    fig, ax = plt.subplots(figsize=(12, 5))\n","    values = dataframe.loc[dataframe[feature].notna(), feature].values\n","    # print(values)\n","    n = len(values)\n","    lorenz = np.cumsum(np.sort(values)) / values.sum()\n","    lorenz = np.append([0],lorenz) # La courbe de Lorenz commence à 0\n","\n","    xaxis = np.linspace(0-1/n,1+1/n,n+1)\n","    #Il y a un segment de taille n pour chaque individu, plus 1 segment supplémentaire d'ordonnée 0.\n","    # #Le premier segment commence à 0-1/n, et le dernier termine à 1+1/n.\n","    plt.plot(xaxis,lorenz,drawstyle='steps-post')\n","    plt.plot(np.arange(2),[x for x in np.arange(2)])\n","    # calcul de l'indice de Gini\n","    AUC = (lorenz.sum() -lorenz[-1]/2 -lorenz[0]/2)/n # Surface sous la courbe de Lorenz. Le premier segment (lorenz[0]) est à moitié en dessous de 0, on le coupe donc en 2, on fait de même pour le dernier segment lorenz[-1] qui est à moitié au dessus de 1.\n","    S = 0.5 - AUC # surface entre la première bissectrice et le courbe de Lorenz\n","    gini = 2*S\n","    plt.annotate('gini =  ' + str(round(gini, 2)), xy = (0.04, 0.88), fontsize = 13, xytext = (0.04, 0.88), color = 'g')\n","    plt.xticks(fontsize=12)\n","    plt.yticks(fontsize=12)\n","    plt.show()\n","\n","\n","def graphs_analyse_uni(dataframe, feature, bins=50, r=5, density=True):\n","    \"\"\"\n","    Affiche histogramme + boxplot + courbe de Lorenz\n","\n","    Args:\n","        dataframe (pandas.DataFrame): The input DataFrame containing the feature.\n","        feature (str): The name of the numerical feature to analyze.\n","        bins (int, optional): The number of bins for the histogram (default is 50).\n","        decimal_places (int, optional): The number of decimal places for rounding numeric values (default is 5).\n","        density (bool, optional): Whether to display the histogram as a density plot (default is True).\n","\n","    The function generates and displays an analysis of the given numerical feature, including an histogram, a box plot, and a Lorenz curve.\n","    \"\"\"\n","    hist_distrib(dataframe, feature, bins, r)\n","    boxplot_distrib(dataframe, feature)\n","    courbe_lorenz(dataframe, feature)\n","\n","\n","def shape_head(df, nb_rows=5):\n","    \"\"\"\n","    Affiche les dimensions et les premières lignes dùun dataframe\n","    Display the dimensions and the first rows of a DataFrame.\n","\n","    Args:\n","        df (pandas.DataFrame): The input DataFrame to display.\n","        nb_rows (int, optional): The number of rows to display (default is 5, max is 60).\n","\n","    The function prints the dimensions of the DataFrame and displays the first few rows.\n","    \"\"\"\n","    print(df.shape)\n","    display(df.head(nb_rows))\n","\n","\n","def doughnut(df, feature, title, width=10, height=10):\n","    \"\"\"\n","    Affiche la répartition d'une feature sous forme de diagramme circulaire\n","    Display the distribution of a feature as a doughnut chart.\n","    Les couleurs sont aléatoires.\n","\n","    Args:\n","        df (pandas.DataFrame): The input DataFrame containing the feature.\n","        feature (str): The name of the feature to visualize.\n","        title (str): The title for the doughnut chart.\n","        width (int, optional): The width of the chart (default is 10).\n","        height (int, optional): The height of the chart (default is 10).\n","\n","    The function creates a doughnut chart to visualize the distribution of the specified feature.\n","    If you don't like the colors, try running it again :)\n","    \"\"\"\n","    colors = generate_random_pastel_colors(20)\n","\n","    grouped_df = df.groupby(feature).size().to_frame(\"count_per_type\").reset_index()\n","    pie = grouped_df.set_index(feature).copy()\n","\n","    fig, ax = plt.subplots(figsize=(width, height))\n","\n","    patches, texts, autotexts = plt.pie(x=pie['count_per_type'], autopct='%1.1f%%',\n","        startangle=-30, labels=pie.index, textprops={'fontsize':11, 'color':'#000'},\n","        labeldistance=1.25, pctdistance=0.85, colors=colors)\n","\n","    plt.title(\n","    label=title,\n","    fontdict={\"fontsize\":17},\n","    pad=20\n","    )\n","\n","    for text in texts:\n","        # text.set_fontweight('bold')\n","        text.set_horizontalalignment('center')\n","\n","    # Customize percent labels\n","    for autotext in autotexts:\n","        autotext.set_horizontalalignment('center')\n","        autotext.set_fontstyle('italic')\n","        autotext.set_fontsize('10')\n","\n","    #draw circle\n","    centre_circle = plt.Circle((0,0),0.7,fc='white')\n","    fig = plt.gcf()\n","    fig.gca().add_artist(centre_circle)\n","\n","    plt.show()\n","\n","\n","def get_non_null_values(df):\n","    \"\"\"\n","    Génère un dataframe contenant le nombre et la proportion de non-null (non-zero) valeurs pour chaque feature\n","    Generate a DataFrame containing the count and proportion of non-null (non-zero) values for each feature.\n","\n","    Args:\n","        df (pandas.DataFrame): The input DataFrame to analyze.\n","\n","    The function calculates and returns a DataFrame with the count and percentage of non-null (non-zero) values for each feature.\n","    \"\"\"\n","    non_null_counts = df.ne(0).sum()\n","    percent_non_null = (non_null_counts / df.shape[0]) * 100\n","    non_null_values_df = pd.DataFrame({'column_name': df.columns,\n","                                       'non_null_count': non_null_counts,\n","                                       'percent_non_null': percent_non_null.round(2),\n","                                       'type': df.dtypes})\n","    non_null_values_df.sort_values('non_null_count', inplace=True)\n","    return non_null_values_df\n","\n","\n","def get_colors(n=7):\n","    \"\"\"\n","    Generate a list of random colors from multiple colormaps.\n","\n","    Args:\n","        n (int, optional): The number of colors to sample from each colormap (default is 7).\n","\n","    Returns:\n","        list: A list of random colors sampled from different colormaps.\n","    \"\"\"\n","    num_colors_per_colormap = n\n","    colormaps = [plt.cm.Pastel2, plt.cm.Set1, plt.cm.Paired]\n","    all_colors = []\n","\n","    for colormap in colormaps:\n","        colors = colormap(np.linspace(0, 1, num_colors_per_colormap))\n","        all_colors.extend(colors)\n","\n","    np.random.shuffle(all_colors)\n","\n","    return all_colors\n"]},{"cell_type":"markdown","metadata":{},"source":["## 2 EDA et preprocessing des questions\n"]},{"attachments":{},"cell_type":"markdown","id":"a334fc5e","metadata":{"id":"a334fc5e"},"source":["### 2.1 Importation des données brutes\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Données compressées sinon on dépasse la limite /objet de Github (50Mb)\n","\n","# path to the zip file\n","zip_file_path = './../data/raw_data/QueryResults.zip'\n","\n","# directory where you want to extract the contents\n","extract_to_dir = './../data/raw_data'\n","\n","# Open the zip file\n","with ZipFile(zip_file_path, 'r') as zip_ref:\n","    # Extract all the contents into the specified directory\n","    zip_ref.extractall(extract_to_dir)\n","\n","# L'encodage est bien UTF-8 (vérifié en ouvrant le .csv ds vscode)\n","raw_data = pd.read_csv('./../data/raw_data/QueryResults.csv', sep=',')\n","\n","quick_look(raw_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Pas de valeurs manquantes, à part FavoriteCount (pas important)\n","\n","# Seulement 2 titres identiques / 50 000 lignes\n","# id est bien une clé primaire\n","# body aussi, pas de questions en doublon (identiques)\n","\n","# On a environ 48 000 sets de tags différents, vérifier + tard combien de tags uniques\n","\n","# Les types semblent corrects,\n","# à part les dates bien sûr\n","\n","raw_data['CreationDate'] = pd.to_datetime(raw_data['CreationDate'])\n","# Après je pense qu'on n'utilisera jamais ces dates... Juste au cas où.\n","\n","raw_data.describe()\n","\n","# Avec nos critères (cf requete sql, fin ntbk2), il a fallu retourner jusqu'à mai 2011\n","# pour avoir 50 000 questions. On retrouve ici le favoriteCount très bas, proche de 0.\n","# On a une moyenne pour le score (environ 50), le nv de vues (66 000) et de réponses (>5)\n","# dans notre corpus.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["raw_questions_tags = raw_data[['Title', 'Body', 'Tags']].copy()\n","\n","# Rename columns\n","raw_questions_tags = raw_questions_tags.rename(columns={'Title': 'title',\n","                                                        'Body': 'body',\n","                                                        'Tags': 'tags'})\n"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2 Suppression des tags html\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["raw_questions_tags['body'] = raw_questions_tags['body'].parallel_apply(\n","    lambda x: BeautifulSoup(x, 'html.parser').get_text())\n","\n","quick_look(raw_questions_tags)\n","\n","# environ 25s sans pandarallel -> 5-6s sur 6 cores\n","# pandarallel adopté !\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Je ne vois pas de balises dans les titres\n","# mais j'avoue je n'ai pas lu les 50 000 titres, donc au cas où :\n","\n","raw_questions_tags['title'] = raw_questions_tags['title'].parallel_apply(\n","    lambda x: BeautifulSoup(x, 'html.parser').get_text())\n","\n","# Le warning n'a pas d'importance ici : du texte qui ne contient pas de tags html\n","# n'est pas modifié par BeautifulSoup.get_text()\n","\n","# J'hésitais à concaténer title + body ici, mais on testera aussi les modeles seulement sur les titres\n","# pour avoir une baseline. On va donc conserver les 2 features, et les traiter en parallèle.\n"]},{"cell_type":"markdown","metadata":{},"source":["### 2.3 Tokenisation, majuscules, ponctuation\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Après qq tests, le + efficace ici semble d'appliquer d'abord la fonction word_tokenize(),\n","# qui gère mieux beaucoup de spécificités de l'anglais (ex: contractions),\n","# puis le RegexpTokenizer pour 'découper' ensuite les adresses mails, URLS, etc...\n","# Il y aurait d'autres stratégies possibles, par exemples utiliser d'abord le module contractions.\n","\n","def preprocess_1_tokenize(text, remove_punct=True):\n","    \"\"\"\n","    Clean input text (lower, strip spaces), Tokenize, Remove punctuation (optional).\n","\n","    Parameters:\n","    - text (str): Input text to tokenize.\n","    - remove_punct (bool): filter punctuation marks.\n","\n","    Returns:\n","    - list: List of tokens.\n","    \"\"\"\n","    # Nettoyage des majuscules et des espaces\n","    text = text.lower().strip()\n","    try:\n","        tokens = nltk.word_tokenize(text)\n","        tokenizer = nltk.RegexpTokenizer(r'\\w+')\n","        tokens = tokenizer.tokenize(\" \".join(tokens))  # Apply RegexpTokenizer to the entire list\n","\n","        if remove_punct:\n","            # Remove punctuation (make sure)\n","            tokens = [token for token in tokens if token not in string.punctuation]\n","\n","    except Exception as e:\n","        print(f\"Error in tokenization: {e}\")\n","        return []\n","\n","    return tokens\n","\n","# Apply to 'body' feature\n","raw_questions_tags['body_tokens'] = raw_questions_tags['body'].parallel_apply(\n","    lambda x: preprocess_1_tokenize(x))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Idem for 'title' column\n","raw_questions_tags['title_tokens'] = raw_questions_tags['title'].parallel_apply(\n","    lambda x: preprocess_1_tokenize(x))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["raw_questions_tags['nb_tokens_body'] = raw_questions_tags['body_tokens'].apply(len)\n","raw_questions_tags['nb_tokens_title'] = raw_questions_tags['title_tokens'].apply(len)\n","\n","raw_questions_tags.describe()\n","\n","# La question la + courte est composée de 7 tokens.\n","# La + longue, de presque 5000, soit... environ 15 pages ?!\n","\n","display(raw_questions_tags.loc[(raw_questions_tags['nb_tokens_body'] == 7)\n","                               | (raw_questions_tags['nb_tokens_body'] == 4845), : ])\n","\n","print(raw_questions_tags.loc[raw_questions_tags['nb_tokens_body'] == 4845, ['body']].values[0])\n","# Ca a l'air compliqué en effet !\n"]},{"cell_type":"markdown","metadata":{},"source":["### 2.4 Fréquence des mots\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract tokens from the 'tokens_uniques' column\n","all_tokens = ([token for tokens_list in raw_questions_tags['title_tokens'] for token in tokens_list]\n","                 + [token for tokens_list in raw_questions_tags['body_tokens'] for token in tokens_list])\n","\n","print(len(all_tokens))\n","# + d'un million de \"mots\" (tokens) différents !\n","\n","# Calculate token frequencies using a loop\n","# J'ai découvert plus tard la fonction FreqDist() de nltk, qui fait la mm chose\n","token_frequencies_dict = {}\n","for token in all_tokens:\n","    token_frequencies_dict[token] = token_frequencies_dict.get(token, 0) + 1\n","\n","# Display the first 50 items in the token frequencies dictionary\n","for token, frequency in list(token_frequencies_dict.items())[:50]:\n","    print(f\"{token}: {frequency}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Sort the dictionary items by values in descending order\n","sorted_token_frequencies = dict(sorted(token_frequencies_dict.items(), key=lambda item: item[1], reverse=True))\n","\n","# Display the first 50 items in the token frequencies dictionary\n","for token, frequency in list(sorted_token_frequencies.items())[:50]:\n","    print(f\"{token}: {frequency}\")\n","\n","# La plupart de ces mots appartiennent probablement à la liste des stopwords ?\n","# Note du futur : c plutôt l'inverse en fait, la liste des stopwords étant très courte (179 mots).\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualisons\n","\n","df_freq = pd.DataFrame(list(token_frequencies_dict.items()), columns=['Token', 'Frequency'])\n","\n","# Sort the DataFrame by frequency in descending order\n","df_freq = df_freq.sort_values(by='Frequency', ascending=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the distribution using Plotly\n","\n","df = df_freq[:200]\n","\n","fig = px.bar(df, x='Token', y='Frequency', labels={'Token': 'Token', 'Frequency': 'Frequency'},\n","             title='Token Frequency Distribution', text='Frequency')\n","fig.update_xaxes(tickangle=90, tickmode='array', tickvals=df.index, ticktext=df['Token'])\n","\n","# Show the plot\n","fig.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Mots rares\n","\n","# Count tags with frequency less than 10\n","tags_below_10 = sum(1 for frequency in sorted_token_frequencies.values() if frequency < 10)\n","print(tags_below_10)\n","\n","# voire tres rares\n","# hapax (1 seule occurence)\n","\n","# Count tags with frequency less than 2\n","tags_below_2 = sum(1 for frequency in sorted_token_frequencies.values() if frequency < 2)\n","print(tags_below_2)\n","\n","# Environ 10% des tokens sont des hapax, ils n'apparaissent qu'une seule fois.\n","# Ces mots ne peuvent donc pas aider à la compréhension par le contexte.\n","\n","# + de 20% des tokens apparaissent - de 10 fois ds tt le corpus.\n","\n","tokens_hapax = [token for token, frequency in list(sorted_token_frequencies.items()) if frequency == 1]\n","tokens_rares = [token for token, frequency in list(sorted_token_frequencies.items()) if frequency <= 10]\n"]},{"cell_type":"markdown","metadata":{},"source":["### 2.5 Stopwords\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get a set of English stopwords from NLTK\n","stopwords = set(nltk.corpus.stopwords.words('english'))\n","print(len(stopwords), '\\n')\n","\n","important_tokens = [token for token in all_tokens if token not in stopwords]\n","print(len(important_tokens), '\\n')\n","# + d'un quart de nos tokens appartiennent à la liste de stopwords (179 mots) !\n","\n","# Display the first 50 items in the token frequencies dictionary\n","for token, frequency in list(sorted_token_frequencies.items())[:200]:\n","    if token in important_tokens:\n","        print(f\"{token}: {frequency}\")\n","\n","# Il y a encore bcp de mots très communs. Pas sûr où placer le seuil.\n","# Le barplot était + parlant.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# About these thresholds :\n","\n","# lim_max could be a bit lower, maybe around 35 000\n","# But some important words appear between 35 000 and 50 000\n","# ex: java, the second most frequent tag (see 3.3)\n","\n","# lim_min could probably be higher\n","# ask mentor\n","\n","def get_forbidden_tokens(lim_max=50000, lim_min=1):\n","    too_frequent_tokens = [token for token, frequency in list(sorted_token_frequencies.items()) if frequency >= lim_max]\n","    too_rare_tokens = [token for token, frequency in list(sorted_token_frequencies.items()) if frequency <= lim_min]\n","\n","    forbidden = set(stopwords)\n","    forbidden.update(too_frequent_tokens)\n","    forbidden.update(too_rare_tokens)\n","\n","    return forbidden\n","\n","forbidden = get_forbidden_tokens()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def preprocess_2_remove_stopwords_frequent_rare(liste_tokens):\n","    filtered_list = [token for token in liste_tokens if token not in forbidden]\n","\n","    return filtered_list\n","\n","# Apply the preprocessing function to bodies\n","raw_questions_tags['body_tokens_no_stopwords'] = raw_questions_tags['body_tokens'].parallel_apply(\n","    preprocess_2_remove_stopwords_frequent_rare)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# and titles\n","raw_questions_tags['title_tokens_no_stopwords'] = raw_questions_tags['title_tokens'].parallel_apply(\n","    preprocess_2_remove_stopwords_frequent_rare)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["raw_questions_tags['nb_tokens_body_no_stopwords'] = raw_questions_tags['body_tokens_no_stopwords'].apply(len)\n","raw_questions_tags['nb_tokens_title_no_stopwords'] = raw_questions_tags['title_tokens_no_stopwords'].apply(len)\n","\n","raw_questions_tags.describe()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Plotting 'nb_mots'\n","raw_questions_tags['nb_tokens_body'].hist(density=False, bins=50, ax=ax, color='#7cf', alpha=0.9, label='nb de mots / question')\n","\n","# Plotting 'nb_mots non stopwords'\n","raw_questions_tags['nb_tokens_body_no_stopwords'].hist(density=False, bins=50, color='#4f6', ax=ax,\n","                                                          alpha=0.6, label='nb mots hors stopwords + freq')\n","\n","plt.title('Vocabulaire', pad=20, fontsize=18)\n","plt.xticks(fontsize=12)\n","plt.yticks(fontsize=12)\n","\n","ax.legend()\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### 2.6 Lemmatization + filtrage syntaxique\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# + filtrage implicite du vocabulaire non-anglais\n","\n","# Lemmatization tends to be more accurate but can be slower than stemming.\n","# On a utilisé le stemming en français, dans le cours openclassrooms\n","# Utilisons la lemmatisation ici, + appropriée pour notre objectif\n","# (extraire le sens important, le thème, pour proposer des tags)\n","# On peut aussi profiter de l'analyse grammaticale (POS) pour\n","# conserver uniquement les mots les plus \"importants\" dans la phrase, les noms et les verbes.\n","\n","\n","def preprocess_3_lemmatize(tokens, filter=True):\n","    lemmatizer = WordNetLemmatizer()\n","\n","    def get_wordnet_pos(pos_tag):\n","        if pos_tag.startswith('J'):\n","            return wordnet.ADJ\n","        elif pos_tag.startswith('R'):\n","            return wordnet.ADV\n","        elif pos_tag.startswith('V'):\n","            return wordnet.VERB\n","        elif pos_tag.startswith('N'):\n","            return wordnet.NOUN\n","        else:\n","            return wordnet.NOUN  # Default to noun if the part of speech is not recognized\n","\n","    # Get part of speech for each token\n","    pos_tags = nltk.pos_tag(tokens)\n","\n","    # Lemmatize each token\n","    lemmatized_tokens = []\n","    for token, pos_tag in pos_tags:\n","        if filter and pos_tag.startswith(('J', 'R')):\n","            # Exclude adjectives (J) and adverbs (R) if filter is enabled\n","            # = do nothing\n","            pass\n","            # = continue\n","        else:\n","            lemmatized_tokens.append(lemmatizer.lemmatize(token, pos=get_wordnet_pos(pos_tag)))\n","\n","    return lemmatized_tokens\n","\n","\n","# Assuming raw_questions_tags is a DataFrame with a column 'tokens_uniques_no_stopwords'\n","# Replace 'tokens_uniques_no_stopwords' with the actual column name if needed\n","raw_questions_tags['body_lemms'] = raw_questions_tags['body_tokens_no_stopwords'].parallel_apply(preprocess_3_lemmatize)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["raw_questions_tags['title_lemms'] = raw_questions_tags['title_tokens_no_stopwords'].parallel_apply(preprocess_3_lemmatize)\n"]},{"cell_type":"code","execution_count":null,"id":"385c1200","metadata":{},"outputs":[],"source":["raw_questions_tags['nb_lemms_body'] = raw_questions_tags['body_lemms'].apply(len)\n","raw_questions_tags['nb_lemms_title'] = raw_questions_tags['title_lemms'].apply(len)\n"]},{"cell_type":"markdown","metadata":{},"source":["### 2.7 tokens (lemms) uniques\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# On aurait pu procéder à cette étape bien plus tôt (juste après l'étude de fréquences)\n","# et ainsi économiser du temps de calcul et de l'espace mémoire, mais on se garde ainsi\n","# la possibilité de tester notre LDA (notebook 3) sur un bag-of-words \"classique\".\n","\n","def preprocess_4_keep_uniques_only(liste_tokens):\n","    seen_tokens = set()\n","    unique_tokens = []\n","\n","    for token in liste_tokens:\n","        if token not in seen_tokens:\n","            seen_tokens.add(token)\n","            unique_tokens.append(token)\n","\n","    return unique_tokens\n","\n","# Apply the preprocessing function to body\n","raw_questions_tags['body_lemms_uniques'] = raw_questions_tags['body_lemms'].parallel_apply(preprocess_4_keep_uniques_only)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# and title\n","raw_questions_tags['title_lemms_uniques'] = raw_questions_tags['title_lemms'].parallel_apply(preprocess_4_keep_uniques_only)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["raw_questions_tags['nb_lemms_uniques_body'] = raw_questions_tags['body_lemms_uniques'].apply(len)\n","raw_questions_tags['nb_lemms_uniques_title'] = raw_questions_tags['title_lemms_uniques'].apply(len)\n","\n","display(raw_questions_tags.describe())\n","\n","display(raw_questions_tags)\n","\n","# Légère baisse pour les titres, en revanche\n","# le nb de tokens dans les questions (body) est divisé par 2 !\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Plotting 'nb_mots'\n","raw_questions_tags['nb_tokens_body'].hist(density=False, bins=50, ax=ax, color='#7cf', alpha=0.9, label='nb de mots / question')\n","\n","# Plotting 'nb_mots non stopwords'\n","raw_questions_tags['nb_tokens_body_no_stopwords'].hist(density=False, bins=50, color='#4f6', ax=ax,\n","                                                          alpha=0.6, label='nb mots hors stopwords + freq')\n","\n","# Plotting 'lemms'\n","raw_questions_tags['nb_lemms_body'].hist(density=False, bins=50, color='#eb1', ax=ax,\n","                                                          alpha=1.0, label='nb_lemms')\n","\n","# Plotting 'lemms uniques'\n","raw_questions_tags['nb_lemms_uniques_body'].hist(density=False, bins=50, color='#f55', ax=ax,\n","                                                          alpha=1.0, label='nb_lemms_uniques')\n","\n","plt.title('Vocabulaire', pad=20, fontsize=18)\n","plt.xticks(fontsize=12)\n","plt.yticks(fontsize=12)\n","\n","ax.legend()\n","\n","plt.show()\n","\n","# On voit une petite différence !\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract tokens from the 'tokens_uniques' column\n","all_lemms = set(([token for tokens_list in raw_questions_tags['title_lemms_uniques'] for token in tokens_list]\n","                 + [token for tokens_list in raw_questions_tags['body_lemms_uniques'] for token in tokens_list]))\n","\n","print(len(all_lemms))\n","# Notre corpus a été divisé par 8 environ.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Corpus de départ\n","graphs_analyse_uni(raw_questions_tags, 'nb_tokens_body', bins=50, r=2, density=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Corpus d'arrivée\n","graphs_analyse_uni(raw_questions_tags, 'nb_lemms_uniques_body', bins=50, r=2, density=True)\n","\n","# Moyenne divisée par 4\n"]},{"cell_type":"markdown","metadata":{},"source":["### 2.8 Explorer le corpus obtenu\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Affichons 20 questions au hasard (+ les tags)\n","display(raw_questions_tags.sample(20))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Pistes restantes\n","\n","# Le preprocessing de nos questions est assez satisfaisant, il répond à nos objectifs.\n","# Pour aller + loin, on pourrait par exemple examiner plus attentivement les valeurs numériques,\n","# les dates, URLs, adresses Email, noms propres, etc...\n","\n","# Ces traitements plus avancés peuvent vite devenir horriblement compliqués\n","# avec un pur traitement nltk + regex, sans être jamais complètement satisfaisants.\n","# Filtrer les dates par exemple, juste les dates... C'est déjà un travail colossal à cause des\n","# (très) nombreux formats possibles.\n","# Identifier les noms propres... c'est simplement impossible sans machine learning.\n","\n","# Nous allons donc en profiter pour tester spacy, qui est orienté objet et utilise différents modèles\n","# préentrainés. (cf partie 4)\n","\n","# Avant cela, intéressons-nous aux tags, qui sont nos futures targets.\n"]},{"cell_type":"markdown","metadata":{},"source":["## 3 EDA et preprocessing des targets\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3.1 Convertion (string to list)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def convert_tags_string_to_list(tags_string):\n","    # just in case\n","    tags_string = tags_string.lower()\n","    # Split tags by '><'\n","    tags_list = tags_string.split('><')\n","\n","    # Remove angle brackets from the first and last tag\n","    tags_list[0] = tags_list[0][1:]\n","    tags_list[-1] = tags_list[-1][:-1]\n","\n","    return tags_list\n","\n","# Apply the preprocessing function to the 'tags' column\n","raw_questions_tags['tags_list'] = raw_questions_tags['tags'].apply(convert_tags_string_to_list)\n","\n","display(raw_questions_tags[['tags', 'tags_list']].head(10))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["raw_questions_tags['nb_tags'] = raw_questions_tags['tags_list'].apply(len)\n","raw_questions_tags['nb_tags'].describe()\n","\n","# min : 5\n","# max : 6\n","# Un intervalle plutôt réduit.\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2 Corpus des tags, Doublons éventuels\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_tags = [tag for tags in raw_questions_tags['tags_list'] for tag in tags]\n","print(len(all_tags))\n","\n","# Au cas où il y aurait des doublons ds les tags\n","raw_questions_tags['tags_uniques'] = raw_questions_tags['tags_list'].apply(preprocess_4_keep_uniques_only)\n","\n","# display(raw_questions_tags)\n","\n","# Fréquence\n","# Factoriser ?\n","tag_frequencies_dict = {}\n","for tag in all_tags:\n","    tag_frequencies_dict[tag] = tag_frequencies_dict.get(tag, 0) + 1\n","\n","# Sort the dictionary items by values in descending order\n","sorted_tag_frequencies = dict(sorted(tag_frequencies_dict.items(), key=lambda item: item[1], reverse=True))\n","\n","# Display the first 50 items in the tag frequencies dictionary\n","for tag, frequency in list(sorted_tag_frequencies.items())[:50]:\n","    print(f\"{tag}: {frequency}\")\n","\n","# 250 000 tags (différents) !\n","# Ca repond a la question : est-ce qu'un humain pourrait facilement faire cette tache.\n","# Il faudrait déjà connaitre ts les tags possibles... Pas évident.\n","\n","# Le tag le plus présent est python\n","# Est-ce qu'il y a plus de codeurs en python qu'en javascript ?\n","# Peut-être. Pas sûr.\n","# Est-ce qu'on va sur stack overflow plus souvent quand on fait du python ?\n","# Je dirais que oui. Il y a tjs de nveaux modules à découvrir en python !\n","\n","# Classement intéressant !\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3 Frequences des tags\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract tags and frequencies\n","tags = list(sorted_tag_frequencies.keys())\n","frequencies = list(sorted_tag_frequencies.values())\n","\n","# Plot\n","plt.figure(figsize=(12, 6))\n","plt.bar(tags[:100], frequencies[:100], color='skyblue')\n","plt.xlabel('Tags')\n","plt.ylabel('Frequency')\n","plt.title('Top 50 Tag Frequencies')\n","plt.xticks(rotation=90, ha='right', fontsize=8)  # Rotate x-axis labels for better readability\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3.4 Les 20 tags les + frequents\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# + d'un tiers des question concernent python, js ou java.\n","\n","# Faisons un top 20\n","df_freq = pd.DataFrame({'Tag': tags, 'Frequency': frequencies})\n","\n","# Display the DataFrame\n","display(df_freq.head(20))\n","\n","# plot a pie\n","colors = generate_random_pastel_colors(20)\n","\n","fig, ax = plt.subplots(figsize=(10, 10))\n","\n","patches, texts, autotexts = plt.pie(x=df_freq['Frequency'][:20], autopct='%1.1f%%',\n","    startangle=-30, labels=df_freq['Tag'][:20], textprops={'fontsize':11, 'color':'#000'},\n","    labeldistance=1.25, pctdistance=0.85, colors=colors)\n","\n","plt.title(\n","label='20 most frequent Tags',\n","fontdict={\"fontsize\":17},\n","pad=20\n",")\n","\n","for text in texts:\n","    # text.set_fontweight('bold')\n","    text.set_horizontalalignment('center')\n","\n","# Customize percent labels\n","for autotext in autotexts:\n","    autotext.set_horizontalalignment('center')\n","    autotext.set_fontstyle('italic')\n","    autotext.set_fontsize('10')\n","\n","#draw circle\n","centre_circle = plt.Circle((0,0),0.7,fc='white')\n","fig = plt.gcf()\n","fig.gca().add_artist(centre_circle)\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3.5 Tags rares\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# A l'inverse, certains tags sont rares\n","\n","# Count tags with frequency less than 10\n","tags_below_10 = sum(1 for frequency in sorted_tag_frequencies.values() if frequency < 10)\n","print(tags_below_10)\n","\n","# voire tres rares\n","# tags hapax (1 seule occurence)\n","\n","# Count tags with frequency less than 2\n","tags_below_2 = sum(1 for frequency in sorted_tag_frequencies.values() if frequency < 2)\n","print(tags_below_2)\n","\n","# On les supprime ?\n","# seuil ?\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3.6 WordCloud\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def show_cloud(list):\n","    cloud = WordCloud(background_color='white',\n","                    stopwords=[],\n","                    max_words=50).generate(\" \".join(list))\n","    plt.imshow(cloud)\n","    plt.axis('off')\n","    plt.show()\n","\n","\n","show_cloud(all_tags)\n","# Pourquoi python tout petit ?? C le tag le + frequent !\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# analyse multi\n","# Nous allons rapidement visualiser les wordclouds des 5 tags les + fréquents.\n","\n","most_frequent_tags = tags[:5]\n","print(most_frequent_tags)\n","\n","subsets = dict()\n","\n","for t in most_frequent_tags:\n","    subsets[t] = [tag for tags in raw_questions_tags['tags_list'] for tag in tags if t in tags]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for subset in subsets:\n","    print(f'Tag : {subset}')\n","    show_cloud(subsets[subset])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Sans les répétitions :\n","\n","for subset in subsets:\n","    print(f'Tag : {subset}')\n","    show_cloud(set(subsets[subset]))\n","\n","# C'est plus parlant !\n"]},{"cell_type":"markdown","metadata":{},"source":["## 4 Spacy\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nlp = spacy.load(\"en_core_web_sm\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### 4.1 Test : use spacy as tokenizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def tokenize_with_spacy(text):\n","    doc = nlp(text)\n","\n","    return [token.text for token in doc]\n","\n","\n","# Apply to original 'title' column\n","raw_questions_tags['title_spacy_tokens'] = raw_questions_tags['title'].parallel_apply(\n","    lambda x: tokenize_with_spacy(x))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["display(raw_questions_tags.head())\n"]},{"cell_type":"markdown","metadata":{},"source":["### 4.2 Preprocessing with spacy\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# pour la suppression des stopwods et tokens très fréquents / très rares\n","forbidden.update(nlp.Defaults.stop_words)\n","\n","def do_everything_with_spacy(text):\n","    doc = nlp(text)\n","\n","    # Filter out punctuation tokens, and tokens that are not nouns or verbs\n","    nouns_and_verbs = [token for token in doc if not token.is_punct and token.pos_ in ('NOUN', 'VERB')]\n","\n","    # Filter out common stop words from nouns and verbs\n","    important_tokens = [token.lemma_ for token in nouns_and_verbs if token.text.lower() not in forbidden]\n","\n","    # Additional filters for dates or other criteria can be added\n","    # but we already filtered non english words implicitely, by keeping only nouns and verbs\n","\n","    return important_tokens\n","\n","\n","# Apply to original 'title' column\n","raw_questions_tags['title_spacy'] = raw_questions_tags['title'].parallel_apply(\n","    lambda x: do_everything_with_spacy(x))\n","\n","# + rapide que le simple tokenizer ??\n"]},{"cell_type":"code","execution_count":null,"id":"3927f356","metadata":{},"outputs":[],"source":["# Apply to original 'body' column (without html)\n","raw_questions_tags['body_spacy'] = raw_questions_tags['body'].parallel_apply(\n","    lambda x: do_everything_with_spacy(x))\n"]},{"cell_type":"code","execution_count":null,"id":"74f173df","metadata":{},"outputs":[],"source":["# keep only uniques\n","raw_questions_tags['body_lemms_uniques_spacy'] = raw_questions_tags['body_spacy'].parallel_apply(preprocess_4_keep_uniques_only)\n","raw_questions_tags['title_lemms_uniques_spacy'] = raw_questions_tags['title_spacy'].parallel_apply(preprocess_4_keep_uniques_only)\n"]},{"cell_type":"code","execution_count":null,"id":"3f49b036","metadata":{},"outputs":[],"source":["raw_questions_tags['nb_lemms_uniques_body_spacy'] = raw_questions_tags['body_lemms_uniques_spacy'].apply(len)\n","raw_questions_tags['nb_lemms_uniques_title_spacy'] = raw_questions_tags['title_spacy'].apply(len)\n"]},{"cell_type":"code","execution_count":null,"id":"a7219514","metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Plotting 'lemms uniques'\n","raw_questions_tags['nb_lemms_uniques_body'].hist(density=False, bins=50, color='#7cf', ax=ax,\n","                                                          alpha=1.0, label='nb_lemms_uniques (nltk)')\n","\n","# Plotting 'lemms'\n","raw_questions_tags['nb_lemms_body'].hist(density=False, bins=50, color='#4f6', ax=ax,\n","                                                          alpha=1.0, label='nb_lemms_uniques (spacy)')\n","\n","plt.title('Vocabulaire', pad=20, fontsize=18)\n","plt.xticks(fontsize=12)\n","plt.yticks(fontsize=12)\n","\n","ax.legend()\n","\n","plt.show()\n","\n","# On voit une petite différence !\n"]},{"cell_type":"markdown","metadata":{},"source":["## Normalisation\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":5}
