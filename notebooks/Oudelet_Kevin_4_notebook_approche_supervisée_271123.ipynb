{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Catégorisez automatiquement des questions**\n",
    "\n",
    "### partie 4/8 : Prédiction de tags, approche supervisée + tracking mlflow\n",
    "\n",
    "#### <br> Notebook d’exploration et de pré-traitement des questions, comprenant une analyse univariée et multivariée, un nettoyage des questions, un feature engineering de type bag of words avec réduction de dimension (du vocabulaire et des tags) \n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import librairies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python version 3.11.5 (main, Sep 11 2023, 13:23:44) [GCC 11.2.0]\n",
      "pyLDAvis version 3.4.0\n",
      "\n",
      "Number of CPU cores: 8\n",
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, random\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "from pandarallel import pandarallel\n",
    "from pprint import pprint\n",
    "import json\n",
    "import requests\n",
    "from collections import Counter\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "# NLP\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import corpus2dense\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim import similarities\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "#\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.metrics import make_scorer, PredictionErrorDisplay\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "print('\\nPython version ' + sys.version)\n",
    "print('pyLDAvis version ' + pyLDAvis.__version__)\n",
    "\n",
    "# Modify if necessary\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"\\nNumber of CPU cores: {num_cores}\")\n",
    "pandarallel.initialize(progress_bar=False, nb_workers=6)\n",
    "\n",
    "#\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.models.signature import infer_signature, ModelSignature #, Schema, ParamSchema\n",
    "from mlflow.types import Schema, ParamSchema, ParamSpec, ColSpec\n",
    "\n",
    "# os.environ['MLFLOW_TRACKING_URI'] = './'\n",
    "\n",
    "# ! REQUIRES CONSOLE COMMAND : mlflow ui\n",
    "# Utilisable seulement en local...\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = MlflowClient(tracking_uri=\"http://127.0.0.1:5000\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_values(df):\n",
    "    \"\"\"Generates a DataFrame containing the count and proportion of missing values for each feature.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with columns for the feature name, count of missing values,\n",
    "        count of non-missing values, proportion of missing values, and data type for each feature.\n",
    "    \"\"\"\n",
    "    # Count the missing values for each column\n",
    "    missing = df.isna().sum()\n",
    "\n",
    "    # Calculate the percentage of missing values\n",
    "    percent_missing = df.isna().mean() * 100\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    missings_df = pd.DataFrame({\n",
    "        'column_name': df.columns,\n",
    "        'missing': missing,\n",
    "        'present': df.shape[0] - missing,  # Count of non-missing values\n",
    "        'percent_missing': percent_missing.round(2),  # Rounded to 2 decimal places\n",
    "        'type': df.dtypes\n",
    "    })\n",
    "\n",
    "    # Sort the DataFrame by the count of missing values\n",
    "    missings_df.sort_values('missing', inplace=True)\n",
    "\n",
    "    return missings_df\n",
    "\n",
    "# with pd.option_context('display.max_rows', 1000):\n",
    "#   display(get_missing_values(df))\n",
    "\n",
    "\n",
    "def quick_look(df, miss=True):\n",
    "    \"\"\"\n",
    "    Display a quick overview of a DataFrame, including shape, head, tail, unique values, and duplicates.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame to inspect.\n",
    "        check_missing (bool, optional): Whether to check and display missing values (default is True).\n",
    "\n",
    "    The function provides a summary of the DataFrame, including its shape, the first and last rows, the count of unique values per column, and the number of duplicates.\n",
    "    If `check_missing` is set to True, it also displays missing value information.\n",
    "    \"\"\"\n",
    "    print(f'shape : {df.shape}')\n",
    "\n",
    "    display(df.head())\n",
    "    display(df.tail())\n",
    "\n",
    "    print('uniques :')\n",
    "    display(df.nunique())\n",
    "\n",
    "    print('Doublons ? ', df.duplicated(keep='first').sum(), '\\n')\n",
    "\n",
    "    if miss:\n",
    "        display(get_missing_values(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlflow_experiment(\n",
    "    experiment_name: str, artifact_location: str, tags: dict[str, str]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a new mlflow experiment with the given name and artifact location.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    experiment_name: str\n",
    "        The name of the experiment to create.\n",
    "    artifact_location: str\n",
    "        The artifact location of the experiment to create.\n",
    "    tags: dict[str,Any]\n",
    "        The tags of the experiment to create.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    experiment_id: str\n",
    "        The id of the created experiment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        experiment_id = mlflow.create_experiment(\n",
    "            name=experiment_name, artifact_location=artifact_location, tags=tags\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Experiment {experiment_name} already exists.\")\n",
    "        experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "    mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "    return experiment_id\n",
    "\n",
    "\n",
    "def get_mlflow_experiment(\n",
    "    experiment_id: str = None, experiment_name: str = None\n",
    ") -> mlflow.entities.Experiment:\n",
    "    \"\"\"\n",
    "    Retrieve the mlflow experiment with the given id or name.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    experiment_id: str\n",
    "        The id of the experiment to retrieve.\n",
    "    experiment_name: str\n",
    "        The name of the experiment to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    experiment: mlflow.entities.Experiment\n",
    "        The mlflow experiment with the given id or name.\n",
    "    \"\"\"\n",
    "    if experiment_id is not None:\n",
    "        experiment = mlflow.get_experiment(experiment_id)\n",
    "    elif experiment_name is not None:\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    else:\n",
    "        raise ValueError(\"Either experiment_id or experiment_name must be provided.\")\n",
    "\n",
    "    return experiment\n",
    "\n",
    "\n",
    "def turn_str_back_into_list(df):\n",
    "    \"\"\"Correct the type change due to .csv export\"\"\"\n",
    "\n",
    "    df['title_nltk'] = df['title_nltk'].apply(ast.literal_eval)\n",
    "    df['body_nltk'] = df['body_nltk'].apply(ast.literal_eval)\n",
    "    df['title_spacy'] = df['title_spacy'].apply(ast.literal_eval)\n",
    "    df['body_spacy'] = df['body_spacy'].apply(ast.literal_eval)\n",
    "    df['all_tags'] = df['all_tags'].apply(ast.literal_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, this needs mlfow ui console command first -> unusable on remote server\n",
    "# all_experiments = client.search_experiments()\n",
    "# pprint(all_experiments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>all_tags</th>\n",
       "      <th>title_nltk</th>\n",
       "      <th>body_nltk</th>\n",
       "      <th>title_spacy</th>\n",
       "      <th>body_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-06-05 15:13:02</td>\n",
       "      <td>How to use memset while handling strings in C++?</td>\n",
       "      <td>I am from Python background and recently learn...</td>\n",
       "      <td>[c++, initialization, c-strings, string-litera...</td>\n",
       "      <td>[memset, handle, string]</td>\n",
       "      <td>[memset, handle, string, python, background, l...</td>\n",
       "      <td>[use, memset, handle, string]</td>\n",
       "      <td>[background, learn, function, memset, follow, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-10-31 12:35:02</td>\n",
       "      <td>How to correct spelling in google docs using k...</td>\n",
       "      <td>I would like to be able to replace a misspelle...</td>\n",
       "      <td>[gmail, keyboard-shortcuts, google-docs, short...</td>\n",
       "      <td>[correct, spell, google, doc, keyboard, shortcut]</td>\n",
       "      <td>[correct, spell, google, doc, shortcut, like, ...</td>\n",
       "      <td>[correct, spelling, keyboard, shortcut]</td>\n",
       "      <td>[like, replace, word, recommend, correction, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-19 10:40:23</td>\n",
       "      <td>live server vscode on another computer</td>\n",
       "      <td>I have 2 computers. when I open the project wi...</td>\n",
       "      <td>[visual-studio-code, server, localhost, ip, live]</td>\n",
       "      <td>[server, vscode, computer]</td>\n",
       "      <td>[server, vscode, computer, open, project, give...</td>\n",
       "      <td>[server, vscode, computer]</td>\n",
       "      <td>[computer, open, project, server, url, want, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-10-23 16:47:04</td>\n",
       "      <td>django ajax post 403 forbidden</td>\n",
       "      <td>using django 1.4 im getting a 403 error when i...</td>\n",
       "      <td>[javascript, ajax, django, http-post, http-sta...</td>\n",
       "      <td>[django, ajax, forbidden]</td>\n",
       "      <td>[django, ajax, get, error, try, post, javascri...</td>\n",
       "      <td>[forbid]</td>\n",
       "      <td>[django, error, try, post, javascript, server,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-04-21 16:10:24</td>\n",
       "      <td>Listen to changes and reload container on code...</td>\n",
       "      <td>I am using docker-compose in visual studio 201...</td>\n",
       "      <td>[angular, visual-studio, docker, docker-compos...</td>\n",
       "      <td>[listen, change, reload, container, code, dock...</td>\n",
       "      <td>[listen, change, reload, container, code, dock...</td>\n",
       "      <td>[listen, change, reload, container, code, dock...</td>\n",
       "      <td>[docker, compose, studio, window, run, contain...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          CreationDate                                              title  \\\n",
       "0  2019-06-05 15:13:02   How to use memset while handling strings in C++?   \n",
       "1  2018-10-31 12:35:02  How to correct spelling in google docs using k...   \n",
       "2  2020-09-19 10:40:23             live server vscode on another computer   \n",
       "3  2012-10-23 16:47:04                     django ajax post 403 forbidden   \n",
       "4  2019-04-21 16:10:24  Listen to changes and reload container on code...   \n",
       "\n",
       "                                                body  \\\n",
       "0  I am from Python background and recently learn...   \n",
       "1  I would like to be able to replace a misspelle...   \n",
       "2  I have 2 computers. when I open the project wi...   \n",
       "3  using django 1.4 im getting a 403 error when i...   \n",
       "4  I am using docker-compose in visual studio 201...   \n",
       "\n",
       "                                            all_tags  \\\n",
       "0  [c++, initialization, c-strings, string-litera...   \n",
       "1  [gmail, keyboard-shortcuts, google-docs, short...   \n",
       "2  [visual-studio-code, server, localhost, ip, live]   \n",
       "3  [javascript, ajax, django, http-post, http-sta...   \n",
       "4  [angular, visual-studio, docker, docker-compos...   \n",
       "\n",
       "                                          title_nltk  \\\n",
       "0                           [memset, handle, string]   \n",
       "1  [correct, spell, google, doc, keyboard, shortcut]   \n",
       "2                         [server, vscode, computer]   \n",
       "3                          [django, ajax, forbidden]   \n",
       "4  [listen, change, reload, container, code, dock...   \n",
       "\n",
       "                                           body_nltk  \\\n",
       "0  [memset, handle, string, python, background, l...   \n",
       "1  [correct, spell, google, doc, shortcut, like, ...   \n",
       "2  [server, vscode, computer, open, project, give...   \n",
       "3  [django, ajax, get, error, try, post, javascri...   \n",
       "4  [listen, change, reload, container, code, dock...   \n",
       "\n",
       "                                         title_spacy  \\\n",
       "0                      [use, memset, handle, string]   \n",
       "1            [correct, spelling, keyboard, shortcut]   \n",
       "2                         [server, vscode, computer]   \n",
       "3                                           [forbid]   \n",
       "4  [listen, change, reload, container, code, dock...   \n",
       "\n",
       "                                          body_spacy  \n",
       "0  [background, learn, function, memset, follow, ...  \n",
       "1  [like, replace, word, recommend, correction, k...  \n",
       "2  [computer, open, project, server, url, want, b...  \n",
       "3  [django, error, try, post, javascript, server,...  \n",
       "4  [docker, compose, studio, window, run, contain...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./../data/cleaned_data/train_bow_uniques.csv', sep=',')\n",
    "test = pd.read_csv('./../data/cleaned_data/test_bow_uniques.csv', sep=',')\n",
    "\n",
    "turn_str_back_into_list(train)\n",
    "turn_str_back_into_list(test)\n",
    "\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic ML models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Often gives good results if enough data\n",
    "# Accepts basically any input, as long as it is numerical\n",
    "\n",
    "# => Perfect for testing different embeddings !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dummy knn : il copie sur le + proche voisin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['javascript', 'internet-explorer', 'class', 'internet-explorer-8', 'classname']\n",
      "['javascript', 'internet-explorer', 'class', 'internet-explorer-8', 'classname'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_tags_using_dummy_knn(df, feature, target, k=1, exemple=None):\n",
    "    documents = df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    dense_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Ici on ne va pas demander au knn de faire de prediction,\n",
    "    # On veut juste qu'il trouve les voisins.\n",
    "    # Mais la fonction fit a besoin de targets en param\n",
    "    target_values = df[target].values\n",
    "\n",
    "    # Initialize kNN model\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=k, metric='cosine', algorithm='brute')\n",
    "    knn_model.fit(dense_matrix, target_values)\n",
    "\n",
    "    # Example query\n",
    "    query_document = exemple\n",
    "    query_bow = gensim_dictionary.doc2bow(query_document)\n",
    "    query_vector = corpus2dense([query_bow], num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Find nearest neighbors\n",
    "    _, indices = knn_model.kneighbors(query_vector)\n",
    "\n",
    "    # Aggregate tags from neighbors\n",
    "    neighbor_tags = [tag for i in indices.flatten() for tag in df.iloc[i][target]]\n",
    "\n",
    "    print(neighbor_tags)\n",
    "\n",
    "    # Predict tags based on most common tags among neighbors\n",
    "    predicted_tags = [tag for tag, _ in Counter(neighbor_tags).most_common(n=10)]\n",
    "    # 5 tags/question en moyenne mais on peut suggérer +\n",
    "    # ici a ameliorer\n",
    "\n",
    "    return predicted_tags, knn_model\n",
    "\n",
    "\n",
    "exemple = [\"your\", 'text', 'document', 'javascript']\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "predicted_tags, knn_test = predict_tags_using_dummy_knn(train, 'title_nltk', 'all_tags', exemple=exemple)\n",
    "print(predicted_tags, '\\n')\n",
    "\n",
    "# javascript ok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### knn basic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c#', '.net', 'ms-word', 'openxml', 'openxml-sdk', 'javascript', 'internet-explorer', 'class', 'internet-explorer-8', 'classname', 'java', 'javascript', 'ajax', 'selenium', 'htmlunit-driver', 'c#', '.net', 'windows', 'f#', 'console', 'javascript', 'jquery', 'jquery-plugins', 'text-to-speech', 'html5-audio', 'javascript', 'html', 'css', 'text', 'truncate', 'javascript', 'jquery', 'css', 'dom', 'document', 'javascript', 'html', 'string', 'text', 'extract', 'javascript', 'dom', 'substring', 'indexof', 'getselection', 'javascript', 'html', 'function', 'text', 'onclick', 'c#', '.net', 'html', 'pdf', 'extract', 'javascript', 'jquery', 'css', 'copy', 'cut', 'javascript', 'html', 'url', 'base64', 'data-uri', 'python', 'module', 'preprocessor', 'nlp', 'stemming', 'javascript', 'php', 'jquery', 'curl', 'http-headers', 'python', 'text', 'replace', 'ms-word', 'python-docx', 'c#', 'javascript', 'html', 'http', 'dom', 'javascript', 'jquery', 'ruby-on-rails', 'tdd', 'jasmine', 'ios', 'swift', 'height', 'uilabel', 'frame', 'javascript', 'html', 'performance', 'three.js', 'webgl', 'ios', 'objective-c', 'uilabel', 'autolayout', 'uistoryboard', 'linux', 'assembly', 'x86-64', 'calling-convention', 'abi', 'javascript', 'jquery', 'html', 'css', 'bootstrap-4', 'localization', 'internationalization', 'translation', 'stripe-payments', 'stripe.net', 'html', 'spring', 'reactjs', 'spring-boot', 'thymeleaf', 'javascript', 'html', 'function', 'anchor', 'href', 'javascript', 'jquery', 'html', 'dom', 'document-ready', 'javascript', 'jquery', 'textarea', 'hyperlink', 'addition', 'html', 'sublimetext2', 'sublimetext', 'indentation', 'reformat', 'javascript', 'arrays', 'object', 'arguments', 'slice', 'javascript', 'jquery', 'ajax', 'asp.net-mvc-3', 'url', 'javascript', 'iphone', 'css', 'ios', 'uiwebview', 'javascript', 'unicode', 'diacritics', 'combining-marks', 'zalgo', 'javascript', 'firebase', 'google-cloud-platform', 'google-cloud-firestore', 'typeerror', 'javascript', 'function', 'oop', 'if-statement', 'conditional-statements', 'javascript', 'html', 'full-text-search', 'local-storage', 'client-side', 'javascript', 'node.js', 'backbone.js', 'ember.js', 'javascript-framework', 'c#', 'javascript', 'jquery', 'asp.net-mvc', 'razor', 'javascript', 'oop', 'functional-programming', 'polymorphism', 'parametric-polymorphism', 'html', 'meta-tags', 'semantics', 'semantic-web', 'semantic-markup', 'mongodb', 'mongodb-query', 'aggregation-framework', 'spring-data-mongodb', 'full-text-indexing', 'python', 'django', 'orm', 'mongodb', 'mongoengine', 'javascript', 'c#', 'asp.net-mvc', 'dictionary', 'asp.net-web-api', 'javascript', 'css', 'encapsulation', 'styling', 'web-component', 'ios', 'ocr', 'xcode4.5', 'tesseract', 'leptonica', 'javascript', 'reactjs', 'functional-programming', 'immutability', 'immutable.js', 'html', 'css', 'text', 'autocomplete', 'sublimetext', 'javascript', 'object', 'recursion', 'comparison', 'equality', 'javascript', 'arrays', 'algorithm', 'big-o', 'time-complexity', 'objective-c', 'ios', 'xcode', 'event-handling', 'uibutton']\n",
      "['javascript', 'html', 'jquery', 'css', 'c#', 'text', 'ios', 'dom', '.net', 'function'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add grid search cv\n",
    "# add score\n",
    "\n",
    "def predict_tags_using_knn(df, feature, target, k=50, exemple=None):\n",
    "    documents = df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    dense_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Ici on ne va pas demander au knn de faire de prediction,\n",
    "    # On veut juste qu'il trouve les voisins.\n",
    "    # Mais la fonction fit a besoin de targets en param\n",
    "    target_values = df[target].values\n",
    "\n",
    "    # Initialize kNN model\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=k, metric='cosine', algorithm='brute')\n",
    "    knn_model.fit(dense_matrix, target_values)\n",
    "\n",
    "    # Example query\n",
    "    query_document = exemple\n",
    "    query_bow = gensim_dictionary.doc2bow(query_document)\n",
    "    query_vector = corpus2dense([query_bow], num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Find nearest neighbors\n",
    "    _, indices = knn_model.kneighbors(query_vector)\n",
    "\n",
    "    # Aggregate tags from neighbors\n",
    "    neighbor_tags = [tag for i in indices.flatten() for tag in df.iloc[i][target]]\n",
    "\n",
    "    print(neighbor_tags)\n",
    "\n",
    "    # Predict tags based on most common tags among neighbors\n",
    "    predicted_tags = [tag for tag, _ in Counter(neighbor_tags).most_common(n=10)]\n",
    "    # 5 tags/question en moyenne mais on peut suggérer +\n",
    "    # ici a ameliorer\n",
    "\n",
    "    return predicted_tags\n",
    "\n",
    "exemple = [\"your\", 'text', 'document', 'javascript']\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "predicted_tags = predict_tags_using_knn(train, 'title_nltk', 'all_tags', exemple=exemple)\n",
    "print(predicted_tags, '\\n')\n",
    "\n",
    "# javascript ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'module', 'preprocessor', 'nlp', 'stemming', 'c#', '.net', 'ms-word', 'openxml', 'openxml-sdk', 'python', 'parsing', 'text', 'file-io', 'python-2.7', 'python-2.7', 'ubuntu', 'python-3.x', 'spatial-index', 'r-tree', 'python', 'plot', 'tree', 'data-visualization', 'visualization', 'c#', '.net', 'windows', 'f#', 'console', 'python', 'html', 'web-scraping', 'text', 'beautifulsoup', 'python', 'python-3.x', 'algorithm', 'sorting', 'mergesort', 'python', 'python-2.7', 'reflection', 'delegation', 'message-passing', 'python', 'python-3.x', 'annotations', 'lint', 'type-hinting', 'python', 'pdf', 'python-3.7', 'pypdf', 'pdf-extraction', 'python', 'selenium', 'selenium-webdriver', 'xpath', 'webdriverwait', 'python', 'macos', 'python-3.x', 'sublimetext2', 'sublimetext', 'python-3.x', 'pdf', 'text', 'extract', 'pdfminer', 'python', 'documentation', 'python-3.7', 'docstring', 'python-dataclasses', 'python', 'text', 'stemming', 'plural', 'singular', 'nlp', 'cluster-analysis', 'data-mining', 'k-means', 'text-mining', 'python', 'image', 'opencv', 'image-processing', 'computer-vision', 'python', 'regex', 'performance', 'perl', 'text-processing', 'python', 'windows', 'user-interface', 'text', 'screen', 'python', 'shell', 'encoding', 'utf-8', 'python-2.x', 'python', 'html', 'excel', 'pandas', 'dataframe', 'python', 'python-3.x', 'python-2.7', 'text-extraction', 'pdfminer', 'python', 'python-2.7', 'tkinter', 'callback', 'tkinter.text', 'python', 'utf-8', 'python-unicode', 'windows-1252', 'cp1252', 'python', 'image', 'text', 'python-imaging-library', 'bold', 'python', 'parsing', 'data-structures', 'dictionary', 'nested', 'python', 'text', 'replace', 'docx', 'zip', 'python', 'selenium', 'xpath', 'selenium-webdriver', 'webdriver', 'python', 'plugins', 'sublimetext2', 'distutils', 'python-requests', 'python', 'text', 'replace', 'ms-word', 'python-docx', 'python', 'utf-8', 'character-encoding', 'locale', 'default', 'python', 'django', 'autocomplete', 'sublimetext2', 'sublimetext', 'c#', '.net', 'html', 'pdf', 'extract', 'python', 'python-3.x', 'file', 'text', 'count', 'python', 'file', 'encryption', 'aes', 'pycrypto', 'python', 'list', 'file', 'ascii', 'newline', 'python', 'performance', 'search', 'profiling', 'large-files', 'python', 'sql-server', 'sql-server-2008', 'python-2.7', 'pymssql', 'html', 'sublimetext2', 'sublimetext', 'indentation', 'reformat', 'mongodb', 'mongodb-query', 'aggregation-framework', 'spring-data-mongodb', 'full-text-indexing', 'python', 'excel', 'com', 'pywin32', 'win32com', 'python', 'loops', 'if-statement', 'break', 'conditional-operator', 'java', 'python', 'jython', 'pip', 'easy-install', 'python', 'unit-testing', 'testing', 'mocking', 'patch', 'python', 'loops', 'for-loop', 'infinite-loop', 'infinite', 'python-2.7', 'exception', 'python-3.x', 'traceback', 'raise', 'python', 'debugging', 'multiprocessing', 'pycharm', 'winpdb', 'python', 'packaging', 'remote-access', 'download', 'software-update', 'python', 'constructor', 'destructor', 'with-statement', 'contextmanager']\n",
      "['python', 'text', 'python-3.x', 'python-2.7', 'html', 'sublimetext2', 'c#', '.net', 'pdf', 'sublimetext'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemple = [\"your\", 'text', 'document', 'python']\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "predicted_tags = predict_tags_using_knn(train, 'title_nltk', 'all_tags', exemple=exemple)\n",
    "print(predicted_tags, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['find', 'class', 'com', 'google', 'firebase', 'provider']\n",
      "['java', 'spring', 'rest', 'gradle', 'spring-boot', 'java', 'android', 'firebase', 'gradle', 'android-gradle-plugin', 'android', 'android-studio', 'firebase', 'android-gradle-plugin', 'google-play-services', 'android', 'android-intent', 'arraylist', 'unmarshalling', 'parcelable', 'php', 'class', 'laravel', 'alias', 'autoloader', 'php', 'sql', 'laravel', 'laravel-5', 'laravel-artisan', 'java', 'spring', 'spring-boot', 'spring-security', 'spring-security-oauth2', 'java', 'maven', 'maven-2', 'maven-3', 'protocol-buffers', 'android', 'google-maps', 'dictionary', 'android-mapview', 'inflate', 'android', 'android-studio', 'flutter', 'sdk', 'android-sdk-manager', 'java', 'spring', 'junit', 'spring-boot', 'spring-data', 'php', 'laravel', 'https', 'laravel-valet', 'valet', 'spring', 'maven', 'spring-mvc', 'spring-boot', 'spring-profiles', 'json', 'angular', 'typescript', 'jwt', 'guard', 'c#', '.net', 'visual-studio-2012', 'compression', 'zip', 'c#', 'asp.net', 'google-api', 'google-oauth', 'google-api-dotnet-client', 'ios', 'ipad', 'frameworks', 'header', 'gpuimage', 'android', 'firebase', 'gradle', 'android-gradle-plugin', 'crashlytics', 'java', 'jar', 'maven-2', 'manifest', 'program-entry-point', 'gcc', 'ubuntu', 'linker', 'shared-libraries', 'ld', 'java', 'interface', 'go', 'static-methods', 'abstract', 'javascript', 'node.js', 'typescript', 'nestjs', 'class-validator', 'c#', 'android', 'firebase', 'unity-game-engine', 'firebase-authentication', 'c#', '.net', 'visual-studio', 'json.net', 'nuget', 'android', 'firebase', 'android-studio', 'android-gradle-plugin', 'jcenter', 'iphone', 'objective-c', 'ios', 'xcode4.3', 'cocoapods', 'javascript', 'node.js', 'mongodb', 'mongoose', 'mongodb-query', 'database', 'security', 'firebase', 'firebase-authentication', 'firebase-security', 'php', 'laravel', 'amazon-web-services', 'laravel-5', 'laravelcollective', 'c#', '.net', 'web-config', 'class-library', 'configurationmanager', 'android', 'unity-game-engine', 'abi', 'arcore', 'unsatisfiedlinkerror', 'javascript', 'android', 'reactjs', 'react-native', 'gradle', 'python', 'tensorflow', 'keras', 'transfer-learning', 'vgg-net', 'angularjs', 'routes', 'angular-ui-router', 'single-page-application', 'angularjs-routing', 'java', 'linux', 'ubuntu', 'netbeans', 'command-line', 'firebase', 'flutter', 'dart', 'firebase-authentication', 'google-authentication', 'java', 'android', 'android-studio', 'gradle', 'build', 'c++', 'ubuntu', 'sdl', 'sdl-2', 'sdl-image', 'google-maps', 'angular', 'typescript', 'angular-cli', 'angular2-google-maps', 'python', 'class', 'inheritance', 'python-3.x', 'language-design', 'java', 'android', 'design-patterns', 'inheritance', 'parcelable', 'javascript', 'node.js', 'angular', 'typescript', 'angular7', 'c++', 'gcc', 'vtable', 'virtual-inheritance', 'vtt', 'javascript', 'firebase', 'google-drive-api', 'firebase-authentication', 'adobe-indesign', 'ios', 'objective-c', 'xcode', 'storyboard', 'xib', 'java', 'maven', 'build', 'interop', 'kotlin', 'c++', 'c++11', 'bit-fields', 'bitmask', 'enum-class', 'java', 'json', 'rest', 'jersey', 'jax-rs', 'android', 'firebase', 'flutter', 'google-play-services', 'flutter-dependencies', 'android', 'android-intent', 'classnotfoundexception', 'unmarshalling', 'parcel']\n",
      "['android', 'java', 'firebase', 'gradle', 'c#', 'javascript', 'spring', 'spring-boot', 'android-gradle-plugin', 'android-studio'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemple1 = test['title_nltk'][0]\n",
    "print(exemple1)\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "predicted_tags1 = predict_tags_using_knn(train, 'title_nltk', 'all_tags', exemple=exemple1)\n",
    "print(predicted_tags1, '\\n')\n",
    "\n",
    "# firebase peut etre predit\n",
    "# grand succes !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'lang', 'noclassdeffounderror', 'scala', 'run', 'code']\n",
      "['c#', '.net', 'wpf', 'code-behind', 'itemspanel', 'java', 'gradle', 'spring-boot', 'jar', 'build.gradle', 'reactjs', 'authentication', 'google-authentication', 'google-api-js-client', 'googleauthr', 'c', 'gcc', 'types', 'openmp', 'typeof', 'php', 'mysql', 'laravel', 'ubuntu', 'server', 'iphone', 'ios', 'xamarin.ios', 'http-response-codes', 'nsurlconnectiondelegate', 'c++', 'python', 'ctypes', 'cython', 'boost-python', 'scala', 'maven', 'apache-spark', 'noclassdeffounderror', 'spark-streaming', 'macos', 'shell', 'scala', 'terminal', 'installation', 'javascript', 'vue.js', 'visual-studio-code', 'nuxt.js', 'prettier', 'c#', 'multithreading', 'winforms', 'backgroundworker', 'infinite-loop', 'asp.net-core', 'oauth', 'identityserver4', 'openid-connect', 'asp.net-core-3.0', 'c++', 'c', 'cuda', 'parallel-processing', 'gpu', 'javascript', 'node.js', 'express', 'firebase', 'firebase-realtime-database', 'python', 'selenium', 'http', 'selenium-webdriver', 'ui-automation', 'c++', 'c', 'types', 'int64', 'long-long', 'javascript', 'angularjs', 'promise', 'deferred', 'finally', 'javascript', 'typescript', 'visual-studio-code', 'vscode-extensions', 'file-properties', 'c#', 'sql-server', 'asp.net-mvc', 'entity-framework', 'asp.net-mvc-4', 'c', 'assembly', 'x86', 'x86-64', 'shellcode', 'c', 'assembly', 'x86', 'reverse-engineering', 'decompiling', 'unix', 'ubuntu', 'command-line', 'go', 'terminal', 'c', 'linux', 'security', 'exploit', 'secure-coding', 'javascript', 'firebase', 'react-native', 'mocking', 'jestjs', 'stack-trace', 'glibc', 'sigabrt', 'segmentation-fault', 'backtrace', 'java', 'rest', 'exception', 'jersey', 'provider', 'javascript', 'python', 'google-chrome', 'sandbox', 'brython', 'java', 'selenium', 'testing', 'selenium-webdriver', 'selenium-chromedriver', 'python', 'python-2.7', 'stdout', 'stderr', 'os.system', 'java', 'python', 'apache-spark', 'directed-acyclic-graphs', 'airflow', 'scala', 'random', 'collections', 'set', 'scala-collections', 'flash', 'apache-flex', 'debugging', 'air', 'flash-builder', 'javascript', 'html', 'ajax', 'url', 'xmlhttprequest', 'iphone', 'xcode', 'ios-simulator', 'ios5', 'freeze', 'android', 'google-maps', 'google-maps-android-api-2', 'latitude-longitude', 'google-places-api', 'c++', 'performance', 'assembly', 'optimization', 'x86', 'git', 'jenkins', 'groovy', 'jenkins-plugins', 'jenkins-pipeline', 'linux', 'bash', 'shell', 'terminal', 'paste', 'android', 'android-emulator', 'android-service', 'monitoring', 'monitor', 'android', 'enums', 'android-custom-view', 'attr', 'custom-view', 'android', 'retrofit', 'rx-java', 'android-networking', 'rx-android', 'java', 'jvm', 'compatibility', 'java-7', 'java-8', 'php', 'templates', 'model-view-controller', 'file-io', 'eval', 'c#', 'android', 'xamarin', 'xamarin.android', 'android-runonuithread', 'python', 'unicode', 'encoding', 'utf-8', 'character-codes', 'javascript', 'vue.js', 'model-view-controller', 'vuejs2', 'vuejs3', 'ios', 'swift', 'ios-simulator', 'xcode6', 'xcode6-beta6', 'linux', 'bash', 'shell', 'unix', 'sudo', 'python', 'pandas', 'visual-studio-code', 'jupyter-notebook', 'tqdm', 'java', 'file', 'jar', 'io', 'extract']\n",
      "['javascript', 'python', 'java', 'c', 'android', 'c#', 'c++', 'scala', 'shell', 'terminal'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemple2 = test['title_nltk'][1]\n",
    "print(exemple2)\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "predicted_tags2 = predict_tags_using_knn(train, 'title_nltk', 'all_tags', exemple=exemple2)\n",
    "print(predicted_tags2, '\\n')\n",
    "\n",
    "# scale ok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision_topics(real_tags:list, predicted_tags:list):\n",
    "    # precision = TP / (TP + FP)\n",
    "    tp = 0\n",
    "    for predicted_tag in predicted_tags:\n",
    "        if predicted_tag in real_tags:\n",
    "            tp += 1\n",
    "\n",
    "    fp = len(predicted_tags) - tp\n",
    "    precision = tp/(tp + fp)\n",
    "    # <=> precision = tp/len(predicted_tags)\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "print(precision_topics(exemple1, predicted_tags1))\n",
    "precision_topics(exemple2, predicted_tags2)\n",
    "\n",
    "# ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score(y_true, y_pred):\n",
    "    precision = 0\n",
    "    for i in range(0, len(y_pred)):\n",
    "        precision += precision_topics(y_true[i], y_pred[i])\n",
    "    precision_moyenne = precision / len(y_pred)\n",
    "\n",
    "    return precision_moyenne\n",
    "\n",
    "custom_precision_scorer = make_scorer(precision_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 0 : ['find', 'class', 'com', 'google', 'firebase', 'provider']\n",
      "real tags : ['java', 'android', 'android-studio', 'error-handling', 'compiler-errors']\n",
      "predicted : ['android', 'java', 'firebase', 'gradle', 'c#', 'javascript', 'spring', 'spring-boot', 'android-gradle-plugin', 'android-studio']\n",
      "0.3 \n",
      "\n",
      "doc 1 : ['get', 'lang', 'noclassdeffounderror', 'scala', 'run', 'code']\n",
      "real tags : ['java', 'scala', 'maven', 'noclassdeffounderror', 'scala-ide']\n",
      "predicted : ['javascript', 'python', 'java', 'c', 'android', 'c#', 'c++', 'scala', 'shell', 'terminal']\n",
      "0.2 \n",
      "\n",
      "doc 2 : ['django', 'bulk', 'create', 'ignore', 'duplicate']\n",
      "real tags : ['python', 'mysql', 'django', 'bulkinsert', 'bulk']\n",
      "predicted : ['django', 'python', 'ios', 'xcode', 'django-models', 'swift', 'javascript', 'mongodb', 'objective-c', 'c#']\n",
      "0.2 \n",
      "\n",
      "precision moyenne = 0.2333333333333333\n"
     ]
    }
   ],
   "source": [
    "# add grid search cv\n",
    "# add score\n",
    "\n",
    "def predict_tags_using_knn(train_df=train, feature='title_nltk', target='all_tags', test_df=test, k=50):\n",
    "    documents = train_df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    dense_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Ici on ne va pas demander au knn de faire de prediction,\n",
    "    # On veut juste qu'il trouve les voisins.\n",
    "    # Mais la fonction fit a besoin de targets en param\n",
    "    target_values = train_df[target].values\n",
    "\n",
    "    # Initialize kNN model\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=k, metric='cosine', algorithm='brute')\n",
    "    knn_model.fit(dense_matrix, target_values)\n",
    "\n",
    "    # Predictions completes en... un peu moins de 24h d'apres mes estimations\n",
    "    # optimser avec pandarallel ?\n",
    "    # use sample\n",
    "    predictions=[]\n",
    "    for i in range(0, 3):\n",
    "        query_document = test_df[feature][i]\n",
    "        print(f'doc {i} : {query_document}')\n",
    "        print(f'real tags : {test[target][i]}')\n",
    "        query_bow = gensim_dictionary.doc2bow(query_document)\n",
    "        query_vector = corpus2dense([query_bow], num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "        # Find nearest neighbors\n",
    "        _, indices = knn_model.kneighbors(query_vector)\n",
    "\n",
    "        # Aggregate tags from neighbors\n",
    "        neighbor_tags = [tag for i in indices.flatten() for tag in train_df.iloc[i][target]]\n",
    "\n",
    "        # print(neighbor_tags)\n",
    "\n",
    "        # Predict tags based on most common tags among neighbors\n",
    "        predicted_tags = [tag for tag, _ in Counter(neighbor_tags).most_common(n=10)]\n",
    "        # 5 tags/question en moyenne mais on peut suggérer +\n",
    "        print(f'predicted : {predicted_tags}')\n",
    "        print(precision_topics(test_df[target][i], predicted_tags), '\\n')\n",
    "\n",
    "        predictions.append(predicted_tags)\n",
    "\n",
    "    precision = precision_score(test_df[target], predictions)\n",
    "    print(f'precision moyenne = {precision}')\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "predictions = predict_tags_using_knn()\n",
    "\n",
    "# 0.2 de precision ?? c enorme !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_topics(real_tags: list, predicted_tags: list):\n",
    "    # recall = TP / (TP + FN)\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    for real_tag in real_tags:\n",
    "        if real_tag in predicted_tags:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    return recall\n",
    "\n",
    "def score_tag_f1(real_tags: list, predicted_tags: list):\n",
    "    precision = precision_score(real_tags, predicted_tags)\n",
    "    recall = recall_topics(real_tags, predicted_tags)\n",
    "\n",
    "    # F1 score = 2 * (precision * recall) / (precision + recall)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return f1\n",
    "\n",
    "def score_tag_accuracy(real_tags: list, predicted_tags: list):\n",
    "    # accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    tp = sum(1 for tag in predicted_tags if tag in real_tags)\n",
    "    fp = sum(1 for tag in predicted_tags if tag not in real_tags)\n",
    "    fn = sum(1 for tag in real_tags if tag not in predicted_tags)\n",
    "\n",
    "    accuracy = (tp + fn) / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# On va utiliser f1 optimiser k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n"
     ]
    }
   ],
   "source": [
    "# pourquoi on ne peut pas utiliser le score precision sckikit ici :\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Assuming y_true is the ground truth (real tags) and y_pred is the predicted tags\n",
    "precision = precision_score(['ok', 'ko'], ['ko', 'ok'], average='micro')  # You can use 'micro', 'macro', or 'weighted' depending on your use case\n",
    "print(f'Precision: {precision}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Assuming y_true is the ground truth (real tags) and y_pred is the predicted tags\n",
    "precision = precision_score([1, 0], [0, 1], average='micro')  # You can use 'micro', 'macro', or 'weighted' depending on your use case\n",
    "\n",
    "print(f'Precision: {precision}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### opti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 266, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 353, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 86, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/utils/_response.py\", line 109, in _get_response_values\n",
      "    y_pred, pos_label = estimator.predict(X), None\n",
      "                        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/pipeline.py\", line 508, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/neighbors/_regression.py\", line 249, in predict\n",
      "    y_pred = np.mean(_y[neigh_ind], axis=1)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<__array_function__ internals>\", line 180, in mean\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 3432, in mean\n",
      "    return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/numpy/core/_methods.py\", line 182, in _mean\n",
      "    ret = um.true_divide(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "TypeError: unsupported operand type(s) for /: 'list' and 'int'\n",
      "\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 266, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 353, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 86, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/utils/_response.py\", line 109, in _get_response_values\n",
      "    y_pred, pos_label = estimator.predict(X), None\n",
      "                        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/pipeline.py\", line 508, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/neighbors/_regression.py\", line 249, in predict\n",
      "    y_pred = np.mean(_y[neigh_ind], axis=1)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<__array_function__ internals>\", line 180, in mean\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 3432, in mean\n",
      "    return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/numpy/core/_methods.py\", line 182, in _mean\n",
      "    ret = um.true_divide(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "TypeError: unsupported operand type(s) for /: 'list' and 'int'\n",
      "\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 266, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 353, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 86, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/utils/_response.py\", line 109, in _get_response_values\n",
      "    y_pred, pos_label = estimator.predict(X), None\n",
      "                        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/pipeline.py\", line 508, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/neighbors/_regression.py\", line 249, in predict\n",
      "    y_pred = np.mean(_y[neigh_ind], axis=1)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<__array_function__ internals>\", line 180, in mean\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 3432, in mean\n",
      "    return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/numpy/core/_methods.py\", line 182, in _mean\n",
      "    ret = um.true_divide(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "TypeError: unsupported operand type(s) for /: 'list' and 'int'\n",
      "\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 266, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 353, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 86, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/utils/_response.py\", line 109, in _get_response_values\n",
      "    y_pred, pos_label = estimator.predict(X), None\n",
      "                        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/pipeline.py\", line 508, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/neighbors/_regression.py\", line 249, in predict\n",
      "    y_pred = np.mean(_y[neigh_ind], axis=1)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<__array_function__ internals>\", line 180, in mean\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 3432, in mean\n",
      "    return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/numpy/core/_methods.py\", line 182, in _mean\n",
      "    ret = um.true_divide(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "TypeError: unsupported operand type(s) for /: 'list' and 'int'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 97\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross-Validation Scores:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m     pprint(cv_scores)\n\u001b[0;32m---> 97\u001b[0m pipe_knn()\n",
      "Cell \u001b[0;32mIn[74], line 73\u001b[0m, in \u001b[0;36mpipe_knn\u001b[0;34m(df, feature, target, embedding, metric, graph)\u001b[0m\n\u001b[1;32m     69\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipe, param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m     70\u001b[0m                         scoring\u001b[38;5;241m=\u001b[39mmake_scorer(score_tag_f1), cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# add, refit='f1' for multiple scoring\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Fit the GridSearchCV object to your training data to perform hyperparameter tuning\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(dense_matrix, target_values)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Access the best hyperparameters\u001b[39;00m\n\u001b[1;32m     76\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    847\u001b[0m         clone(base_estimator),\n\u001b[1;32m    848\u001b[0m         X,\n\u001b[1;32m    849\u001b[0m         y,\n\u001b[1;32m    850\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    851\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    852\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    853\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    854\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    856\u001b[0m     )\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    859\u001b[0m     )\n\u001b[1;32m    860\u001b[0m )\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:754\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    751\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    753\u001b[0m fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m--> 754\u001b[0m test_scores \u001b[38;5;241m=\u001b[39m _score(estimator, X_test, y_test, scorer, error_score)\n\u001b[1;32m    755\u001b[0m score_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m-\u001b[39m fit_time\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_train_score:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:813\u001b[0m, in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[1;32m    811\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scorer(estimator, X_test)\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 813\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scorer(estimator, X_test, y_test)\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer, _MultimetricScorer):\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;66;03m# If `_MultimetricScorer` raises exception, the `error_score`\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;66;03m# parameter is equal to \"raise\".\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:266\u001b[0m, in \u001b[0;36m_BaseScorer.__call__\u001b[0;34m(self, estimator, X, y_true, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     _kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[0;32m--> 266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score(partial(_cached_call, \u001b[38;5;28;01mNone\u001b[39;00m), estimator, X, y_true, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:353\u001b[0m, in \u001b[0;36m_PredictScorer._score\u001b[0;34m(self, method_caller, estimator, X, y_true, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate predicted target values for X relative to y_true.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    Score function applied to prediction of estimator on X.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_overlap(\n\u001b[1;32m    346\u001b[0m     message\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is an overlap between set kwargs of this scorer instance and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m    352\u001b[0m )\n\u001b[0;32m--> 353\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m method_caller(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m, X)\n\u001b[1;32m    354\u001b[0m scoring_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sign \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score_func(y_true, y_pred, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscoring_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:86\u001b[0m, in \u001b[0;36m_cached_call\u001b[0;34m(cache, estimator, response_method, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m response_method \u001b[38;5;129;01min\u001b[39;00m cache:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cache[response_method]\n\u001b[0;32m---> 86\u001b[0m result, _ \u001b[38;5;241m=\u001b[39m _get_response_values(\n\u001b[1;32m     87\u001b[0m     estimator, \u001b[38;5;241m*\u001b[39margs, response_method\u001b[38;5;241m=\u001b[39mresponse_method, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     88\u001b[0m )\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     cache[response_method] \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/utils/_response.py:109\u001b[0m, in \u001b[0;36m_get_response_values\u001b[0;34m(estimator, X, response_method, pos_label)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response_method \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should either be a classifier to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused with response_method=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or the response_method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got a regressor with response_method=\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         )\n\u001b[0;32m--> 109\u001b[0m     y_pred, pos_label \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mpredict(X), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_pred, pos_label\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/pipeline.py:508\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    507\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt)\n\u001b[0;32m--> 508\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/neighbors/_regression.py:237\u001b[0m, in \u001b[0;36mKNeighborsRegressor.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict the target for the provided data.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m    Target values.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkneighbors(X, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    238\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/neighbors/_base.py:859\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    857\u001b[0m         kwds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_params_\n\u001b[0;32m--> 859\u001b[0m     chunked_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    860\u001b[0m         pairwise_distances_chunked(\n\u001b[1;32m    861\u001b[0m             X,\n\u001b[1;32m    862\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X,\n\u001b[1;32m    863\u001b[0m             reduce_func\u001b[38;5;241m=\u001b[39mreduce_func,\n\u001b[1;32m    864\u001b[0m             metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_,\n\u001b[1;32m    865\u001b[0m             n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    866\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[1;32m    867\u001b[0m         )\n\u001b[1;32m    868\u001b[0m     )\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mball_tree\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkd_tree\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:2017\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2016\u001b[0m     X_chunk \u001b[38;5;241m=\u001b[39m X[sl]\n\u001b[0;32m-> 2017\u001b[0m D_chunk \u001b[38;5;241m=\u001b[39m pairwise_distances(X_chunk, Y, metric\u001b[38;5;241m=\u001b[39mmetric, n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (X \u001b[38;5;129;01mis\u001b[39;00m Y \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   2019\u001b[0m     metric, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2020\u001b[0m ) \u001b[38;5;129;01mis\u001b[39;00m euclidean_distances:\n\u001b[1;32m   2021\u001b[0m     \u001b[38;5;66;03m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[1;32m   2022\u001b[0m     \u001b[38;5;66;03m# i.e. \"l2\"\u001b[39;00m\n\u001b[1;32m   2023\u001b[0m     D_chunk\u001b[38;5;241m.\u001b[39mflat[sl\u001b[38;5;241m.\u001b[39mstart :: _num_samples(X) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:2195\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   2192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m distance\u001b[38;5;241m.\u001b[39msquareform(distance\u001b[38;5;241m.\u001b[39mpdist(X, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[1;32m   2193\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(distance\u001b[38;5;241m.\u001b[39mcdist, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m-> 2195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:1765\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1762\u001b[0m X, Y, dtype \u001b[38;5;241m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[1;32m   1764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(X, Y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[1;32m   1768\u001b[0m fd \u001b[38;5;241m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:1105\u001b[0m, in \u001b[0;36mcosine_distances\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine distance between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \n\u001b[1;32m   1081\u001b[0m \u001b[38;5;124;03mCosine distance is defined as 1.0 minus the cosine similarity.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;124;03mscipy.spatial.distance.cosine : Dense matrices only.\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;66;03m# 1.0 - cosine_similarity(X, Y) without copy\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m S \u001b[38;5;241m=\u001b[39m cosine_similarity(X, Y)\n\u001b[1;32m   1106\u001b[0m S \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1107\u001b[0m S \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    186\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:1585\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1583\u001b[0m     Y_normalized \u001b[38;5;241m=\u001b[39m normalize(Y, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1585\u001b[0m K \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X_normalized, Y_normalized\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39mdense_output)\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m K\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/utils/extmath.py:196\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     ret \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m b\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 196\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m ):\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/scipy/sparse/_base.py:1461\u001b[0m, in \u001b[0;36missparse\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m sparray\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m _spbase\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[0;32m-> 1461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21missparse\u001b[39m(x):\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Is `x` of a sparse array type?\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m \n\u001b[1;32m   1464\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _spbase)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the plotting function\n",
    "def plot_performance_vs_neighbors(grid_search):\n",
    "    # Extract the results from the GridSearchCV object\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Extract the parameters and scores for both uniform and distance weights\n",
    "    params_uniform = [param for param in results['params'][::2]]\n",
    "    params_distance = [param for param in results['params'][1::2]]\n",
    "    test_scores_uniform = results['mean_test_r2'][::2]\n",
    "    test_scores_distance = results['mean_test_r2'][1::2]\n",
    "\n",
    "    # Extract the parameter values for uniform and distance weights\n",
    "    n_neighbors_uniform = [param['knn_regressor__n_neighbors'] for param in params_uniform]\n",
    "    n_neighbors_distance = [param['knn_regressor__n_neighbors'] for param in params_distance]\n",
    "\n",
    "    # Create separate plots for uniform and distance weights\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot for uniform weight\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(n_neighbors_uniform, test_scores_uniform, marker='o', linestyle='-')\n",
    "    plt.title(\"Performance vs. Number of Neighbors (Uniform Weight)\")\n",
    "    plt.xlabel(\"Number of Neighbors\")\n",
    "    plt.ylabel(\"Mean Test R-squared\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot for distance weight\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(n_neighbors_distance, test_scores_distance, marker='o', linestyle='-')\n",
    "    plt.title(\"Performance vs. Number of Neighbors (Distance Weight)\")\n",
    "    plt.xlabel(\"Number of Neighbors\")\n",
    "    plt.ylabel(\"Mean Test R-squared\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pipe_knn(df=train, feature='title_nltk', target='all_tags', embedding='bow_dense', metric='cosine', graph=True):\n",
    "\n",
    "    documents = df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    dense_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Ici on ne va pas demander au knn de faire de prediction,\n",
    "    # On veut juste qu'il trouve les voisins.\n",
    "    # Mais la fonction fit a besoin de targets en param\n",
    "    target_values = df[target].values\n",
    "\n",
    "    # Create a KNN Regressor\n",
    "    knn_regressor = KNeighborsRegressor(metric=metric)\n",
    "\n",
    "    # Create a pipeline with preprocessing and a knn regressor, to simplify gridsearch\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"knn_regressor\", knn_regressor)\n",
    "    ])\n",
    "\n",
    "    # Define hyperparameters and their possible values for grid search\n",
    "    param_grid = {\n",
    "        'knn_regressor__n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "        'knn_regressor__weights': ['uniform', 'distance']\n",
    "    }\n",
    "\n",
    "    # Create the GridSearchCV object with multiple scoring metrics\n",
    "    # scoring = {'neg_mean_squared_error': 'neg_mean_squared_error', 'r2': 'r2'}\n",
    "    grid_search = GridSearchCV(pipe, param_grid=param_grid,\n",
    "                            scoring=make_scorer(score_tag_f1), cv=5, verbose=1) # add, refit='f1' for multiple scoring\n",
    "\n",
    "    # Fit the GridSearchCV object to your training data to perform hyperparameter tuning\n",
    "    grid_search.fit(dense_matrix, target_values)\n",
    "\n",
    "    # Access the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "    # Create the KNN regressor with the best hyperparameters\n",
    "    best_knn_regressor = KNeighborsRegressor(metric=metric,\n",
    "                                             n_neighbors=best_params['knn_regressor__n_neighbors'],\n",
    "                                             weights=best_params['knn_regressor__weights'])\n",
    "\n",
    "    # Create a pipeline with the preprocessor and the tuned knn regressor\n",
    "    pipeline_with_tuned_knn = Pipeline(steps=[\n",
    "        (\"knn_regressor\", best_knn_regressor)  # Use the tuned neighbor and weight values here\n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation (on training set) and display the scores for each split\n",
    "    # scoring = ['r2', 'neg_mean_squared_error']\n",
    "    cv_scores = cross_validate(pipeline_with_tuned_knn, dense_matrix, target_values, cv=5, scoring=make_scorer(score_tag_f1))\n",
    "    # print(\"Cross-Validation Scores (training):\", '\\n', cv_scores)\n",
    "    print(\"Cross-Validation Scores:\")\n",
    "    pprint(cv_scores)\n",
    "\n",
    "\n",
    "pipe_knn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for i, score in enumerate(cv_scores['test_r2']):\n",
    "        print(f\"Split {i+1} : r2 = {score}\")\n",
    "\n",
    "    r2_val = cv_scores['test_r2'].mean()\n",
    "    mse_val = -cv_scores['test_neg_mean_squared_error'].mean()\n",
    "    rmse_val = np.sqrt(mse_val)\n",
    "\n",
    "    # fit model on training set\n",
    "    pipeline_with_tuned_knn.fit(dense_matrix, target_values)\n",
    "    # Make no predictions here\n",
    "\n",
    "    # Calculate scores on training\n",
    "    f1 = score_tag_f1(train['all_tags'], pipeline_with_tuned_knn.predict(dense_matrix))\n",
    "    # and testing set\n",
    "    r2_test, rmse_test = score_tag_f1(y_test, y_pred)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"R-squared (val) =  {r2_val}\")\n",
    "    print(f\"R-squared (train) =  {r2_train}\")\n",
    "    print(f\"R-squared (test) =  {r2_test}\")\n",
    "    print(f\"RMSE (val) =  {rmse_val}\")\n",
    "    print(f\"RMSE (train) =  {rmse_train}\")\n",
    "    print(f\"RMSE (test) =  {rmse_test}\" '\\n')\n",
    "\n",
    "    # display results/error as graph on first iteration (if asked to)\n",
    "    if alea == 0 and graph:\n",
    "        plot_performance_vs_neighbors(grid_search)\n",
    "        plot_predictions(r2_train, r2_test, y_pred, y_test, kind='actual_vs_predicted', y=y)\n",
    "        # plot_predictions(r2_train, r2_test, y_pred, y_test, kind='residual_vs_predicted', y=y)\n",
    "\n",
    "    # Return scores for this random state\n",
    "    return r2_val, rmse_val, r2_train, rmse_train, r2_test, rmse_test, time_fit, time_predict\n",
    "\n",
    "\n",
    "def test_knn_n_times(y=y_E, scaler=robust, graph=False, metric='euclidean'):\n",
    "    print(f'Modèle : kNN')\n",
    "    print('target : ', y.name)\n",
    "\n",
    "    results_r2_val, results_rmse_val,  results_r2_train, results_rmse_train = [], [], [], []\n",
    "    results_r2_test, results_rmse_test, results_time_fit, results_time_predict = [], [], [], []\n",
    "\n",
    "    for n in range(nb_iter):\n",
    "        print('Iteration ', n+1)\n",
    "        r2_val, rmse_val, r2_train, rmse_train, r2_test, rmse_test, time_fit, time_predict = pipe_knn(alea=n,\n",
    "                                                                                      y=y,\n",
    "                                                                                      scaler=scaler,\n",
    "                                                                                      graph=graph,\n",
    "                                                                                      metric=metric)\n",
    "        results_r2_val.append(r2_val)\n",
    "        results_rmse_val.append(rmse_val)\n",
    "        results_r2_train.append(r2_train)\n",
    "        results_rmse_train.append(rmse_train)\n",
    "        results_r2_test.append(r2_test)\n",
    "        results_rmse_test.append(rmse_test)\n",
    "        results_time_fit.append(time_fit)\n",
    "        results_time_predict.append(time_predict)\n",
    "\n",
    "    # Calculate means and std devs\n",
    "    r2_val_moy = np.mean(results_r2_val)\n",
    "    rmse_val_moy = np.mean(results_rmse_val)\n",
    "    r2_train_moy = np.mean(results_r2_train)\n",
    "    rmse_train_moy = np.mean(results_rmse_train)\n",
    "    r2_test_moy = np.mean(results_r2_test)\n",
    "    rmse_test_moy = np.mean(results_rmse_test)\n",
    "    time_fit_moy = np.mean(results_time_fit)\n",
    "    time_predict_moy = np.mean(results_time_predict)\n",
    "\n",
    "    r2_val_std = np.std(results_r2_val)\n",
    "    rmse_val_std = np.std(results_rmse_val)\n",
    "    r2_train_std = np.std(results_r2_train)\n",
    "    rmse_train_std = np.std(results_rmse_train)\n",
    "    r2_test_std = np.std(results_r2_test)\n",
    "    rmse_test_std = np.std(results_rmse_test)\n",
    "    time_fit_std = np.std(results_time_fit)\n",
    "    time_predict_std = np.std(results_time_predict)\n",
    "\n",
    "    # Mise en forme\n",
    "    results = {'model': 'kNN',\n",
    "               'set': dataset,\n",
    "               'scaler': scaler,\n",
    "               'target': y.name,\n",
    "               'r2_test_moy': r2_test_moy,\n",
    "               'r2_test_std': r2_test_std,\n",
    "               'rmse_test_moy': rmse_test_moy,\n",
    "               'rmse_test_std': rmse_test_std,\n",
    "               'r2_train_moy': r2_train_moy,\n",
    "               'r2_train_std': r2_train_std,\n",
    "               'rmse_train_moy': rmse_train_moy,\n",
    "               'rmse_train_std': rmse_train_std,\n",
    "               'r2_val_moy': r2_val_moy,\n",
    "               'r2_val_std': r2_val_std,\n",
    "               'rmse_val_moy': rmse_val_moy,\n",
    "               'rmse_val_std': rmse_val_std,\n",
    "               'time_fit_moy': time_fit_moy,\n",
    "               'time_fit_std': time_fit_std,\n",
    "               'time_predict_moy': time_predict_moy,\n",
    "               'time_predict_std': time_predict_std,\n",
    "               }\n",
    "\n",
    "    print(results, '\\n')\n",
    "\n",
    "    # Append a new row for this model\n",
    "    model_results.append(results)\n",
    "\n",
    "test_knn_n_times(scaler=robust, graph=True)\n",
    "# test_knn_n_times(y=y_EI, scaler=robust, graph=True)\n",
    "\n",
    "affichage_results()\n",
    "\n",
    "# 0.4, ce qui est bien, mais pas top.\n",
    "# dataset trop petit pour un knn ? (relativement peu d'individus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def suggest_topics_using_knn(df, feature, alea=42):\n",
    "    documents = df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Set training parameters.\n",
    "    num_topics = 10\n",
    "    chunksize = 2000\n",
    "    passes = 20\n",
    "    iterations = 400\n",
    "    eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "    # Make a index to word dictionary.\n",
    "    temp = gensim_dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = gensim_dictionary.id2token\n",
    "\n",
    "    model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every, random_state=alea)\n",
    "\n",
    "    top_topics = model.top_topics(corpus, topn=20)\n",
    "\n",
    "    # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "    # = umass if same topn (default 20)\n",
    "    avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "    print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "    # Compute Coherence Score (Umass)\n",
    "    coherence_umass = CoherenceModel(model=model, texts=documents, dictionary=gensim_dictionary, coherence='u_mass')\n",
    "    coherence_lda_umass = coherence_umass.get_coherence()\n",
    "    print('u_mass Coherence Score: %.4f.' % coherence_lda_umass)\n",
    "\n",
    "    # Compute Coherence Score (cv)\n",
    "    coherence_cv = CoherenceModel(model=model, texts=documents, dictionary=gensim_dictionary, coherence='c_v')\n",
    "    coherence_lda_cv = coherence_cv.get_coherence()\n",
    "    print('c_v Coherence Score: %.4f.' % coherence_lda_cv)\n",
    "\n",
    "    # Compute Coherence Score (npmi)\n",
    "    coherence_npmi = CoherenceModel(model=model, texts=documents, dictionary=gensim_dictionary, coherence='c_npmi')\n",
    "    coherence_lda_npmi = coherence_npmi.get_coherence()\n",
    "    print('c_npmi Coherence Score: %.4f.' % coherence_lda_npmi)\n",
    "\n",
    "    # Perplexity is not a coherence score but a measure of how well the model predicts a sample.\n",
    "    # A lower perplexity indicates better model performance.\n",
    "    perplexity = model.log_perplexity(corpus)\n",
    "    print('Perplexity: %.4f.' % perplexity)\n",
    "\n",
    "    # Visualize the topics\n",
    "    vis_data = gensimvis.prepare(model, corpus, gensim_dictionary)\n",
    "    display(pyLDAvis.display(vis_data))\n",
    "\n",
    "    # Uncomment the next line if you want to save the plot to a file\n",
    "    # pyLDAvis.save_html(vis_data, 'artifacts/lda_vis.html')\n",
    "\n",
    "    pprint(top_topics)\n",
    "    # to print all topics\n",
    "    # pprint(model.print_topics())\n",
    "\n",
    "    return model, corpus, gensim_dictionary\n",
    "\n",
    "lda_test, corpus_test, dict_test = suggest_topics_using_LDA(train, 'title_nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plotting function\n",
    "def plot_performance_vs_neighbors(grid_search):\n",
    "    # Extract the results from the GridSearchCV object\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Extract the parameters and scores for both uniform and distance weights\n",
    "    params_uniform = [param for param in results['params'][::2]]\n",
    "    params_distance = [param for param in results['params'][1::2]]\n",
    "    test_scores_uniform = results['mean_test_r2'][::2]\n",
    "    test_scores_distance = results['mean_test_r2'][1::2]\n",
    "\n",
    "    # Extract the parameter values for uniform and distance weights\n",
    "    n_neighbors_uniform = [param['knn_regressor__n_neighbors'] for param in params_uniform]\n",
    "    n_neighbors_distance = [param['knn_regressor__n_neighbors'] for param in params_distance]\n",
    "\n",
    "    # Create separate plots for uniform and distance weights\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot for uniform weight\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(n_neighbors_uniform, test_scores_uniform, marker='o', linestyle='-')\n",
    "    plt.title(\"Performance vs. Number of Neighbors (Uniform Weight)\")\n",
    "    plt.xlabel(\"Number of Neighbors\")\n",
    "    plt.ylabel(\"Mean Test R-squared\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot for distance weight\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(n_neighbors_distance, test_scores_distance, marker='o', linestyle='-')\n",
    "    plt.title(\"Performance vs. Number of Neighbors (Distance Weight)\")\n",
    "    plt.xlabel(\"Number of Neighbors\")\n",
    "    plt.ylabel(\"Mean Test R-squared\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pipe_knn(alea, y, scaler, graph, metric):\n",
    "\n",
    "    # Séparation des jeux de données entrainement / validation, preprocessing\n",
    "    X_train, X_test, y_train, y_test, preprocessor = preprocessing(y, alea=alea, test_size=test_size, \\\n",
    "                                                                    scaler=scaler)\n",
    "    # Create a KNN Regressor\n",
    "    knn_regressor = KNeighborsRegressor(metric=metric)\n",
    "\n",
    "    # Create a pipeline with preprocessing and a knn regressor\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"knn_regressor\", knn_regressor)\n",
    "    ])\n",
    "\n",
    "    # Define hyperparameters and their possible values for grid search\n",
    "    param_grid = {\n",
    "        'knn_regressor__n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "        'knn_regressor__weights': ['uniform', 'distance']\n",
    "    }\n",
    "\n",
    "    # Create the GridSearchCV object with multiple scoring metrics\n",
    "    scoring = {'neg_mean_squared_error': 'neg_mean_squared_error', 'r2': 'r2'}\n",
    "    grid_search = GridSearchCV(pipe, param_grid=param_grid,\n",
    "                            scoring=scoring, cv=5, refit='r2', verbose=1)\n",
    "\n",
    "    # Fit the GridSearchCV object to your training data to perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Access the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "    # Create the KNN regressor with the best hyperparameters\n",
    "    best_knn_regressor = KNeighborsRegressor(metric=metric,\n",
    "                                             n_neighbors=best_params['knn_regressor__n_neighbors'],\n",
    "                                             weights=best_params['knn_regressor__weights'])\n",
    "\n",
    "    # Create a pipeline with the preprocessor and the tuned knn regressor\n",
    "    pipeline_with_tuned_knn = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"knn_regressor\", best_knn_regressor)  # Use the tuned neighbor and weight values here\n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation (on training set) and display the scores for each split\n",
    "    scoring = ['r2', 'neg_mean_squared_error']\n",
    "    cv_scores = cross_validate(pipeline_with_tuned_knn, X_train, y_train, cv=5, scoring=scoring)\n",
    "    # print(\"Cross-Validation Scores (training):\", '\\n', cv_scores)\n",
    "    print(\"Cross-Validation Scores:\")\n",
    "    for i, score in enumerate(cv_scores['test_r2']):\n",
    "        print(f\"Split {i+1} : r2 = {score}\")\n",
    "\n",
    "    r2_val = cv_scores['test_r2'].mean()\n",
    "    mse_val = -cv_scores['test_neg_mean_squared_error'].mean()\n",
    "    rmse_val = np.sqrt(mse_val)\n",
    "\n",
    "    # fit model on training set\n",
    "    time_fit = fit_and_timeit(pipeline_with_tuned_knn, X_train, y_train)\n",
    "    # Make predictions\n",
    "    y_pred, time_predict = predict_and_timeit(pipeline_with_tuned_knn, X_test)\n",
    "\n",
    "    # Calculate scores on training\n",
    "    r2_train, rmse_train = calcul_scores(y_train, pipeline_with_tuned_knn.predict(X_train))\n",
    "    # and testing set\n",
    "    r2_test, rmse_test = calcul_scores(y_test, y_pred)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"R-squared (val) =  {r2_val}\")\n",
    "    print(f\"R-squared (train) =  {r2_train}\")\n",
    "    print(f\"R-squared (test) =  {r2_test}\")\n",
    "    print(f\"RMSE (val) =  {rmse_val}\")\n",
    "    print(f\"RMSE (train) =  {rmse_train}\")\n",
    "    print(f\"RMSE (test) =  {rmse_test}\" '\\n')\n",
    "\n",
    "    # display results/error as graph on first iteration (if asked to)\n",
    "    if alea == 0 and graph:\n",
    "        plot_performance_vs_neighbors(grid_search)\n",
    "        plot_predictions(r2_train, r2_test, y_pred, y_test, kind='actual_vs_predicted', y=y)\n",
    "        # plot_predictions(r2_train, r2_test, y_pred, y_test, kind='residual_vs_predicted', y=y)\n",
    "\n",
    "    # Return scores for this random state\n",
    "    return r2_val, rmse_val, r2_train, rmse_train, r2_test, rmse_test, time_fit, time_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning Models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_p5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
