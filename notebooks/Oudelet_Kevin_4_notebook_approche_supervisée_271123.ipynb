{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Catégorisez automatiquement des questions**\n",
    "\n",
    "### partie 4/8 : Prédiction de tags, approche supervisée + tracking mlflow\n",
    "\n",
    "#### <br> Notebook d’exploration et de pré-traitement des questions, comprenant une analyse univariée et multivariée, un nettoyage des questions, un feature engineering de type bag of words avec réduction de dimension (du vocabulaire et des tags) \n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import librairies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "from pandarallel import pandarallel\n",
    "from pprint import pprint\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "# NLP\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim import similarities\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "#\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, PredictionErrorDisplay\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print('\\nPython version ' + sys.version)\n",
    "print('pyLDAvis version ' + pyLDAvis.__version__)\n",
    "\n",
    "# Modify if necessary\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"\\nNumber of CPU cores: {num_cores}\")\n",
    "pandarallel.initialize(progress_bar=False, nb_workers=6)\n",
    "\n",
    "#\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.models.signature import infer_signature, ModelSignature #, Schema, ParamSchema\n",
    "from mlflow.types import Schema, ParamSchema, ParamSpec, ColSpec\n",
    "\n",
    "# os.environ['MLFLOW_TRACKING_URI'] = './'\n",
    "\n",
    "# ! REQUIRES CONSOLE COMMAND : mlflow ui\n",
    "# Utilisable seulement en local...\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = MlflowClient(tracking_uri=\"http://127.0.0.1:5000\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlflow_experiment(\n",
    "    experiment_name: str, artifact_location: str, tags: dict[str, str]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a new mlflow experiment with the given name and artifact location.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    experiment_name: str\n",
    "        The name of the experiment to create.\n",
    "    artifact_location: str\n",
    "        The artifact location of the experiment to create.\n",
    "    tags: dict[str,Any]\n",
    "        The tags of the experiment to create.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    experiment_id: str\n",
    "        The id of the created experiment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        experiment_id = mlflow.create_experiment(\n",
    "            name=experiment_name, artifact_location=artifact_location, tags=tags\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Experiment {experiment_name} already exists.\")\n",
    "        experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "    mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "    return experiment_id\n",
    "\n",
    "\n",
    "def get_mlflow_experiment(\n",
    "    experiment_id: str = None, experiment_name: str = None\n",
    ") -> mlflow.entities.Experiment:\n",
    "    \"\"\"\n",
    "    Retrieve the mlflow experiment with the given id or name.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    experiment_id: str\n",
    "        The id of the experiment to retrieve.\n",
    "    experiment_name: str\n",
    "        The name of the experiment to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    experiment: mlflow.entities.Experiment\n",
    "        The mlflow experiment with the given id or name.\n",
    "    \"\"\"\n",
    "    if experiment_id is not None:\n",
    "        experiment = mlflow.get_experiment(experiment_id)\n",
    "    elif experiment_name is not None:\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    else:\n",
    "        raise ValueError(\"Either experiment_id or experiment_name must be provided.\")\n",
    "\n",
    "    return experiment\n",
    "\n",
    "\n",
    "def turn_str_back_into_list(df):\n",
    "    \"\"\"Correct the type change due to .csv export\"\"\"\n",
    "\n",
    "    df['title_nltk'] = df['title_nltk'].apply(ast.literal_eval)\n",
    "    df['body_nltk'] = df['body_nltk'].apply(ast.literal_eval)\n",
    "    df['title_spacy'] = df['title_spacy'].apply(ast.literal_eval)\n",
    "    df['body_spacy'] = df['body_spacy'].apply(ast.literal_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, this needs mlfow ui console command first -> unusable on remote server\n",
    "# all_experiments = client.search_experiments()\n",
    "# pprint(all_experiments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>all_tags</th>\n",
       "      <th>title_nltk</th>\n",
       "      <th>body_nltk</th>\n",
       "      <th>title_spacy</th>\n",
       "      <th>body_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-06-05 15:13:02</td>\n",
       "      <td>How to use memset while handling strings in C++?</td>\n",
       "      <td>I am from Python background and recently learn...</td>\n",
       "      <td>['c++', 'initialization', 'c-strings', 'string...</td>\n",
       "      <td>[memset, handle, string]</td>\n",
       "      <td>[memset, handle, string, python, background, l...</td>\n",
       "      <td>[use, memset, handle, string]</td>\n",
       "      <td>[background, learn, function, memset, follow, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-10-31 12:35:02</td>\n",
       "      <td>How to correct spelling in google docs using k...</td>\n",
       "      <td>I would like to be able to replace a misspelle...</td>\n",
       "      <td>['gmail', 'keyboard-shortcuts', 'google-docs',...</td>\n",
       "      <td>[correct, spell, google, doc, keyboard, shortcut]</td>\n",
       "      <td>[correct, spell, google, doc, shortcut, like, ...</td>\n",
       "      <td>[correct, spelling, keyboard, shortcut]</td>\n",
       "      <td>[like, replace, word, recommend, correction, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-19 10:40:23</td>\n",
       "      <td>live server vscode on another computer</td>\n",
       "      <td>I have 2 computers. when I open the project wi...</td>\n",
       "      <td>['visual-studio-code', 'server', 'localhost', ...</td>\n",
       "      <td>[server, vscode, computer]</td>\n",
       "      <td>[server, vscode, computer, open, project, give...</td>\n",
       "      <td>[server, vscode, computer]</td>\n",
       "      <td>[computer, open, project, server, url, want, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-10-23 16:47:04</td>\n",
       "      <td>django ajax post 403 forbidden</td>\n",
       "      <td>using django 1.4 im getting a 403 error when i...</td>\n",
       "      <td>['javascript', 'ajax', 'django', 'http-post', ...</td>\n",
       "      <td>[django, ajax, forbidden]</td>\n",
       "      <td>[django, ajax, get, error, try, post, javascri...</td>\n",
       "      <td>[forbid]</td>\n",
       "      <td>[django, error, try, post, javascript, server,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-04-21 16:10:24</td>\n",
       "      <td>Listen to changes and reload container on code...</td>\n",
       "      <td>I am using docker-compose in visual studio 201...</td>\n",
       "      <td>['angular', 'visual-studio', 'docker', 'docker...</td>\n",
       "      <td>[listen, change, reload, container, code, dock...</td>\n",
       "      <td>[listen, change, reload, container, code, dock...</td>\n",
       "      <td>[listen, change, reload, container, code, dock...</td>\n",
       "      <td>[docker, compose, studio, window, run, contain...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          CreationDate                                              title  \\\n",
       "0  2019-06-05 15:13:02   How to use memset while handling strings in C++?   \n",
       "1  2018-10-31 12:35:02  How to correct spelling in google docs using k...   \n",
       "2  2020-09-19 10:40:23             live server vscode on another computer   \n",
       "3  2012-10-23 16:47:04                     django ajax post 403 forbidden   \n",
       "4  2019-04-21 16:10:24  Listen to changes and reload container on code...   \n",
       "\n",
       "                                                body  \\\n",
       "0  I am from Python background and recently learn...   \n",
       "1  I would like to be able to replace a misspelle...   \n",
       "2  I have 2 computers. when I open the project wi...   \n",
       "3  using django 1.4 im getting a 403 error when i...   \n",
       "4  I am using docker-compose in visual studio 201...   \n",
       "\n",
       "                                            all_tags  \\\n",
       "0  ['c++', 'initialization', 'c-strings', 'string...   \n",
       "1  ['gmail', 'keyboard-shortcuts', 'google-docs',...   \n",
       "2  ['visual-studio-code', 'server', 'localhost', ...   \n",
       "3  ['javascript', 'ajax', 'django', 'http-post', ...   \n",
       "4  ['angular', 'visual-studio', 'docker', 'docker...   \n",
       "\n",
       "                                          title_nltk  \\\n",
       "0                           [memset, handle, string]   \n",
       "1  [correct, spell, google, doc, keyboard, shortcut]   \n",
       "2                         [server, vscode, computer]   \n",
       "3                          [django, ajax, forbidden]   \n",
       "4  [listen, change, reload, container, code, dock...   \n",
       "\n",
       "                                           body_nltk  \\\n",
       "0  [memset, handle, string, python, background, l...   \n",
       "1  [correct, spell, google, doc, shortcut, like, ...   \n",
       "2  [server, vscode, computer, open, project, give...   \n",
       "3  [django, ajax, get, error, try, post, javascri...   \n",
       "4  [listen, change, reload, container, code, dock...   \n",
       "\n",
       "                                         title_spacy  \\\n",
       "0                      [use, memset, handle, string]   \n",
       "1            [correct, spelling, keyboard, shortcut]   \n",
       "2                         [server, vscode, computer]   \n",
       "3                                           [forbid]   \n",
       "4  [listen, change, reload, container, code, dock...   \n",
       "\n",
       "                                          body_spacy  \n",
       "0  [background, learn, function, memset, follow, ...  \n",
       "1  [like, replace, word, recommend, correction, k...  \n",
       "2  [computer, open, project, server, url, want, b...  \n",
       "3  [django, error, try, post, javascript, server,...  \n",
       "4  [docker, compose, studio, window, run, contain...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./../data/cleaned_data/train_bow_uniques.csv', sep=',')\n",
    "test = pd.read_csv('./../data/cleaned_data/test_bow_uniques.csv', sep=',')\n",
    "\n",
    "turn_str_back_into_list(train)\n",
    "turn_str_back_into_list(test)\n",
    "\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_data(data, dataset):\n",
    "    \"\"\"\n",
    "    Prepares our dataframe for preprocessing\n",
    "    \"\"\"\n",
    "    num = [column for column in data.select_dtypes(include=['number']).columns if column not in targets]\n",
    "    cat = [column for column in data if column not in (targets + num)]\n",
    "\n",
    "    X = data.drop(targets, axis=1).copy()\n",
    "    Y = data[targets].copy()\n",
    "\n",
    "    # Dans ce notebook, nous allons prédire la conso ttle d'énergie\n",
    "    y_E = Y['SiteEnergyUse(kBtu)'].copy()\n",
    "    y_EI = Y['SiteEUI(kBtu/sf)'].copy()\n",
    "\n",
    "    # A DECOMMENTER POUR TESTER LA TARGET EMISSIONS\n",
    "    # y_E = (Y['TotalGHGEmissions'] + 1).copy()\n",
    "\n",
    "    return (cat, num, X, y_E, y_EI, dataset)\n",
    "\n",
    "# On les initialize pour simplifier le travail du relecteur syntaxique ^^\n",
    "cat, num, X, y_E, y_EI, dataset = None, None, None, None, None, None\n",
    "\n",
    "\n",
    "def preprocessing(y=y_E, alea=42, test_size=test_size, scaler=None):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=alea)\n",
    "\n",
    "    numeric_preprocessor = Pipeline(steps=[\n",
    "        (\"scaler\", scaler)\n",
    "        ])\n",
    "\n",
    "    # drop_idx_[0] ?\n",
    "    categorical_preprocessor = Pipeline(steps=[\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ])\n",
    "\n",
    "    # ascategorical first ? Apparemment pas besoin\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"categorical\", categorical_preprocessor, cat),\n",
    "        (\"numerical\", numeric_preprocessor, num)],\n",
    "        verbose_feature_names_out=True,  # prepend the preprocessor names\n",
    "        remainder='passthrough'\n",
    "        )\n",
    "\n",
    "    return (X_train, X_test, y_train, y_test, preprocessor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic ML models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Often gives good results if enough data\n",
    "# Accepts basically any input, as long as it is numerical\n",
    "\n",
    "# => Perfect for testing different embeddings !\n",
    "\n",
    "# add random state\n",
    "# add grid search cv\n",
    "# add other score ? silhouette ? ...\n",
    "\n",
    "def suggest_topics_using_knn(df, feature, alea=42):\n",
    "    documents = df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Set training parameters.\n",
    "    num_topics = 10\n",
    "    chunksize = 2000\n",
    "    passes = 20\n",
    "    iterations = 400\n",
    "    eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "    # Make a index to word dictionary.\n",
    "    temp = gensim_dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = gensim_dictionary.id2token\n",
    "\n",
    "    model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every, random_state=alea)\n",
    "\n",
    "    top_topics = model.top_topics(corpus, topn=20)\n",
    "\n",
    "    # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "    # = umass if same topn (default 20)\n",
    "    avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "    print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "    # Compute Coherence Score (Umass)\n",
    "    coherence_umass = CoherenceModel(model=model, texts=documents, dictionary=gensim_dictionary, coherence='u_mass')\n",
    "    coherence_lda_umass = coherence_umass.get_coherence()\n",
    "    print('u_mass Coherence Score: %.4f.' % coherence_lda_umass)\n",
    "\n",
    "    # Compute Coherence Score (cv)\n",
    "    coherence_cv = CoherenceModel(model=model, texts=documents, dictionary=gensim_dictionary, coherence='c_v')\n",
    "    coherence_lda_cv = coherence_cv.get_coherence()\n",
    "    print('c_v Coherence Score: %.4f.' % coherence_lda_cv)\n",
    "\n",
    "    # Compute Coherence Score (npmi)\n",
    "    coherence_npmi = CoherenceModel(model=model, texts=documents, dictionary=gensim_dictionary, coherence='c_npmi')\n",
    "    coherence_lda_npmi = coherence_npmi.get_coherence()\n",
    "    print('c_npmi Coherence Score: %.4f.' % coherence_lda_npmi)\n",
    "\n",
    "    # Perplexity is not a coherence score but a measure of how well the model predicts a sample.\n",
    "    # A lower perplexity indicates better model performance.\n",
    "    perplexity = model.log_perplexity(corpus)\n",
    "    print('Perplexity: %.4f.' % perplexity)\n",
    "\n",
    "    # Visualize the topics\n",
    "    vis_data = gensimvis.prepare(model, corpus, gensim_dictionary)\n",
    "    display(pyLDAvis.display(vis_data))\n",
    "\n",
    "    # Uncomment the next line if you want to save the plot to a file\n",
    "    # pyLDAvis.save_html(vis_data, 'artifacts/lda_vis.html')\n",
    "\n",
    "    pprint(top_topics)\n",
    "    # to print all topics\n",
    "    # pprint(model.print_topics())\n",
    "\n",
    "    return model, corpus, gensim_dictionary\n",
    "\n",
    "lda_test, corpus_test, dict_test = suggest_topics_using_LDA(train, 'title_nltk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plotting function\n",
    "def plot_performance_vs_neighbors(grid_search):\n",
    "    # Extract the results from the GridSearchCV object\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Extract the parameters and scores for both uniform and distance weights\n",
    "    params_uniform = [param for param in results['params'][::2]]\n",
    "    params_distance = [param for param in results['params'][1::2]]\n",
    "    test_scores_uniform = results['mean_test_r2'][::2]\n",
    "    test_scores_distance = results['mean_test_r2'][1::2]\n",
    "\n",
    "    # Extract the parameter values for uniform and distance weights\n",
    "    n_neighbors_uniform = [param['knn_regressor__n_neighbors'] for param in params_uniform]\n",
    "    n_neighbors_distance = [param['knn_regressor__n_neighbors'] for param in params_distance]\n",
    "\n",
    "    # Create separate plots for uniform and distance weights\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot for uniform weight\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(n_neighbors_uniform, test_scores_uniform, marker='o', linestyle='-')\n",
    "    plt.title(\"Performance vs. Number of Neighbors (Uniform Weight)\")\n",
    "    plt.xlabel(\"Number of Neighbors\")\n",
    "    plt.ylabel(\"Mean Test R-squared\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot for distance weight\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(n_neighbors_distance, test_scores_distance, marker='o', linestyle='-')\n",
    "    plt.title(\"Performance vs. Number of Neighbors (Distance Weight)\")\n",
    "    plt.xlabel(\"Number of Neighbors\")\n",
    "    plt.ylabel(\"Mean Test R-squared\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pipe_knn(alea, y, scaler, graph, metric):\n",
    "\n",
    "    # Séparation des jeux de données entrainement / validation, preprocessing\n",
    "    X_train, X_test, y_train, y_test, preprocessor = preprocessing(y, alea=alea, test_size=test_size, \\\n",
    "                                                                    scaler=scaler)\n",
    "    # Create a KNN Regressor\n",
    "    knn_regressor = KNeighborsRegressor(metric=metric)\n",
    "\n",
    "    # Create a pipeline with preprocessing and a knn regressor\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"knn_regressor\", knn_regressor)\n",
    "    ])\n",
    "\n",
    "    # Define hyperparameters and their possible values for grid search\n",
    "    param_grid = {\n",
    "        'knn_regressor__n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "        'knn_regressor__weights': ['uniform', 'distance']\n",
    "    }\n",
    "\n",
    "    # Create the GridSearchCV object with multiple scoring metrics\n",
    "    scoring = {'neg_mean_squared_error': 'neg_mean_squared_error', 'r2': 'r2'}\n",
    "    grid_search = GridSearchCV(pipe, param_grid=param_grid,\n",
    "                            scoring=scoring, cv=5, refit='r2', verbose=1)\n",
    "\n",
    "    # Fit the GridSearchCV object to your training data to perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Access the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "    # Create the KNN regressor with the best hyperparameters\n",
    "    best_knn_regressor = KNeighborsRegressor(metric=metric,\n",
    "                                             n_neighbors=best_params['knn_regressor__n_neighbors'],\n",
    "                                             weights=best_params['knn_regressor__weights'])\n",
    "\n",
    "    # Create a pipeline with the preprocessor and the tuned knn regressor\n",
    "    pipeline_with_tuned_knn = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"knn_regressor\", best_knn_regressor)  # Use the tuned neighbor and weight values here\n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation (on training set) and display the scores for each split\n",
    "    scoring = ['r2', 'neg_mean_squared_error']\n",
    "    cv_scores = cross_validate(pipeline_with_tuned_knn, X_train, y_train, cv=5, scoring=scoring)\n",
    "    # print(\"Cross-Validation Scores (training):\", '\\n', cv_scores)\n",
    "    print(\"Cross-Validation Scores:\")\n",
    "    for i, score in enumerate(cv_scores['test_r2']):\n",
    "        print(f\"Split {i+1} : r2 = {score}\")\n",
    "\n",
    "    r2_val = cv_scores['test_r2'].mean()\n",
    "    mse_val = -cv_scores['test_neg_mean_squared_error'].mean()\n",
    "    rmse_val = np.sqrt(mse_val)\n",
    "\n",
    "    # fit model on training set\n",
    "    time_fit = fit_and_timeit(pipeline_with_tuned_knn, X_train, y_train)\n",
    "    # Make predictions\n",
    "    y_pred, time_predict = predict_and_timeit(pipeline_with_tuned_knn, X_test)\n",
    "\n",
    "    # Calculate scores on training\n",
    "    r2_train, rmse_train = calcul_scores(y_train, pipeline_with_tuned_knn.predict(X_train))\n",
    "    # and testing set\n",
    "    r2_test, rmse_test = calcul_scores(y_test, y_pred)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"R-squared (val) =  {r2_val}\")\n",
    "    print(f\"R-squared (train) =  {r2_train}\")\n",
    "    print(f\"R-squared (test) =  {r2_test}\")\n",
    "    print(f\"RMSE (val) =  {rmse_val}\")\n",
    "    print(f\"RMSE (train) =  {rmse_train}\")\n",
    "    print(f\"RMSE (test) =  {rmse_test}\" '\\n')\n",
    "\n",
    "    # display results/error as graph on first iteration (if asked to)\n",
    "    if alea == 0 and graph:\n",
    "        plot_performance_vs_neighbors(grid_search)\n",
    "        plot_predictions(r2_train, r2_test, y_pred, y_test, kind='actual_vs_predicted', y=y)\n",
    "        # plot_predictions(r2_train, r2_test, y_pred, y_test, kind='residual_vs_predicted', y=y)\n",
    "\n",
    "    # Return scores for this random state\n",
    "    return r2_val, rmse_val, r2_train, rmse_train, r2_test, rmse_test, time_fit, time_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning Models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_p5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
