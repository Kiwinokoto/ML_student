{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Catégorisez automatiquement des questions**\n",
    "\n",
    "### partie 4/8 : Prédiction de tags, approche supervisée + tracking mlflow\n",
    "\n",
    "#### <br> Notebook d’exploration et de pré-traitement des questions, comprenant une analyse univariée et multivariée, un nettoyage des questions, un feature engineering de type bag of words avec réduction de dimension (du vocabulaire et des tags) \n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import librairies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python version 3.11.5 (main, Sep 11 2023, 13:23:44) [GCC 11.2.0]\n",
      "pyLDAvis version 3.4.0\n",
      "\n",
      "Number of CPU cores: 8\n",
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, random\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "from pandarallel import pandarallel\n",
    "from pprint import pprint\n",
    "import json\n",
    "import requests\n",
    "from collections import Counter\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "# NLP\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import corpus2dense\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim import similarities\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "#\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.metrics import make_scorer, PredictionErrorDisplay\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "print('\\nPython version ' + sys.version)\n",
    "print('pyLDAvis version ' + pyLDAvis.__version__)\n",
    "\n",
    "# Modify if necessary\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"\\nNumber of CPU cores: {num_cores}\")\n",
    "pandarallel.initialize(progress_bar=False, nb_workers=6)\n",
    "\n",
    "#\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.models.signature import infer_signature, ModelSignature #, Schema, ParamSchema\n",
    "from mlflow.types import Schema, ParamSchema, ParamSpec, ColSpec\n",
    "\n",
    "# os.environ['MLFLOW_TRACKING_URI'] = './'\n",
    "\n",
    "# ! REQUIRES CONSOLE COMMAND : mlflow ui\n",
    "# Utilisable seulement en local...\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = MlflowClient(tracking_uri=\"http://127.0.0.1:5000\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_values(df):\n",
    "    \"\"\"Generates a DataFrame containing the count and proportion of missing values for each feature.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with columns for the feature name, count of missing values,\n",
    "        count of non-missing values, proportion of missing values, and data type for each feature.\n",
    "    \"\"\"\n",
    "    # Count the missing values for each column\n",
    "    missing = df.isna().sum()\n",
    "\n",
    "    # Calculate the percentage of missing values\n",
    "    percent_missing = df.isna().mean() * 100\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    missings_df = pd.DataFrame({\n",
    "        'column_name': df.columns,\n",
    "        'missing': missing,\n",
    "        'present': df.shape[0] - missing,  # Count of non-missing values\n",
    "        'percent_missing': percent_missing.round(2),  # Rounded to 2 decimal places\n",
    "        'type': df.dtypes\n",
    "    })\n",
    "\n",
    "    # Sort the DataFrame by the count of missing values\n",
    "    missings_df.sort_values('missing', inplace=True)\n",
    "\n",
    "    return missings_df\n",
    "\n",
    "# with pd.option_context('display.max_rows', 1000):\n",
    "#   display(get_missing_values(df))\n",
    "\n",
    "\n",
    "def quick_look(df, miss=True):\n",
    "    \"\"\"\n",
    "    Display a quick overview of a DataFrame, including shape, head, tail, unique values, and duplicates.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame to inspect.\n",
    "        check_missing (bool, optional): Whether to check and display missing values (default is True).\n",
    "\n",
    "    The function provides a summary of the DataFrame, including its shape, the first and last rows, the count of unique values per column, and the number of duplicates.\n",
    "    If `check_missing` is set to True, it also displays missing value information.\n",
    "    \"\"\"\n",
    "    print(f'shape : {df.shape}')\n",
    "\n",
    "    display(df.head())\n",
    "    display(df.tail())\n",
    "\n",
    "    print('uniques :')\n",
    "    display(df.nunique())\n",
    "\n",
    "    print('Doublons ? ', df.duplicated(keep='first').sum(), '\\n')\n",
    "\n",
    "    if miss:\n",
    "        display(get_missing_values(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlflow_experiment(\n",
    "    experiment_name: str, artifact_location: str, tags: dict[str, str]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a new mlflow experiment with the given name and artifact location.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    experiment_name: str\n",
    "        The name of the experiment to create.\n",
    "    artifact_location: str\n",
    "        The artifact location of the experiment to create.\n",
    "    tags: dict[str,Any]\n",
    "        The tags of the experiment to create.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    experiment_id: str\n",
    "        The id of the created experiment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        experiment_id = mlflow.create_experiment(\n",
    "            name=experiment_name, artifact_location=artifact_location, tags=tags\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Experiment {experiment_name} already exists.\")\n",
    "        experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "    mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "    return experiment_id\n",
    "\n",
    "\n",
    "def get_mlflow_experiment(\n",
    "    experiment_id: str = None, experiment_name: str = None\n",
    ") -> mlflow.entities.Experiment:\n",
    "    \"\"\"\n",
    "    Retrieve the mlflow experiment with the given id or name.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    experiment_id: str\n",
    "        The id of the experiment to retrieve.\n",
    "    experiment_name: str\n",
    "        The name of the experiment to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    experiment: mlflow.entities.Experiment\n",
    "        The mlflow experiment with the given id or name.\n",
    "    \"\"\"\n",
    "    if experiment_id is not None:\n",
    "        experiment = mlflow.get_experiment(experiment_id)\n",
    "    elif experiment_name is not None:\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    else:\n",
    "        raise ValueError(\"Either experiment_id or experiment_name must be provided.\")\n",
    "\n",
    "    return experiment\n",
    "\n",
    "\n",
    "def turn_str_back_into_list(df):\n",
    "    \"\"\"Correct the type change due to .csv export\"\"\"\n",
    "\n",
    "    df['title_nltk'] = df['title_nltk'].apply(ast.literal_eval)\n",
    "    df['body_nltk'] = df['body_nltk'].apply(ast.literal_eval)\n",
    "    df['title_spacy'] = df['title_spacy'].apply(ast.literal_eval)\n",
    "    df['body_spacy'] = df['body_spacy'].apply(ast.literal_eval)\n",
    "    df['all_tags'] = df['all_tags'].apply(ast.literal_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, this needs mlfow ui console command first -> unusable on remote server\n",
    "# all_experiments = client.search_experiments()\n",
    "# pprint(all_experiments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>all_tags</th>\n",
       "      <th>title_nltk</th>\n",
       "      <th>body_nltk</th>\n",
       "      <th>title_spacy</th>\n",
       "      <th>body_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42893</th>\n",
       "      <td>2017-02-23 11:34:31</td>\n",
       "      <td>Do we need clear MDC after HTTP request in Spring</td>\n",
       "      <td>According to this answer thread local variable...</td>\n",
       "      <td>[java, spring, logging, log4j, logback]</td>\n",
       "      <td>[need, mdc, request, spring]</td>\n",
       "      <td>[need, mdc, request, spring, accord, answer, t...</td>\n",
       "      <td>[need, request]</td>\n",
       "      <td>[accord, answer, thread, variable, use, clear,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42894</th>\n",
       "      <td>2011-10-13 20:57:32</td>\n",
       "      <td>How to make i18n with Handlebars.js (mustache ...</td>\n",
       "      <td>I'm currently using Handlebars.js (associated ...</td>\n",
       "      <td>[javascript, jquery, internationalization, han...</td>\n",
       "      <td>[make, i18n, handlebar, template]</td>\n",
       "      <td>[make, i18n, handlebar, template, associate, b...</td>\n",
       "      <td>[template]</td>\n",
       "      <td>[associate, web, app, client, render, issue, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42895</th>\n",
       "      <td>2012-09-06 00:16:46</td>\n",
       "      <td>How can I make R read my environmental variables?</td>\n",
       "      <td>I am running R on EC2 spot instances and I nee...</td>\n",
       "      <td>[linux, r, ubuntu, amazon-ec2, environment-var...</td>\n",
       "      <td>[make, read, variable]</td>\n",
       "      <td>[make, read, variable, run, spot, instance, ne...</td>\n",
       "      <td>[read, variable]</td>\n",
       "      <td>[run, spot, instance, need, terminate, cancel,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42896</th>\n",
       "      <td>2021-03-23 03:50:50</td>\n",
       "      <td>How to prevent react-query from fetching initi...</td>\n",
       "      <td>I'm using react-query v3.13 to fetch data from...</td>\n",
       "      <td>[javascript, reactjs, fetch, react-query, swr]</td>\n",
       "      <td>[prevent, query, fetch, enable]</td>\n",
       "      <td>[prevent, query, fetch, enable, data, want, po...</td>\n",
       "      <td>[prevent, react, query, fetch, enable]</td>\n",
       "      <td>[react, query, fetch, datum, want, api, point,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42897</th>\n",
       "      <td>2016-03-17 04:19:15</td>\n",
       "      <td>Inserting into table with an Identity column w...</td>\n",
       "      <td>I have a table A_tbl in my database. I have cr...</td>\n",
       "      <td>[sql, sql-server, database, ssms, database-rep...</td>\n",
       "      <td>[insert, table, identity, column, replication,...</td>\n",
       "      <td>[insert, table, identity, column, replication,...</td>\n",
       "      <td>[insert, table, column, replication, cause, er...</td>\n",
       "      <td>[table, database, create, trigger, capture, in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              CreationDate                                              title  \\\n",
       "42893  2017-02-23 11:34:31  Do we need clear MDC after HTTP request in Spring   \n",
       "42894  2011-10-13 20:57:32  How to make i18n with Handlebars.js (mustache ...   \n",
       "42895  2012-09-06 00:16:46  How can I make R read my environmental variables?   \n",
       "42896  2021-03-23 03:50:50  How to prevent react-query from fetching initi...   \n",
       "42897  2016-03-17 04:19:15  Inserting into table with an Identity column w...   \n",
       "\n",
       "                                                    body  \\\n",
       "42893  According to this answer thread local variable...   \n",
       "42894  I'm currently using Handlebars.js (associated ...   \n",
       "42895  I am running R on EC2 spot instances and I nee...   \n",
       "42896  I'm using react-query v3.13 to fetch data from...   \n",
       "42897  I have a table A_tbl in my database. I have cr...   \n",
       "\n",
       "                                                all_tags  \\\n",
       "42893            [java, spring, logging, log4j, logback]   \n",
       "42894  [javascript, jquery, internationalization, han...   \n",
       "42895  [linux, r, ubuntu, amazon-ec2, environment-var...   \n",
       "42896     [javascript, reactjs, fetch, react-query, swr]   \n",
       "42897  [sql, sql-server, database, ssms, database-rep...   \n",
       "\n",
       "                                              title_nltk  \\\n",
       "42893                       [need, mdc, request, spring]   \n",
       "42894                  [make, i18n, handlebar, template]   \n",
       "42895                             [make, read, variable]   \n",
       "42896                    [prevent, query, fetch, enable]   \n",
       "42897  [insert, table, identity, column, replication,...   \n",
       "\n",
       "                                               body_nltk  \\\n",
       "42893  [need, mdc, request, spring, accord, answer, t...   \n",
       "42894  [make, i18n, handlebar, template, associate, b...   \n",
       "42895  [make, read, variable, run, spot, instance, ne...   \n",
       "42896  [prevent, query, fetch, enable, data, want, po...   \n",
       "42897  [insert, table, identity, column, replication,...   \n",
       "\n",
       "                                             title_spacy  \\\n",
       "42893                                    [need, request]   \n",
       "42894                                         [template]   \n",
       "42895                                   [read, variable]   \n",
       "42896             [prevent, react, query, fetch, enable]   \n",
       "42897  [insert, table, column, replication, cause, er...   \n",
       "\n",
       "                                              body_spacy  \n",
       "42893  [accord, answer, thread, variable, use, clear,...  \n",
       "42894  [associate, web, app, client, render, issue, w...  \n",
       "42895  [run, spot, instance, need, terminate, cancel,...  \n",
       "42896  [react, query, fetch, datum, want, api, point,...  \n",
       "42897  [table, database, create, trigger, capture, in...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4767, 8)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./../data/cleaned_data/train_bow_uniques.csv', sep=',')\n",
    "test = pd.read_csv('./../data/cleaned_data/test_bow_uniques.csv', sep=',')\n",
    "\n",
    "turn_str_back_into_list(train)\n",
    "turn_str_back_into_list(test)\n",
    "\n",
    "display(train.tail())\n",
    "\n",
    "train.shape\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic ML models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Often gives good results if enough data\n",
    "# Accepts basically any input, as long as it is numerical\n",
    "\n",
    "# => Perfect for testing different embeddings !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dummy knn : il copie sur le + proche voisin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['javascript', 'internet-explorer', 'class', 'internet-explorer-8', 'classname']\n",
      "['javascript', 'internet-explorer', 'class', 'internet-explorer-8', 'classname'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_tags_using_dummy_knn(df, feature, target, k=1, exemple=None):\n",
    "    documents = df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    dense_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Ici on ne va pas demander au knn de faire de prediction,\n",
    "    # On veut juste qu'il trouve les voisins.\n",
    "    # Mais la fonction fit a besoin de targets en param\n",
    "    target_values = df[target].values\n",
    "\n",
    "    # Initialize kNN model\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=k, metric='cosine', algorithm='brute')\n",
    "    knn_model.fit(dense_matrix, target_values)\n",
    "\n",
    "    # Example query\n",
    "    query_document = exemple\n",
    "    query_bow = gensim_dictionary.doc2bow(query_document)\n",
    "    query_vector = corpus2dense([query_bow], num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Find nearest neighbors\n",
    "    _, indices = knn_model.kneighbors(query_vector)\n",
    "\n",
    "    # Aggregate tags from neighbors\n",
    "    neighbor_tags = [tag for i in indices.flatten() for tag in df.iloc[i][target]]\n",
    "\n",
    "    print(neighbor_tags)\n",
    "\n",
    "    # Predict tags based on most common tags among neighbors\n",
    "    predicted_tags = [tag for tag, _ in Counter(neighbor_tags).most_common(n=10)]\n",
    "    # 5 tags/question en moyenne mais on peut suggérer +\n",
    "    # ici a ameliorer\n",
    "\n",
    "    return predicted_tags, knn_model\n",
    "\n",
    "\n",
    "exemple = [\"your\", 'text', 'document', 'javascript']\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "predicted_tags, knn_test = predict_tags_using_dummy_knn(train, 'title_nltk', 'all_tags', exemple=exemple)\n",
    "print(predicted_tags, '\\n')\n",
    "\n",
    "# javascript ok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### knn basic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c#', '.net', 'ms-word', 'openxml', 'openxml-sdk', 'javascript', 'internet-explorer', 'class', 'internet-explorer-8', 'classname', 'java', 'javascript', 'ajax', 'selenium', 'htmlunit-driver', 'c#', '.net', 'windows', 'f#', 'console', 'javascript', 'jquery', 'jquery-plugins', 'text-to-speech', 'html5-audio', 'javascript', 'html', 'css', 'text', 'truncate', 'javascript', 'jquery', 'css', 'dom', 'document', 'javascript', 'html', 'string', 'text', 'extract', 'javascript', 'dom', 'substring', 'indexof', 'getselection', 'javascript', 'html', 'function', 'text', 'onclick', 'c#', '.net', 'html', 'pdf', 'extract', 'javascript', 'jquery', 'css', 'copy', 'cut', 'javascript', 'html', 'url', 'base64', 'data-uri', 'python', 'module', 'preprocessor', 'nlp', 'stemming', 'javascript', 'php', 'jquery', 'curl', 'http-headers', 'python', 'text', 'replace', 'ms-word', 'python-docx', 'c#', 'javascript', 'html', 'http', 'dom', 'javascript', 'jquery', 'ruby-on-rails', 'tdd', 'jasmine', 'ios', 'swift', 'height', 'uilabel', 'frame', 'javascript', 'html', 'performance', 'three.js', 'webgl', 'ios', 'objective-c', 'uilabel', 'autolayout', 'uistoryboard', 'linux', 'assembly', 'x86-64', 'calling-convention', 'abi', 'javascript', 'jquery', 'html', 'css', 'bootstrap-4', 'localization', 'internationalization', 'translation', 'stripe-payments', 'stripe.net', 'html', 'spring', 'reactjs', 'spring-boot', 'thymeleaf', 'javascript', 'html', 'function', 'anchor', 'href', 'javascript', 'jquery', 'html', 'dom', 'document-ready', 'javascript', 'jquery', 'textarea', 'hyperlink', 'addition', 'html', 'sublimetext2', 'sublimetext', 'indentation', 'reformat', 'javascript', 'arrays', 'object', 'arguments', 'slice', 'javascript', 'jquery', 'ajax', 'asp.net-mvc-3', 'url', 'javascript', 'iphone', 'css', 'ios', 'uiwebview', 'javascript', 'unicode', 'diacritics', 'combining-marks', 'zalgo', 'javascript', 'firebase', 'google-cloud-platform', 'google-cloud-firestore', 'typeerror', 'javascript', 'function', 'oop', 'if-statement', 'conditional-statements', 'javascript', 'html', 'full-text-search', 'local-storage', 'client-side', 'javascript', 'node.js', 'backbone.js', 'ember.js', 'javascript-framework', 'c#', 'javascript', 'jquery', 'asp.net-mvc', 'razor', 'javascript', 'oop', 'functional-programming', 'polymorphism', 'parametric-polymorphism', 'html', 'meta-tags', 'semantics', 'semantic-web', 'semantic-markup', 'mongodb', 'mongodb-query', 'aggregation-framework', 'spring-data-mongodb', 'full-text-indexing', 'python', 'django', 'orm', 'mongodb', 'mongoengine', 'javascript', 'c#', 'asp.net-mvc', 'dictionary', 'asp.net-web-api', 'javascript', 'css', 'encapsulation', 'styling', 'web-component', 'ios', 'ocr', 'xcode4.5', 'tesseract', 'leptonica', 'javascript', 'reactjs', 'functional-programming', 'immutability', 'immutable.js', 'html', 'css', 'text', 'autocomplete', 'sublimetext', 'javascript', 'object', 'recursion', 'comparison', 'equality', 'javascript', 'arrays', 'algorithm', 'big-o', 'time-complexity', 'objective-c', 'ios', 'xcode', 'event-handling', 'uibutton']\n",
      "['javascript', 'html', 'jquery', 'css', 'c#', 'text', 'ios', 'dom', '.net', 'function'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add grid search cv\n",
    "# add score\n",
    "\n",
    "def predict_tags_using_knn(df, feature, target, k=50, exemple=None):\n",
    "    documents = df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    dense_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Ici on ne va pas demander au knn de faire de prediction,\n",
    "    # On veut juste qu'il trouve les voisins.\n",
    "    # Mais la fonction fit a besoin de targets en param\n",
    "    target_values = df[target].values\n",
    "\n",
    "    # Initialize kNN model\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=k, metric='cosine', algorithm='brute')\n",
    "    knn_model.fit(dense_matrix, target_values)\n",
    "\n",
    "    # Example query\n",
    "    query_document = exemple\n",
    "    query_bow = gensim_dictionary.doc2bow(query_document)\n",
    "    query_vector = corpus2dense([query_bow], num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Find nearest neighbors\n",
    "    _, indices = knn_model.kneighbors(query_vector)\n",
    "\n",
    "    # Aggregate tags from neighbors\n",
    "    neighbor_tags = [tag for i in indices.flatten() for tag in df.iloc[i][target]]\n",
    "\n",
    "    print(neighbor_tags)\n",
    "\n",
    "    # Predict tags based on most common tags among neighbors\n",
    "    predicted_tags = [tag for tag, _ in Counter(neighbor_tags).most_common(n=10)]\n",
    "    # 5 tags/question en moyenne mais on peut suggérer +\n",
    "    # ici a ameliorer\n",
    "\n",
    "    return predicted_tags\n",
    "\n",
    "exemple = [\"your\", 'text', 'document', 'javascript']\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "predicted_tags = predict_tags_using_knn(train, 'title_nltk', 'all_tags', exemple=exemple)\n",
    "print(predicted_tags, '\\n')\n",
    "\n",
    "# javascript ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'module', 'preprocessor', 'nlp', 'stemming', 'c#', '.net', 'ms-word', 'openxml', 'openxml-sdk', 'python', 'parsing', 'text', 'file-io', 'python-2.7', 'python-2.7', 'ubuntu', 'python-3.x', 'spatial-index', 'r-tree', 'python', 'plot', 'tree', 'data-visualization', 'visualization', 'c#', '.net', 'windows', 'f#', 'console', 'python', 'html', 'web-scraping', 'text', 'beautifulsoup', 'python', 'python-3.x', 'algorithm', 'sorting', 'mergesort', 'python', 'python-2.7', 'reflection', 'delegation', 'message-passing', 'python', 'python-3.x', 'annotations', 'lint', 'type-hinting', 'python', 'pdf', 'python-3.7', 'pypdf', 'pdf-extraction', 'python', 'selenium', 'selenium-webdriver', 'xpath', 'webdriverwait', 'python', 'macos', 'python-3.x', 'sublimetext2', 'sublimetext', 'python-3.x', 'pdf', 'text', 'extract', 'pdfminer', 'python', 'documentation', 'python-3.7', 'docstring', 'python-dataclasses', 'python', 'text', 'stemming', 'plural', 'singular', 'nlp', 'cluster-analysis', 'data-mining', 'k-means', 'text-mining', 'python', 'image', 'opencv', 'image-processing', 'computer-vision', 'python', 'regex', 'performance', 'perl', 'text-processing', 'python', 'windows', 'user-interface', 'text', 'screen', 'python', 'shell', 'encoding', 'utf-8', 'python-2.x', 'python', 'html', 'excel', 'pandas', 'dataframe', 'python', 'python-3.x', 'python-2.7', 'text-extraction', 'pdfminer', 'python', 'python-2.7', 'tkinter', 'callback', 'tkinter.text', 'python', 'utf-8', 'python-unicode', 'windows-1252', 'cp1252', 'python', 'image', 'text', 'python-imaging-library', 'bold', 'python', 'parsing', 'data-structures', 'dictionary', 'nested', 'python', 'text', 'replace', 'docx', 'zip', 'python', 'selenium', 'xpath', 'selenium-webdriver', 'webdriver', 'python', 'plugins', 'sublimetext2', 'distutils', 'python-requests', 'python', 'text', 'replace', 'ms-word', 'python-docx', 'python', 'utf-8', 'character-encoding', 'locale', 'default', 'python', 'django', 'autocomplete', 'sublimetext2', 'sublimetext', 'c#', '.net', 'html', 'pdf', 'extract', 'python', 'python-3.x', 'file', 'text', 'count', 'python', 'file', 'encryption', 'aes', 'pycrypto', 'python', 'list', 'file', 'ascii', 'newline', 'python', 'performance', 'search', 'profiling', 'large-files', 'python', 'sql-server', 'sql-server-2008', 'python-2.7', 'pymssql', 'html', 'sublimetext2', 'sublimetext', 'indentation', 'reformat', 'mongodb', 'mongodb-query', 'aggregation-framework', 'spring-data-mongodb', 'full-text-indexing', 'python', 'excel', 'com', 'pywin32', 'win32com', 'python', 'loops', 'if-statement', 'break', 'conditional-operator', 'java', 'python', 'jython', 'pip', 'easy-install', 'python', 'unit-testing', 'testing', 'mocking', 'patch', 'python', 'loops', 'for-loop', 'infinite-loop', 'infinite', 'python-2.7', 'exception', 'python-3.x', 'traceback', 'raise', 'python', 'debugging', 'multiprocessing', 'pycharm', 'winpdb', 'python', 'packaging', 'remote-access', 'download', 'software-update', 'python', 'constructor', 'destructor', 'with-statement', 'contextmanager']\n",
      "['python', 'text', 'python-3.x', 'python-2.7', 'html', 'sublimetext2', 'c#', '.net', 'pdf', 'sublimetext'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemple = [\"your\", 'text', 'document', 'python']\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "predicted_tags = predict_tags_using_knn(train, 'title_nltk', 'all_tags', exemple=exemple)\n",
    "print(predicted_tags, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['find', 'class', 'com', 'google', 'firebase', 'provider']\n",
      "['java', 'spring', 'rest', 'gradle', 'spring-boot', 'java', 'android', 'firebase', 'gradle', 'android-gradle-plugin', 'android', 'android-studio', 'firebase', 'android-gradle-plugin', 'google-play-services', 'android', 'android-intent', 'arraylist', 'unmarshalling', 'parcelable', 'php', 'class', 'laravel', 'alias', 'autoloader', 'php', 'sql', 'laravel', 'laravel-5', 'laravel-artisan', 'java', 'spring', 'spring-boot', 'spring-security', 'spring-security-oauth2', 'java', 'maven', 'maven-2', 'maven-3', 'protocol-buffers', 'android', 'google-maps', 'dictionary', 'android-mapview', 'inflate', 'android', 'android-studio', 'flutter', 'sdk', 'android-sdk-manager', 'java', 'spring', 'junit', 'spring-boot', 'spring-data', 'php', 'laravel', 'https', 'laravel-valet', 'valet', 'spring', 'maven', 'spring-mvc', 'spring-boot', 'spring-profiles', 'json', 'angular', 'typescript', 'jwt', 'guard', 'c#', '.net', 'visual-studio-2012', 'compression', 'zip', 'c#', 'asp.net', 'google-api', 'google-oauth', 'google-api-dotnet-client', 'ios', 'ipad', 'frameworks', 'header', 'gpuimage', 'android', 'firebase', 'gradle', 'android-gradle-plugin', 'crashlytics', 'java', 'jar', 'maven-2', 'manifest', 'program-entry-point', 'gcc', 'ubuntu', 'linker', 'shared-libraries', 'ld', 'java', 'interface', 'go', 'static-methods', 'abstract', 'javascript', 'node.js', 'typescript', 'nestjs', 'class-validator', 'c#', 'android', 'firebase', 'unity-game-engine', 'firebase-authentication', 'c#', '.net', 'visual-studio', 'json.net', 'nuget', 'android', 'firebase', 'android-studio', 'android-gradle-plugin', 'jcenter', 'iphone', 'objective-c', 'ios', 'xcode4.3', 'cocoapods', 'javascript', 'node.js', 'mongodb', 'mongoose', 'mongodb-query', 'database', 'security', 'firebase', 'firebase-authentication', 'firebase-security', 'php', 'laravel', 'amazon-web-services', 'laravel-5', 'laravelcollective', 'c#', '.net', 'web-config', 'class-library', 'configurationmanager', 'android', 'unity-game-engine', 'abi', 'arcore', 'unsatisfiedlinkerror', 'javascript', 'android', 'reactjs', 'react-native', 'gradle', 'python', 'tensorflow', 'keras', 'transfer-learning', 'vgg-net', 'angularjs', 'routes', 'angular-ui-router', 'single-page-application', 'angularjs-routing', 'java', 'linux', 'ubuntu', 'netbeans', 'command-line', 'firebase', 'flutter', 'dart', 'firebase-authentication', 'google-authentication', 'java', 'android', 'android-studio', 'gradle', 'build', 'c++', 'ubuntu', 'sdl', 'sdl-2', 'sdl-image', 'google-maps', 'angular', 'typescript', 'angular-cli', 'angular2-google-maps', 'python', 'class', 'inheritance', 'python-3.x', 'language-design', 'java', 'android', 'design-patterns', 'inheritance', 'parcelable', 'javascript', 'node.js', 'angular', 'typescript', 'angular7', 'c++', 'gcc', 'vtable', 'virtual-inheritance', 'vtt', 'javascript', 'firebase', 'google-drive-api', 'firebase-authentication', 'adobe-indesign', 'ios', 'objective-c', 'xcode', 'storyboard', 'xib', 'java', 'maven', 'build', 'interop', 'kotlin', 'c++', 'c++11', 'bit-fields', 'bitmask', 'enum-class', 'java', 'json', 'rest', 'jersey', 'jax-rs', 'android', 'firebase', 'flutter', 'google-play-services', 'flutter-dependencies', 'android', 'android-intent', 'classnotfoundexception', 'unmarshalling', 'parcel']\n",
      "['android', 'java', 'firebase', 'gradle', 'c#', 'javascript', 'spring', 'spring-boot', 'android-gradle-plugin', 'android-studio'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemple1 = test['title_nltk'][0]\n",
    "print(exemple1)\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "predicted_tags1 = predict_tags_using_knn(train, 'title_nltk', 'all_tags', exemple=exemple1)\n",
    "print(predicted_tags1, '\\n')\n",
    "\n",
    "# firebase peut etre predit\n",
    "# grand succes !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'lang', 'noclassdeffounderror', 'scala', 'run', 'code']\n",
      "['c#', '.net', 'wpf', 'code-behind', 'itemspanel', 'java', 'gradle', 'spring-boot', 'jar', 'build.gradle', 'reactjs', 'authentication', 'google-authentication', 'google-api-js-client', 'googleauthr', 'c', 'gcc', 'types', 'openmp', 'typeof', 'php', 'mysql', 'laravel', 'ubuntu', 'server', 'iphone', 'ios', 'xamarin.ios', 'http-response-codes', 'nsurlconnectiondelegate', 'c++', 'python', 'ctypes', 'cython', 'boost-python', 'scala', 'maven', 'apache-spark', 'noclassdeffounderror', 'spark-streaming', 'macos', 'shell', 'scala', 'terminal', 'installation', 'javascript', 'vue.js', 'visual-studio-code', 'nuxt.js', 'prettier', 'c#', 'multithreading', 'winforms', 'backgroundworker', 'infinite-loop', 'asp.net-core', 'oauth', 'identityserver4', 'openid-connect', 'asp.net-core-3.0', 'c++', 'c', 'cuda', 'parallel-processing', 'gpu', 'javascript', 'node.js', 'express', 'firebase', 'firebase-realtime-database', 'python', 'selenium', 'http', 'selenium-webdriver', 'ui-automation', 'c++', 'c', 'types', 'int64', 'long-long', 'javascript', 'angularjs', 'promise', 'deferred', 'finally', 'javascript', 'typescript', 'visual-studio-code', 'vscode-extensions', 'file-properties', 'c#', 'sql-server', 'asp.net-mvc', 'entity-framework', 'asp.net-mvc-4', 'c', 'assembly', 'x86', 'x86-64', 'shellcode', 'c', 'assembly', 'x86', 'reverse-engineering', 'decompiling', 'unix', 'ubuntu', 'command-line', 'go', 'terminal', 'c', 'linux', 'security', 'exploit', 'secure-coding', 'javascript', 'firebase', 'react-native', 'mocking', 'jestjs', 'stack-trace', 'glibc', 'sigabrt', 'segmentation-fault', 'backtrace', 'java', 'rest', 'exception', 'jersey', 'provider', 'javascript', 'python', 'google-chrome', 'sandbox', 'brython', 'java', 'selenium', 'testing', 'selenium-webdriver', 'selenium-chromedriver', 'python', 'python-2.7', 'stdout', 'stderr', 'os.system', 'java', 'python', 'apache-spark', 'directed-acyclic-graphs', 'airflow', 'scala', 'random', 'collections', 'set', 'scala-collections', 'flash', 'apache-flex', 'debugging', 'air', 'flash-builder', 'javascript', 'html', 'ajax', 'url', 'xmlhttprequest', 'iphone', 'xcode', 'ios-simulator', 'ios5', 'freeze', 'android', 'google-maps', 'google-maps-android-api-2', 'latitude-longitude', 'google-places-api', 'c++', 'performance', 'assembly', 'optimization', 'x86', 'git', 'jenkins', 'groovy', 'jenkins-plugins', 'jenkins-pipeline', 'linux', 'bash', 'shell', 'terminal', 'paste', 'android', 'android-emulator', 'android-service', 'monitoring', 'monitor', 'android', 'enums', 'android-custom-view', 'attr', 'custom-view', 'android', 'retrofit', 'rx-java', 'android-networking', 'rx-android', 'java', 'jvm', 'compatibility', 'java-7', 'java-8', 'php', 'templates', 'model-view-controller', 'file-io', 'eval', 'c#', 'android', 'xamarin', 'xamarin.android', 'android-runonuithread', 'python', 'unicode', 'encoding', 'utf-8', 'character-codes', 'javascript', 'vue.js', 'model-view-controller', 'vuejs2', 'vuejs3', 'ios', 'swift', 'ios-simulator', 'xcode6', 'xcode6-beta6', 'linux', 'bash', 'shell', 'unix', 'sudo', 'python', 'pandas', 'visual-studio-code', 'jupyter-notebook', 'tqdm', 'java', 'file', 'jar', 'io', 'extract']\n",
      "['javascript', 'python', 'java', 'c', 'android', 'c#', 'c++', 'scala', 'shell', 'terminal'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemple2 = test['title_nltk'][1]\n",
    "print(exemple2)\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "predicted_tags2 = predict_tags_using_knn(train, 'title_nltk', 'all_tags', exemple=exemple2)\n",
    "print(predicted_tags2, '\\n')\n",
    "\n",
    "# scale ok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notre objectif de prédiction de tags ressemble a un pb de classification multi-label,\n",
    "# où la matrice de confusion est extrêmement déséquilibrée :\n",
    "# 5 tags sont prédits positifs, contre environ 250 000 tags (si on travaille sur all_tags)\n",
    "# predits negatifs. Autrement dit :\n",
    "\n",
    "# On peut utiliser la precision pour évaluer notre modèle. C'est même exactement l'outil qu'il nous faut :\n",
    "# \"précision = la proportion de prédictions correctes parmi les points que l’on a prédits positifs.\"\n",
    "# En + c de loin le plus léger en ressources, puisqu'il ne s'occupe que des 5 tags prédits.\n",
    "\n",
    "# En revanche je pense que le recall n'a pas vraiment de sens ici, il sera \"écrasé\" par\n",
    "# le nombre de tags predits negatifs, sa valeur sera tjs très proche de zero.\n",
    "# (même remarque pour la spécificité et l'accuracy)\n",
    "# Et sans recall, pas de f1 score.\n",
    "# à vérifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision_topics(real_tags:list, predicted_tags:list): # pour comparer 2 listes\n",
    "    # precision = TP / (TP + FP)\n",
    "    tp = 0\n",
    "    for predicted_tag in predicted_tags:\n",
    "        if predicted_tag in real_tags:\n",
    "            tp += 1\n",
    "\n",
    "    fp = len(predicted_tags) - tp\n",
    "    precision = tp/(tp + fp)\n",
    "    # <=> precision = tp/len(predicted_tags)\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "print(precision_topics(exemple1, predicted_tags1))\n",
    "precision_topics(exemple2, predicted_tags2)\n",
    "\n",
    "# ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score(y_true, y_pred): # pour comparer 2 df ou 2 matrices de mm shape[0]\n",
    "    precision = 0\n",
    "    for i in range(0, len(y_pred)):\n",
    "        precision += precision_topics(y_true[i], y_pred[i])\n",
    "    precision_moyenne = precision / len(y_pred)\n",
    "\n",
    "    return precision_moyenne\n",
    "\n",
    "custom_precision_scorer = make_scorer(precision_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_topics(all_tags: list, predicted_tags: list):\n",
    "    # recall = TP / (TP + FN)\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    for real_tag in all_tags:\n",
    "        if real_tag in predicted_tags:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1_topics(real_tags: list, predicted_tags: list, all_tags:list):\n",
    "    precision = precision_score(real_tags, predicted_tags)\n",
    "    recall = recall_topics(all_tags, predicted_tags)\n",
    "\n",
    "    # F1 score = 2 * (precision * recall) / (precision + recall)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def compute_score(y_true, y_pred, scorer=precision_score):\n",
    "    score = 0\n",
    "    for i in range(0, len(y_pred)):\n",
    "        score += scorer(y_true[i], y_pred[i])\n",
    "    score_moyen = score / len(y_pred)\n",
    "\n",
    "    return score_moyen\n",
    "\n",
    "\n",
    "def score_tag_accuracy(real_tags: list, predicted_tags: list):\n",
    "    # accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    tp = sum(1 for tag in predicted_tags if tag in real_tags)\n",
    "    fp = sum(1 for tag in predicted_tags if tag not in real_tags)\n",
    "    fn = sum(1 for tag in real_tags if tag not in predicted_tags)\n",
    "\n",
    "    accuracy = (tp + fn) / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# On va probablement utiliser la precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5869\n",
      "42898\n",
      "(42898, 5869) \n",
      "\n",
      "array([[1., 1., 1., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)\n",
      "\n",
      "\n",
      "doc 3000 : ['tell', 'cmake', 'clang', 'window']\n",
      "real tags : ['c++', 'windows', 'build', 'cmake', 'clang']\n",
      "predicted : ['c++', 'cmake', 'windows', 'clang', 'c']\n",
      "0.8 \n",
      "\n",
      "doc 3001 : ['round', 'corner']\n",
      "real tags : ['android', 'material-design', 'bottom-sheet', 'material-components-android', 'material-components']\n",
      "predicted : ['android', 'ios', 'rounding', 'android-layout', 'swift']\n",
      "0.2 \n",
      "\n",
      "doc 3002 : ['future', 'cause', 'memory', 'leak']\n",
      "real tags : ['java', 'android', 'multithreading', 'memory-leaks', 'future']\n",
      "predicted : ['memory-leaks', 'memory', 'ios', 'javascript', 'c#']\n",
      "0.2 \n",
      "\n",
      "doc 3003 : ['pytorch', 'work']\n",
      "real tags : ['python', 'numpy', 'linear-algebra', 'pytorch', 'array-broadcasting']\n",
      "predicted : ['javascript', 'ios', 'c++', 'android', 'objective-c']\n",
      "0.0 \n",
      "\n",
      "doc 3004 : ['python', 'get', 'name', 'cpu', 'usage', 'peak', 'mem', 'window']\n",
      "real tags : ['python', 'windows', 'memory', 'process', 'cpu']\n",
      "predicted : ['python', 'windows', 'c#', 'python-3.x', 'asp.net']\n",
      "0.4 \n",
      "\n",
      "precision moyenne = 0.32\n",
      "precision moyenne = 0.32\n"
     ]
    }
   ],
   "source": [
    "# add grid search cv\n",
    "# add recall, f1 score ?\n",
    "\n",
    "def predict_tags_using_knn(train_df=train, feature='title_nltk', target='all_tags', test_df=test, k=50):\n",
    "    # 1 PREPROCESSING\n",
    "    documents = train_df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    print(len(gensim_dictionary))\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "    print(len(corpus))\n",
    "    # taille corpus ?\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    dense_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "    # taille matrice ? afficher\n",
    "    print(dense_matrix.shape, '\\n')\n",
    "    # pas tres dense ici, c notre bow donc tres sparse en fait\n",
    "    # curieux d'appeler \"corpus2dense()\" une fonction qui retourne une matrice sparse\n",
    "    pprint(dense_matrix[:10]) # vraiment tres dense, quasiment que des 0 ! Bref\n",
    "    print('\\n')\n",
    "\n",
    "    # Ici on ne va pas demander au knn de faire de prediction,\n",
    "    # On veut juste qu'il trouve les voisins.\n",
    "    # Mais la fonction fit a besoin de targets en param\n",
    "    target_values = train_df[target].values\n",
    "\n",
    "    # 2 MODEL TRAINING\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=k, metric='cosine', algorithm='brute')\n",
    "    knn_model.fit(dense_matrix, target_values)\n",
    "\n",
    "    # 3 PREDICTION\n",
    "    # Predictions completes en 1h ou 2\n",
    "    # optimiser avec pandarallel ?\n",
    "    # use a sample en attendant\n",
    "    predictions=[]\n",
    "    min_range=3000\n",
    "    max_range=3005 # test.shape[0]=4767\n",
    "    for i in range(min_range, max_range):\n",
    "        query_document = test_df[feature][i]\n",
    "        print(f'doc {i} : {query_document}')\n",
    "        print(f'real tags : {test[target][i]}')\n",
    "        query_bow = gensim_dictionary.doc2bow(query_document)\n",
    "        query_vector = corpus2dense([query_bow], num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "        # Find nearest neighbors\n",
    "        _, indices = knn_model.kneighbors(query_vector)\n",
    "\n",
    "        # Aggregate tags from neighbors\n",
    "        neighbor_tags = [tag for i in indices.flatten() for tag in train_df.iloc[i][target]]\n",
    "\n",
    "        # Predict tags based on most common tags among neighbors\n",
    "        predicted_tags = [tag for tag, _ in Counter(neighbor_tags).most_common(n=5)]\n",
    "        # 5 tags/question en moyenne mais on peut suggérer +\n",
    "        predictions.append(predicted_tags)\n",
    "        print(f'predicted : {predicted_tags}')\n",
    "        print(precision_topics(test_df[target][i], predicted_tags), '\\n')\n",
    "\n",
    "    true_tags = [tags for tags in test_df[target][min_range:max_range]]\n",
    "\n",
    "    mean_precision = precision_score(true_tags, predictions)\n",
    "    precis_moy = compute_score(true_tags, predictions, scorer=precision_topics)\n",
    "    print(f'precision moyenne = {mean_precision}')\n",
    "    print(f'precision moyenne = {precis_moy}')\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "predict_tags_using_knn()\n",
    "\n",
    "# 0.34 de precision ?? c enorme !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 3000 : ['tell', 'cmake', 'clang', 'window']\n",
      "real tags : ['c++', 'windows', 'build', 'cmake', 'clang']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected only 0s and 1s in label indicator. Also got [0.02 0.04 0.06 0.08 0.1  0.12 0.14 0.2  0.34 0.36 0.52]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Call the function with your DataFrame and the desired text feature and target tags\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m predictions_rf \u001b[38;5;241m=\u001b[39m predict_tags_using_rf()\n",
      "Cell \u001b[0;32mIn[16], line 36\u001b[0m, in \u001b[0;36mpredict_tags_using_rf\u001b[0;34m(train_df, feature, target, test_df, n_estimators)\u001b[0m\n\u001b[1;32m     33\u001b[0m prediction_encoded \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict(query_vector\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Convert back to original tag format\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m predicted_tags \u001b[38;5;241m=\u001b[39m mlb\u001b[38;5;241m.\u001b[39minverse_transform(prediction_encoded\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     38\u001b[0m predictions\u001b[38;5;241m.\u001b[39mappend(predicted_tags)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_p5/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:943\u001b[0m, in \u001b[0;36mMultiLabelBinarizer.inverse_transform\u001b[0;34m(self, yt)\u001b[0m\n\u001b[1;32m    941\u001b[0m unexpected \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msetdiff1d(yt, [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unexpected) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 943\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    944\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected only 0s and 1s in label indicator. Also got \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    945\u001b[0m             unexpected\n\u001b[1;32m    946\u001b[0m         )\n\u001b[1;32m    947\u001b[0m     )\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mcompress(indicators)) \u001b[38;5;28;01mfor\u001b[39;00m indicators \u001b[38;5;129;01min\u001b[39;00m yt]\n",
      "\u001b[0;31mValueError\u001b[0m: Expected only 0s and 1s in label indicator. Also got [0.02 0.04 0.06 0.08 0.1  0.12 0.14 0.2  0.34 0.36 0.52]"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def predict_tags_using_rf(train_df=train[::100], feature='title_nltk', target='all_tags', test_df=test, n_estimators=50):\n",
    "    documents = train_df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    dense_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Convert multi-label tags into binary format\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y_encoded = mlb.fit_transform(train_df[target])\n",
    "\n",
    "    # Fit Random Forest model\n",
    "    rf_model = RandomForestRegressor(n_estimators=n_estimators, random_state=42)\n",
    "    rf_model.fit(dense_matrix, y_encoded)\n",
    "\n",
    "    # Predictions\n",
    "    predictions = []\n",
    "    min_range = 3000\n",
    "    max_range = 3005\n",
    "    for i in range(min_range, max_range):\n",
    "        query_document = test_df[feature][i]\n",
    "        print(f'doc {i} : {query_document}')\n",
    "        print(f'real tags : {test[target][i]}')\n",
    "        query_bow = gensim_dictionary.doc2bow(query_document)\n",
    "        query_vector = corpus2dense([query_bow], num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "        # Predict tags using Random Forest\n",
    "        prediction_encoded = rf_model.predict(query_vector.reshape(1, -1))\n",
    "\n",
    "        # Convert back to original tag format\n",
    "        predicted_tags = mlb.inverse_transform(prediction_encoded.reshape(1, -1))\n",
    "\n",
    "        predictions.append(predicted_tags)\n",
    "        print(f'predicted : {predictions[-1]}', '\\n')\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Call the function with your DataFrame and the desired text feature and target tags\n",
    "predictions_rf = predict_tags_using_rf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pourquoi on ne peut pas utiliser le score precision sckikit ici :\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Assuming y_true is the ground truth (real tags) and y_pred is the predicted tags\n",
    "precision = precision_score(['ok', 'ko'], ['ko', 'ok'], average='micro')  # You can use 'micro', 'macro', or 'weighted' depending on your use case\n",
    "print(f'Precision: {precision}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_true is the ground truth (real tags) and y_pred is the predicted tags\n",
    "precision = precision_score([1, 0], [0, 1], average='micro')  # You can use 'micro', 'macro', or 'weighted' depending on your use case\n",
    "\n",
    "print(f'Precision: {precision}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### opti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plotting function\n",
    "def plot_performance_vs_neighbors(grid_search):\n",
    "    # Extract the results from the GridSearchCV object\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Extract the parameters and scores for both uniform and distance weights\n",
    "    params_uniform = [param for param in results['params'][::2]]\n",
    "    params_distance = [param for param in results['params'][1::2]]\n",
    "    test_scores_uniform = results['mean_test_r2'][::2]\n",
    "    test_scores_distance = results['mean_test_r2'][1::2]\n",
    "\n",
    "    # Extract the parameter values for uniform and distance weights\n",
    "    n_neighbors_uniform = [param['knn_regressor__n_neighbors'] for param in params_uniform]\n",
    "    n_neighbors_distance = [param['knn_regressor__n_neighbors'] for param in params_distance]\n",
    "\n",
    "    # Create separate plots for uniform and distance weights\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot for uniform weight\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(n_neighbors_uniform, test_scores_uniform, marker='o', linestyle='-')\n",
    "    plt.title(\"Performance vs. Number of Neighbors (Uniform Weight)\")\n",
    "    plt.xlabel(\"Number of Neighbors\")\n",
    "    plt.ylabel(\"Mean Test R-squared\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot for distance weight\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(n_neighbors_distance, test_scores_distance, marker='o', linestyle='-')\n",
    "    plt.title(\"Performance vs. Number of Neighbors (Distance Weight)\")\n",
    "    plt.xlabel(\"Number of Neighbors\")\n",
    "    plt.ylabel(\"Mean Test R-squared\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pipe_knn(df=train, feature='title_nltk', target='all_tags', embedding='bow_dense', metric='cosine', graph=True):\n",
    "\n",
    "    documents = df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    dense_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    # Ici on ne va pas demander au knn de faire de prediction,\n",
    "    # On veut juste qu'il trouve les voisins.\n",
    "    # Mais la fonction fit a besoin de targets en param\n",
    "    target_values = df[target].values\n",
    "\n",
    "    # Create a KNN Regressor\n",
    "    knn_regressor = KNeighborsRegressor(metric=metric)\n",
    "\n",
    "    # Create a pipeline with preprocessing and a knn regressor, to simplify gridsearch\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"knn_regressor\", knn_regressor)\n",
    "    ])\n",
    "\n",
    "    # Define hyperparameters and their possible values for grid search\n",
    "    param_grid = {\n",
    "        'knn_regressor__n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "        'knn_regressor__weights': ['uniform', 'distance']\n",
    "    }\n",
    "\n",
    "    # Create the GridSearchCV object with multiple scoring metrics\n",
    "    # scoring = {'neg_mean_squared_error': 'neg_mean_squared_error', 'r2': 'r2'}\n",
    "    grid_search = GridSearchCV(pipe, param_grid=param_grid,\n",
    "                            scoring=make_scorer(score_tag_f1), cv=5, verbose=1) # add, refit='f1' for multiple scoring\n",
    "\n",
    "    # Fit the GridSearchCV object to your training data to perform hyperparameter tuning\n",
    "    grid_search.fit(dense_matrix, target_values)\n",
    "\n",
    "    # Access the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "    # Create the KNN regressor with the best hyperparameters\n",
    "    best_knn_regressor = KNeighborsRegressor(metric=metric,\n",
    "                                             n_neighbors=best_params['knn_regressor__n_neighbors'],\n",
    "                                             weights=best_params['knn_regressor__weights'])\n",
    "\n",
    "    # Create a pipeline with the preprocessor and the tuned knn regressor\n",
    "    pipeline_with_tuned_knn = Pipeline(steps=[\n",
    "        (\"knn_regressor\", best_knn_regressor)  # Use the tuned neighbor and weight values here\n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation (on training set) and display the scores for each split\n",
    "    # scoring = ['r2', 'neg_mean_squared_error']\n",
    "    cv_scores = cross_validate(pipeline_with_tuned_knn, dense_matrix, target_values, cv=5, scoring=make_scorer(score_tag_f1))\n",
    "    # print(\"Cross-Validation Scores (training):\", '\\n', cv_scores)\n",
    "    print(\"Cross-Validation Scores:\")\n",
    "    pprint(cv_scores)\n",
    "\n",
    "\n",
    "pipe_knn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for i, score in enumerate(cv_scores['test_r2']):\n",
    "        print(f\"Split {i+1} : r2 = {score}\")\n",
    "\n",
    "    r2_val = cv_scores['test_r2'].mean()\n",
    "    mse_val = -cv_scores['test_neg_mean_squared_error'].mean()\n",
    "    rmse_val = np.sqrt(mse_val)\n",
    "\n",
    "    # fit model on training set\n",
    "    pipeline_with_tuned_knn.fit(dense_matrix, target_values)\n",
    "    # Make no predictions here\n",
    "\n",
    "    # Calculate scores on training\n",
    "    f1 = score_tag_f1(train['all_tags'], pipeline_with_tuned_knn.predict(dense_matrix))\n",
    "    # and testing set\n",
    "    r2_test, rmse_test = score_tag_f1(y_test, y_pred)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"R-squared (val) =  {r2_val}\")\n",
    "    print(f\"R-squared (train) =  {r2_train}\")\n",
    "    print(f\"R-squared (test) =  {r2_test}\")\n",
    "    print(f\"RMSE (val) =  {rmse_val}\")\n",
    "    print(f\"RMSE (train) =  {rmse_train}\")\n",
    "    print(f\"RMSE (test) =  {rmse_test}\" '\\n')\n",
    "\n",
    "    # display results/error as graph on first iteration (if asked to)\n",
    "    if alea == 0 and graph:\n",
    "        plot_performance_vs_neighbors(grid_search)\n",
    "        plot_predictions(r2_train, r2_test, y_pred, y_test, kind='actual_vs_predicted', y=y)\n",
    "        # plot_predictions(r2_train, r2_test, y_pred, y_test, kind='residual_vs_predicted', y=y)\n",
    "\n",
    "    # Return scores for this random state\n",
    "    return r2_val, rmse_val, r2_train, rmse_train, r2_test, rmse_test, time_fit, time_predict\n",
    "\n",
    "\n",
    "def test_knn_n_times(y=y_E, scaler=robust, graph=False, metric='euclidean'):\n",
    "    print(f'Modèle : kNN')\n",
    "    print('target : ', y.name)\n",
    "\n",
    "    results_r2_val, results_rmse_val,  results_r2_train, results_rmse_train = [], [], [], []\n",
    "    results_r2_test, results_rmse_test, results_time_fit, results_time_predict = [], [], [], []\n",
    "\n",
    "    for n in range(nb_iter):\n",
    "        print('Iteration ', n+1)\n",
    "        r2_val, rmse_val, r2_train, rmse_train, r2_test, rmse_test, time_fit, time_predict = pipe_knn(alea=n,\n",
    "                                                                                      y=y,\n",
    "                                                                                      scaler=scaler,\n",
    "                                                                                      graph=graph,\n",
    "                                                                                      metric=metric)\n",
    "        results_r2_val.append(r2_val)\n",
    "        results_rmse_val.append(rmse_val)\n",
    "        results_r2_train.append(r2_train)\n",
    "        results_rmse_train.append(rmse_train)\n",
    "        results_r2_test.append(r2_test)\n",
    "        results_rmse_test.append(rmse_test)\n",
    "        results_time_fit.append(time_fit)\n",
    "        results_time_predict.append(time_predict)\n",
    "\n",
    "    # Calculate means and std devs\n",
    "    r2_val_moy = np.mean(results_r2_val)\n",
    "    rmse_val_moy = np.mean(results_rmse_val)\n",
    "    r2_train_moy = np.mean(results_r2_train)\n",
    "    rmse_train_moy = np.mean(results_rmse_train)\n",
    "    r2_test_moy = np.mean(results_r2_test)\n",
    "    rmse_test_moy = np.mean(results_rmse_test)\n",
    "    time_fit_moy = np.mean(results_time_fit)\n",
    "    time_predict_moy = np.mean(results_time_predict)\n",
    "\n",
    "    r2_val_std = np.std(results_r2_val)\n",
    "    rmse_val_std = np.std(results_rmse_val)\n",
    "    r2_train_std = np.std(results_r2_train)\n",
    "    rmse_train_std = np.std(results_rmse_train)\n",
    "    r2_test_std = np.std(results_r2_test)\n",
    "    rmse_test_std = np.std(results_rmse_test)\n",
    "    time_fit_std = np.std(results_time_fit)\n",
    "    time_predict_std = np.std(results_time_predict)\n",
    "\n",
    "    # Mise en forme\n",
    "    results = {'model': 'kNN',\n",
    "               'set': dataset,\n",
    "               'scaler': scaler,\n",
    "               'target': y.name,\n",
    "               'r2_test_moy': r2_test_moy,\n",
    "               'r2_test_std': r2_test_std,\n",
    "               'rmse_test_moy': rmse_test_moy,\n",
    "               'rmse_test_std': rmse_test_std,\n",
    "               'r2_train_moy': r2_train_moy,\n",
    "               'r2_train_std': r2_train_std,\n",
    "               'rmse_train_moy': rmse_train_moy,\n",
    "               'rmse_train_std': rmse_train_std,\n",
    "               'r2_val_moy': r2_val_moy,\n",
    "               'r2_val_std': r2_val_std,\n",
    "               'rmse_val_moy': rmse_val_moy,\n",
    "               'rmse_val_std': rmse_val_std,\n",
    "               'time_fit_moy': time_fit_moy,\n",
    "               'time_fit_std': time_fit_std,\n",
    "               'time_predict_moy': time_predict_moy,\n",
    "               'time_predict_std': time_predict_std,\n",
    "               }\n",
    "\n",
    "    print(results, '\\n')\n",
    "\n",
    "    # Append a new row for this model\n",
    "    model_results.append(results)\n",
    "\n",
    "test_knn_n_times(scaler=robust, graph=True)\n",
    "# test_knn_n_times(y=y_EI, scaler=robust, graph=True)\n",
    "\n",
    "affichage_results()\n",
    "\n",
    "# 0.4, ce qui est bien, mais pas top.\n",
    "# dataset trop petit pour un knn ? (relativement peu d'individus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def suggest_topics_using_knn(df, feature, alea=42):\n",
    "    documents = df[feature].tolist()\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Set training parameters.\n",
    "    num_topics = 10\n",
    "    chunksize = 2000\n",
    "    passes = 20\n",
    "    iterations = 400\n",
    "    eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "    # Make a index to word dictionary.\n",
    "    temp = gensim_dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = gensim_dictionary.id2token\n",
    "\n",
    "    model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every, random_state=alea)\n",
    "\n",
    "    top_topics = model.top_topics(corpus, topn=20)\n",
    "\n",
    "    # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "    # = umass if same topn (default 20)\n",
    "    avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "    print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "    # Compute Coherence Score (Umass)\n",
    "    coherence_umass = CoherenceModel(model=model, texts=documents, dictionary=gensim_dictionary, coherence='u_mass')\n",
    "    coherence_lda_umass = coherence_umass.get_coherence()\n",
    "    print('u_mass Coherence Score: %.4f.' % coherence_lda_umass)\n",
    "\n",
    "    # Compute Coherence Score (cv)\n",
    "    coherence_cv = CoherenceModel(model=model, texts=documents, dictionary=gensim_dictionary, coherence='c_v')\n",
    "    coherence_lda_cv = coherence_cv.get_coherence()\n",
    "    print('c_v Coherence Score: %.4f.' % coherence_lda_cv)\n",
    "\n",
    "    # Compute Coherence Score (npmi)\n",
    "    coherence_npmi = CoherenceModel(model=model, texts=documents, dictionary=gensim_dictionary, coherence='c_npmi')\n",
    "    coherence_lda_npmi = coherence_npmi.get_coherence()\n",
    "    print('c_npmi Coherence Score: %.4f.' % coherence_lda_npmi)\n",
    "\n",
    "    # Perplexity is not a coherence score but a measure of how well the model predicts a sample.\n",
    "    # A lower perplexity indicates better model performance.\n",
    "    perplexity = model.log_perplexity(corpus)\n",
    "    print('Perplexity: %.4f.' % perplexity)\n",
    "\n",
    "    # Visualize the topics\n",
    "    vis_data = gensimvis.prepare(model, corpus, gensim_dictionary)\n",
    "    display(pyLDAvis.display(vis_data))\n",
    "\n",
    "    # Uncomment the next line if you want to save the plot to a file\n",
    "    # pyLDAvis.save_html(vis_data, 'artifacts/lda_vis.html')\n",
    "\n",
    "    pprint(top_topics)\n",
    "    # to print all topics\n",
    "    # pprint(model.print_topics())\n",
    "\n",
    "    return model, corpus, gensim_dictionary\n",
    "\n",
    "lda_test, corpus_test, dict_test = suggest_topics_using_LDA(train, 'title_nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plotting function\n",
    "def plot_performance_vs_neighbors(grid_search):\n",
    "    # Extract the results from the GridSearchCV object\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Extract the parameters and scores for both uniform and distance weights\n",
    "    params_uniform = [param for param in results['params'][::2]]\n",
    "    params_distance = [param for param in results['params'][1::2]]\n",
    "    test_scores_uniform = results['mean_test_r2'][::2]\n",
    "    test_scores_distance = results['mean_test_r2'][1::2]\n",
    "\n",
    "    # Extract the parameter values for uniform and distance weights\n",
    "    n_neighbors_uniform = [param['knn_regressor__n_neighbors'] for param in params_uniform]\n",
    "    n_neighbors_distance = [param['knn_regressor__n_neighbors'] for param in params_distance]\n",
    "\n",
    "    # Create separate plots for uniform and distance weights\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot for uniform weight\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(n_neighbors_uniform, test_scores_uniform, marker='o', linestyle='-')\n",
    "    plt.title(\"Performance vs. Number of Neighbors (Uniform Weight)\")\n",
    "    plt.xlabel(\"Number of Neighbors\")\n",
    "    plt.ylabel(\"Mean Test R-squared\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot for distance weight\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(n_neighbors_distance, test_scores_distance, marker='o', linestyle='-')\n",
    "    plt.title(\"Performance vs. Number of Neighbors (Distance Weight)\")\n",
    "    plt.xlabel(\"Number of Neighbors\")\n",
    "    plt.ylabel(\"Mean Test R-squared\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pipe_knn(alea, y, scaler, graph, metric):\n",
    "\n",
    "    # Séparation des jeux de données entrainement / validation, preprocessing\n",
    "    X_train, X_test, y_train, y_test, preprocessor = preprocessing(y, alea=alea, test_size=test_size, \\\n",
    "                                                                    scaler=scaler)\n",
    "    # Create a KNN Regressor\n",
    "    knn_regressor = KNeighborsRegressor(metric=metric)\n",
    "\n",
    "    # Create a pipeline with preprocessing and a knn regressor\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"knn_regressor\", knn_regressor)\n",
    "    ])\n",
    "\n",
    "    # Define hyperparameters and their possible values for grid search\n",
    "    param_grid = {\n",
    "        'knn_regressor__n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "        'knn_regressor__weights': ['uniform', 'distance']\n",
    "    }\n",
    "\n",
    "    # Create the GridSearchCV object with multiple scoring metrics\n",
    "    scoring = {'neg_mean_squared_error': 'neg_mean_squared_error', 'r2': 'r2'}\n",
    "    grid_search = GridSearchCV(pipe, param_grid=param_grid,\n",
    "                            scoring=scoring, cv=5, refit='r2', verbose=1)\n",
    "\n",
    "    # Fit the GridSearchCV object to your training data to perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Access the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "    # Create the KNN regressor with the best hyperparameters\n",
    "    best_knn_regressor = KNeighborsRegressor(metric=metric,\n",
    "                                             n_neighbors=best_params['knn_regressor__n_neighbors'],\n",
    "                                             weights=best_params['knn_regressor__weights'])\n",
    "\n",
    "    # Create a pipeline with the preprocessor and the tuned knn regressor\n",
    "    pipeline_with_tuned_knn = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"knn_regressor\", best_knn_regressor)  # Use the tuned neighbor and weight values here\n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation (on training set) and display the scores for each split\n",
    "    scoring = ['r2', 'neg_mean_squared_error']\n",
    "    cv_scores = cross_validate(pipeline_with_tuned_knn, X_train, y_train, cv=5, scoring=scoring)\n",
    "    # print(\"Cross-Validation Scores (training):\", '\\n', cv_scores)\n",
    "    print(\"Cross-Validation Scores:\")\n",
    "    for i, score in enumerate(cv_scores['test_r2']):\n",
    "        print(f\"Split {i+1} : r2 = {score}\")\n",
    "\n",
    "    r2_val = cv_scores['test_r2'].mean()\n",
    "    mse_val = -cv_scores['test_neg_mean_squared_error'].mean()\n",
    "    rmse_val = np.sqrt(mse_val)\n",
    "\n",
    "    # fit model on training set\n",
    "    time_fit = fit_and_timeit(pipeline_with_tuned_knn, X_train, y_train)\n",
    "    # Make predictions\n",
    "    y_pred, time_predict = predict_and_timeit(pipeline_with_tuned_knn, X_test)\n",
    "\n",
    "    # Calculate scores on training\n",
    "    r2_train, rmse_train = calcul_scores(y_train, pipeline_with_tuned_knn.predict(X_train))\n",
    "    # and testing set\n",
    "    r2_test, rmse_test = calcul_scores(y_test, y_pred)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"R-squared (val) =  {r2_val}\")\n",
    "    print(f\"R-squared (train) =  {r2_train}\")\n",
    "    print(f\"R-squared (test) =  {r2_test}\")\n",
    "    print(f\"RMSE (val) =  {rmse_val}\")\n",
    "    print(f\"RMSE (train) =  {rmse_train}\")\n",
    "    print(f\"RMSE (test) =  {rmse_test}\" '\\n')\n",
    "\n",
    "    # display results/error as graph on first iteration (if asked to)\n",
    "    if alea == 0 and graph:\n",
    "        plot_performance_vs_neighbors(grid_search)\n",
    "        plot_predictions(r2_train, r2_test, y_pred, y_test, kind='actual_vs_predicted', y=y)\n",
    "        # plot_predictions(r2_train, r2_test, y_pred, y_test, kind='residual_vs_predicted', y=y)\n",
    "\n",
    "    # Return scores for this random state\n",
    "    return r2_val, rmse_val, r2_train, rmse_train, r2_test, rmse_test, time_fit, time_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning Models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_p5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
