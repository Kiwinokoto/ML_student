{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python version 3.11.5 (main, Sep 11 2023, 13:23:44) [GCC 11.2.0]\n",
      "pyLDAvis version 3.4.0\n",
      "\n",
      "Number of CPU cores: 8\n",
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, random\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "from pandarallel import pandarallel\n",
    "from pprint import pprint\n",
    "import json\n",
    "import requests\n",
    "from collections import Counter\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "# NLP\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import corpus2dense\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim import similarities\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "#\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.metrics import make_scorer, PredictionErrorDisplay, r2_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "print('\\nPython version ' + sys.version)\n",
    "print('pyLDAvis version ' + pyLDAvis.__version__)\n",
    "\n",
    "# Modify if necessary\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"\\nNumber of CPU cores: {num_cores}\")\n",
    "pandarallel.initialize(progress_bar=False, nb_workers=6)\n",
    "\n",
    "#\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.models.signature import infer_signature, ModelSignature #, Schema, ParamSchema\n",
    "from mlflow.types import Schema, ParamSchema, ParamSpec, ColSpec\n",
    "\n",
    "# os.environ['MLFLOW_TRACKING_URI'] = './'\n",
    "\n",
    "# ! REQUIRES CONSOLE COMMAND : mlflow ui\n",
    "# depuis dossier notebooks\n",
    "# at least once, to creat mlruns folder\n",
    "\n",
    "# Utilisable seulement en local...\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = MlflowClient(tracking_uri=\"http://127.0.0.1:5000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlflow_experiment(\n",
    "    experiment_name: str, artifact_location: str, tags: dict[str, str]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a new mlflow experiment with the given name and artifact location.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    experiment_name: str\n",
    "        The name of the experiment to create.\n",
    "    artifact_location: str\n",
    "        The artifact location of the experiment to create.\n",
    "    tags: dict[str,Any]\n",
    "        The tags of the experiment to create.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    experiment_id: str\n",
    "        The id of the created experiment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        experiment_id = mlflow.create_experiment(\n",
    "            name=experiment_name, artifact_location=artifact_location, tags=tags\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Experiment {experiment_name} already exists.\")\n",
    "        experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "    mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "    return experiment_id\n",
    "\n",
    "\n",
    "def get_mlflow_experiment(\n",
    "    experiment_id: str = None, experiment_name: str = None\n",
    ") -> mlflow.entities.Experiment:\n",
    "    \"\"\"\n",
    "    Retrieve the mlflow experiment with the given id or name.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    experiment_id: str\n",
    "        The id of the experiment to retrieve.\n",
    "    experiment_name: str\n",
    "        The name of the experiment to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    experiment: mlflow.entities.Experiment\n",
    "        The mlflow experiment with the given id or name.\n",
    "    \"\"\"\n",
    "    if experiment_id is not None:\n",
    "        experiment = mlflow.get_experiment(experiment_id)\n",
    "    elif experiment_name is not None:\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    else:\n",
    "        raise ValueError(\"Either experiment_id or experiment_name must be provided.\")\n",
    "\n",
    "    return experiment\n",
    "\n",
    "\n",
    "def turn_str_back_into_list(df):\n",
    "    \"\"\"Correct the type change due to .csv export\"\"\"\n",
    "\n",
    "    df['title_nltk'] = df['title_nltk'].apply(ast.literal_eval)\n",
    "    df['body_nltk'] = df['body_nltk'].apply(ast.literal_eval)\n",
    "    df['title_spacy'] = df['title_spacy'].apply(ast.literal_eval)\n",
    "    df['body_spacy'] = df['body_spacy'].apply(ast.literal_eval)\n",
    "    df['all_tags'] = df['all_tags'].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "def token_list_into_bow(X):\n",
    "    documents = X.tolist()\n",
    "    # print(documents)\n",
    "    gensim_dictionary = Dictionary(documents)\n",
    "    corpus = [gensim_dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Convert Gensim corpus to dense matrix\n",
    "    bow_matrix = corpus2dense(corpus, num_terms=len(gensim_dictionary)).T\n",
    "\n",
    "    return bow_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Experiment: artifact_location='/home/ubuntu/Bureau/OC/Projet5_oudelet_kevin/notebooks/artifacts', creation_time=1705840547540, experiment_id='627378971600475090', last_update_time=1705840547540, lifecycle_stage='active', name='knn_optimisation', tags={'feature': 'title', 'modele': 'knn', 'nlp': 'nltk'}>,\n",
      " <Experiment: artifact_location='mlflow-artifacts:/0', creation_time=1705504459415, experiment_id='0', last_update_time=1705504459415, lifecycle_stage='active', name='Default', tags={}>]\n"
     ]
    }
   ],
   "source": [
    "# Again, this needs mlfow ui console command first -> unusable on remote server\n",
    "all_experiments = client.search_experiments()\n",
    "pprint(all_experiments)\n",
    "\n",
    "train = pd.read_csv('./../data/cleaned_data/train_bow_uniques.csv', sep=',')\n",
    "test = pd.read_csv('./../data/cleaned_data/test_bow_uniques.csv', sep=',')\n",
    "\n",
    "turn_str_back_into_list(train)\n",
    "turn_str_back_into_list(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add scorers : precision, r2, jaccard?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sans le preprocessor\n",
    "# obligé de transformer la target mm si on ne s'sn sert pas vraiment, car grid_search.fit()\n",
    "# n'accepte que des valeurs numériques.\n",
    "# du coup on peut utiliser des metriques classiques pour le score (ici r2),\n",
    "# mais ca n'a aucun sens metier interpretable\n",
    "\n",
    "# ici convertir les tags en bag of words ou les one hot encoder revient exactement au meme, donc\n",
    "# autant utiliser le bow, on a deja le transformer.\n",
    "\n",
    "# ca prend trop de ressources ! Il est tps d'utiliser les nested runs de mlflow\n",
    "\n",
    "def pipe_knn(train_df=train, feature='title_nltk', target='all_tags', test_df=test, input=['']):\n",
    "    # Load your training data and labels\n",
    "    X_train = train_df[feature].values\n",
    "    y_train = train_df[target].values\n",
    "\n",
    "    X_bow_matrix = token_list_into_bow(X_train)\n",
    "    y_bow_matrix = token_list_into_bow(y_train)\n",
    "\n",
    "    # Create a KNN Regressor\n",
    "    knn_regressor = KNeighborsRegressor(metric=metric)\n",
    "\n",
    "    # Create a pipeline with preprocessing and a knn regressor, to simplify gridsearch\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"knn_regressor\", knn_regressor)\n",
    "    ])\n",
    "\n",
    "    # Define hyperparameters and their possible values for grid search\n",
    "    param_grid = {\n",
    "        'knn_regressor__n_neighbors': [1],\n",
    "        'knn_regressor__weights': ['uniform'] # , 'distance'\n",
    "    }\n",
    "\n",
    "    # Create the GridSearchCV object with multiple scoring metrics\n",
    "    # scoring = {'neg_mean_squared_error': 'neg_mean_squared_error', 'r2': 'r2'}\n",
    "    grid_search = GridSearchCV(pipe, param_grid=param_grid,\n",
    "                            scoring='r2', cv=5, verbose=1) # add, refit='precision' for multiple scoring\n",
    "\n",
    "    # Fit the GridSearchCV object to your training data to perform hyperparameter tuning\n",
    "    grid_search.fit(X_bow_matrix, y_bow_matrix)\n",
    "\n",
    "    # Access the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "    # Create the KNN regressor with the best hyperparameters\n",
    "    best_knn_regressor = KNeighborsRegressor(# metric=metric,\n",
    "                                             n_neighbors=best_params['knn_regressor__n_neighbors'],\n",
    "                                             weights=best_params['knn_regressor__weights'])\n",
    "\n",
    "    # Create a pipeline with the preprocessor and the tuned knn regressor\n",
    "    pipeline_with_tuned_knn = Pipeline(steps=[\n",
    "        (\"knn_regressor\", best_knn_regressor)  # Use the tuned neighbor and weight values here\n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation (on training set) and display the scores for each split\n",
    "    # scoring = ['r2', 'neg_mean_squared_error']\n",
    "    cv_scores = cross_validate(pipeline_with_tuned_knn, X_bow_matrix, y_bow_matrix, cv=5, scoring='r2')\n",
    "    # print(\"Cross-Validation Scores (training):\", '\\n', cv_scores)\n",
    "    print(\"Cross-Validation Scores:\")\n",
    "    pprint(cv_scores)\n",
    "    for i, score in enumerate(cv_scores['test_score']):\n",
    "        print(f\"Split {i+1} : precision = {score}\")\n",
    "\n",
    "    return best_knn_regressor.predict(input)\n",
    "\n",
    "\n",
    "# pipe_knn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment knn_optimisation already exists.\n"
     ]
    }
   ],
   "source": [
    "experiment_id = create_mlflow_experiment(\n",
    "    experiment_name=\"knn_optimisation\",\n",
    "    artifact_location=\"./artifacts\",\n",
    "    tags={\"modele\": \"knn\", \"feature\": \"title\", 'nlp': 'nltk'},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: knn_optimisation\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best Hyperparameters: {'knn_regressor__n_neighbors': 1, 'knn_regressor__weights': 'uniform'}\n",
      "Cross-Validation Scores:\n",
      "{'fit_time': array([0.00050712, 0.00046039, 0.00036788, 0.00030231, 0.0004909 ]),\n",
      " 'score_time': array([0.01880622, 0.00252485, 0.00210047, 0.00295734, 0.00212431]),\n",
      " 'test_score': array([0.67516111, 0.60668551, 0.64869993, 0.63764831, 0.6433043 ])}\n",
      "Split 1 : precision = 0.675161110761773\n",
      "Split 2 : precision = 0.6066855122136453\n",
      "Split 3 : precision = 0.6486999294162054\n",
      "Split 4 : precision = 0.6376483056687834\n",
      "Split 5 : precision = 0.6433043046297113\n",
      "run_id: 73a25a353cb348d88acf3cef6bcb903c\n",
      "experiment_id: 627378971600475090\n",
      "Artifact Location: /home/ubuntu/Bureau/OC/Projet5_oudelet_kevin/notebooks/artifacts\n",
      "status: RUNNING\n",
      "start_time: 1705847188766\n",
      "end_time: None\n"
     ]
    }
   ],
   "source": [
    "experiment = get_mlflow_experiment(experiment_id=experiment_id)\n",
    "print(\"Name: {}\".format(experiment.name))\n",
    "\n",
    "\n",
    "# define a custom model\n",
    "class MyKnn(mlflow.pyfunc.PythonModel):\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        return self.make_prediction(model_input, params)\n",
    "\n",
    "    def make_prediction(self, model_input, params=None):\n",
    "        # do something with the model input\n",
    "        return pipe_knn(input=model_input)\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=\"testing\", experiment_id=experiment_id) as run:\n",
    "\n",
    "    # log model using autolog\n",
    "    # mlflow.autolog()\n",
    "    # mlflow.sklearn.autolog()\n",
    "    pipe_knn(train_df=train[:100])\n",
    "\n",
    "    # log model\n",
    "    mlflow.sklearn.log_model(sk_model=MyKnn(), artifact_path=\"knn\")\n",
    "    # mlflow.pyfunc.log_model(artifact_path=\"knn\", python_model=KNeighborsRegressor())\n",
    "\n",
    "    # print run info\n",
    "    print(\"run_id: {}\".format(run.info.run_id))\n",
    "    print(\"experiment_id: {}\".format(run.info.experiment_id))\n",
    "    print(\"Artifact Location: {}\".format(experiment.artifact_location))\n",
    "    print(\"status: {}\".format(run.info.status))\n",
    "    print(\"start_time: {}\".format(run.info.start_time))\n",
    "    print(\"end_time: {}\".format(run.info.end_time))\n",
    "    # print(\"lifecycle_stage: {}\".format(run.info.lifecycle_stage)) # deprecated, use alias or tags\n",
    "\n",
    "\n",
    "# J'esperais qu'mlflow allait nous permettre de contourner le probleme de\n",
    "# l'entrainement du modele, qui demande bcp d'espace memoire.\n",
    "# probleme : mm sans l'ui, le tracking/logging mlflow consomment enormement !\n",
    "# la solution a l'air pire que le probleme...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_p5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
